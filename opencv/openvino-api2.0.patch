diff --git a/cmake/OpenCVDetectInferenceEngine.cmake b/cmake/OpenCVDetectInferenceEngine.cmake
index 319fd5bf0a..9a2eb38b03 100644
--- a/cmake/OpenCVDetectInferenceEngine.cmake
+++ b/cmake/OpenCVDetectInferenceEngine.cmake
@@ -13,67 +13,3 @@ if(WITH_OPENVINO)
     return()
   endif()
 endif()
-
-# ======================
-
-if(WITH_OPENVINO)
-  find_package(OpenVINO QUIET)
-  if(OpenVINO_FOUND)
-    message(STATUS "OpenVINO FOUND: ${OpenVINO_VERSION}")
-    math(EXPR ver "${OpenVINO_VERSION_MAJOR} * 1000000 + ${OpenVINO_VERSION_MINOR} * 10000 + ${OpenVINO_VERSION_PATCH} * 100")
-    ocv_add_external_target(openvino "" "openvino::runtime" "INF_ENGINE_RELEASE=${ver};HAVE_NGRAPH;HAVE_DNN_NGRAPH;HAVE_INF_ENGINE")
-    set(HAVE_OPENVINO 1)
-    return()
-  endif()
-endif()
-
-# ======================
-
-find_package(InferenceEngine QUIET)
-if(InferenceEngine_FOUND)
-  set(INF_ENGINE_TARGET ${InferenceEngine_LIBRARIES})
-  set(INF_ENGINE_VERSION "${InferenceEngine_VERSION}")
-  message(STATUS "Detected InferenceEngine: cmake package (${InferenceEngine_VERSION})")
-endif()
-
-if(DEFINED InferenceEngine_VERSION)
-  message(STATUS "InferenceEngine: ${InferenceEngine_VERSION}")
-  if(NOT INF_ENGINE_RELEASE AND NOT (InferenceEngine_VERSION VERSION_LESS "2021.4"))
-    math(EXPR INF_ENGINE_RELEASE_INIT "${InferenceEngine_VERSION_MAJOR} * 1000000 + ${InferenceEngine_VERSION_MINOR} * 10000 + ${InferenceEngine_VERSION_PATCH} * 100")
-  endif()
-endif()
-if(NOT INF_ENGINE_RELEASE AND NOT INF_ENGINE_RELEASE_INIT)
-  message(STATUS "WARNING: InferenceEngine version has not been set, 2021.4.2 will be used by default. Set INF_ENGINE_RELEASE variable if you experience build errors.")
-  set(INF_ENGINE_RELEASE_INIT "2021040200")
-elseif(DEFINED INF_ENGINE_RELEASE)
-  set(INF_ENGINE_RELEASE_INIT "${INF_ENGINE_RELEASE}")
-endif()
-set(INF_ENGINE_RELEASE "${INF_ENGINE_RELEASE_INIT}" CACHE STRING "Force IE version, should be in form YYYYAABBCC (e.g. 2020.1.0.2 -> 2020010002)")
-
-set(tgts)
-set(defs)
-
-# Add more features to the target
-if(INF_ENGINE_TARGET)
-  set_target_properties(${INF_ENGINE_TARGET} PROPERTIES
-      INTERFACE_COMPILE_DEFINITIONS "HAVE_INF_ENGINE=1;INF_ENGINE_RELEASE=${INF_ENGINE_RELEASE}"
-  )
-  list(APPEND tgts ${INF_ENGINE_TARGET})
-  list(APPEND defs "INF_ENGINE_RELEASE=${INF_ENGINE_RELEASE}" "HAVE_INF_ENGINE")
-endif()
-
-if(WITH_NGRAPH OR NOT DEFINED WITH_NGRAPH)
-  find_package(ngraph QUIET)
-  if(ngraph_FOUND)
-    ocv_assert(TARGET ngraph::ngraph)
-    if(INF_ENGINE_RELEASE VERSION_LESS "2019039999")
-      message(WARNING "nGraph is not tested with current InferenceEngine version: INF_ENGINE_RELEASE=${INF_ENGINE_RELEASE}")
-    endif()
-    message(STATUS "Detected ngraph: cmake package (${ngraph_VERSION})")
-    set(HAVE_NGRAPH ON)
-    list(APPEND tgts ngraph::ngraph)
-    list(APPEND defs "HAVE_NGRAPH" "HAVE_DNN_NGRAPH")
-  endif()
-endif()
-
-ocv_add_external_target(openvino "" "${tgts}" "${defs}")
diff --git a/modules/dnn/src/ie_ngraph.cpp b/modules/dnn/src/ie_ngraph.cpp
index 7eeb62bcf6..6e7b9f9be5 100644
--- a/modules/dnn/src/ie_ngraph.cpp
+++ b/modules/dnn/src/ie_ngraph.cpp
@@ -14,7 +14,7 @@
 #include <opencv2/dnn/shape_utils.hpp>
 
 #ifdef HAVE_DNN_NGRAPH
-#include <ie_extension.h>
+#include <openvino/core/extension.hpp>
 #endif  // HAVE_DNN_NGRAPH
 
 #include <opencv2/core/utils/configuration.private.hpp>
@@ -35,36 +35,6 @@ static bool DNN_IE_SERIALIZE = utils::getConfigurationParameterBool("OPENCV_DNN_
 static std::string kDefaultInpLayerName = "opencv_ngraph_empty_inp_layer_name";
 static constexpr const char* kOpenCVLayersType = "opencv_ngraph_layer";
 
-#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2022_1)
-static std::string shapesToStr(const std::vector<Mat>& mats)
-{
-    std::ostringstream shapes;
-    shapes << mats.size() << " ";
-    for (const Mat& m : mats)
-    {
-        shapes << m.dims << " ";
-        for (int i = 0; i < m.dims; ++i)
-            shapes << m.size[i] << " ";
-    }
-    return shapes.str();
-}
-
-static void strToShapes(const std::string& str, std::vector<std::vector<size_t> >& shapes)
-{
-    std::istringstream ss(str);
-    int num, dims;
-    ss >> num;
-    shapes.resize(num);
-    for (int i = 0; i < num; ++i)
-    {
-        ss >> dims;
-        shapes[i].resize(dims);
-        for (int j = 0; j < dims; ++j)
-            ss >> shapes[i][j];
-    }
-}
-#endif // OpenVINO < 2022.1
-
 static std::vector<Ptr<NgraphBackendWrapper> >
 ngraphWrappers(const std::vector<Ptr<BackendWrapper> >& ptrs)
 {
@@ -78,13 +48,11 @@ ngraphWrappers(const std::vector<Ptr<BackendWrapper> >& ptrs)
     return wrappers;
 }
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
-
 class NgraphCustomOp: public ov::op::Op {
 public:
     OPENVINO_OP(kOpenCVLayersType);
 
-    NgraphCustomOp(const ngraph::OutputVector& inputs, Ptr<Layer>& cvLayer, const std::vector<Mat>& outputs, const std::vector<Mat>& internals):
+    NgraphCustomOp(const ov::OutputVector& inputs, Ptr<Layer>& cvLayer, const std::vector<Mat>& outputs, const std::vector<Mat>& internals):
         Op(inputs), cvLayer(cvLayer), outputs(outputs), internals(internals)
     {
         constructor_validate_and_infer_types();
@@ -103,7 +71,7 @@ public:
         }
     }
 
-    std::shared_ptr<ngraph::Node> clone_with_new_inputs(const ngraph::OutputVector& new_args) const override
+    std::shared_ptr<ov::Node> clone_with_new_inputs(const ov::OutputVector& new_args) const override
     {
         return std::make_shared<NgraphCustomOp>(new_args, cvLayer, outputs, internals);
     }
@@ -131,265 +99,13 @@ public:
     std::vector<Mat> outputs, internals;
 };
 
-#else
-
-class NgraphCustomOp: public ngraph::op::Op {
-public:
-    const ngraph::NodeTypeInfo& get_type_info() const override
-    {
-        static constexpr ngraph::NodeTypeInfo type_info{kOpenCVLayersType, static_cast<uint64_t>(0)};
-        return type_info;
-    }
-
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_3)
-    NgraphCustomOp(const ngraph::OutputVector& inputs,
-#else
-    NgraphCustomOp(const ngraph::NodeVector& inputs,
-#endif
-                   const std::map<std::string, InferenceEngine::Parameter>& params = {}):
-        Op(inputs), params(params)
-    {
-        constructor_validate_and_infer_types();
-    }
-
-    ~NgraphCustomOp()
-    {
-        // nothing
-    }
-
-    void validate_and_infer_types() override
-    {
-        std::vector<std::vector<size_t> > shapes;
-        strToShapes(params["outputs"], shapes);
-        set_output_size(shapes.size());
-        for (size_t i = 0; i < shapes.size(); ++i)
-        {
-            ngraph::Shape output_shape(shapes[i]);
-            set_output_type(i, get_input_element_type(0), output_shape);
-        }
-    }
-
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_4)
-    std::shared_ptr<ngraph::Node> clone_with_new_inputs(const ngraph::OutputVector& new_args) const override
-    {
-        return std::make_shared<NgraphCustomOp>(new_args, params);
-    }
-#else
-    std::shared_ptr<ngraph::Node> copy_with_new_args(const ngraph::NodeVector& new_args) const override
-    {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_3)
-        return std::make_shared<NgraphCustomOp>(ngraph::as_output_vector(new_args), params);
-#else
-        return std::make_shared<NgraphCustomOp>(new_args, params);
-#endif
-    }
-#endif
-
-    bool visit_attributes(ngraph::AttributeVisitor& visitor) override
-    {
-        for (auto& attr : params)
-        {
-            if (attr.second.is<std::string>())
-                visitor.on_attribute(attr.first, attr.second.as<std::string>());
-        }
-        return true;
-    }
-
-    std::map<std::string, InferenceEngine::Parameter> params;
-};
-
-
-class InfEngineNgraphCustomLayer : public InferenceEngine::ILayerExecImpl
-{
-public:
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_2)
-    explicit InfEngineNgraphCustomLayer(const std::shared_ptr<ngraph::Node>& _node)
-    {
-        node = std::dynamic_pointer_cast<NgraphCustomOp>(_node);
-        CV_Assert(node);
-        std::string implStr = node->params["impl"];
-        std::istringstream iss(implStr);
-#else
-    explicit InfEngineNgraphCustomLayer(const InferenceEngine::CNNLayer& layer) : cnnLayer(layer)
-    {
-        std::istringstream iss(layer.GetParamAsString("impl"));
-#endif
-        size_t ptr;
-        iss >> ptr;
-        cvLayer = (Layer*)ptr;
-
-        std::vector<std::vector<size_t> > shapes;
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_2)
-        strToShapes(node->params["internals"], shapes);
-#else
-        strToShapes(layer.GetParamAsString("internals"), shapes);
-#endif
-        internals.resize(shapes.size());
-        for (int i = 0; i < shapes.size(); ++i)
-            internals[i].create(std::vector<int>(shapes[i].begin(), shapes[i].end()), CV_32F);
-    }
-
-    ~InfEngineNgraphCustomLayer()
-    {
-        // nothing
-    }
-
-    virtual InferenceEngine::StatusCode execute(std::vector<InferenceEngine::Blob::Ptr>& inputs,
-                                                std::vector<InferenceEngine::Blob::Ptr>& outputs,
-                                                InferenceEngine::ResponseDesc *resp) noexcept
-    {
-        std::vector<Mat> inpMats, outMats;
-        infEngineBlobsToMats(inputs, inpMats);
-        infEngineBlobsToMats(outputs, outMats);
-
-        try
-        {
-            cvLayer->forward(inpMats, outMats, internals);
-            return InferenceEngine::StatusCode::OK;
-        }
-        catch (...)
-        {
-            return InferenceEngine::StatusCode::GENERAL_ERROR;
-        }
-    }
-
-    virtual InferenceEngine::StatusCode
-    getSupportedConfigurations(std::vector<InferenceEngine::LayerConfig>& conf,
-                               InferenceEngine::ResponseDesc* resp) noexcept
-    {
-        std::vector<InferenceEngine::DataConfig> inDataConfig;
-        std::vector<InferenceEngine::DataConfig> outDataConfig;
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_2)
-        InferenceEngine::SizeVector order;
-        for (int i = 0; i < node->get_input_size(); ++i)
-        {
-            InferenceEngine::DataConfig conf;
-            auto shape = node->input_value(i).get_shape();
-            order.resize(shape.size());
-            std::iota(order.begin(), order.end(), 0);
-            conf.desc = InferenceEngine::TensorDesc(InferenceEngine::Precision::FP32, shape, {shape, order});
-            inDataConfig.push_back(conf);
-        }
-
-        for (int i = 0; i < node->get_output_size(); ++i)
-        {
-            InferenceEngine::DataConfig conf;
-            auto shape = node->output(i).get_shape();
-            order.resize(shape.size());
-            std::iota(order.begin(), order.end(), 0);
-            conf.desc = InferenceEngine::TensorDesc(InferenceEngine::Precision::FP32, shape, {shape, order});
-            outDataConfig.push_back(conf);
-        }
-#else
-        for (auto& it : cnnLayer.insData)
-        {
-            InferenceEngine::DataConfig conf;
-            conf.desc = it.lock()->getTensorDesc();
-            inDataConfig.push_back(conf);
-        }
-
-        for (auto& it : cnnLayer.outData)
-        {
-            InferenceEngine::DataConfig conf;
-            conf.desc = it->getTensorDesc();
-            outDataConfig.push_back(conf);
-        }
-#endif
-
-        InferenceEngine::LayerConfig layerConfig;
-        layerConfig.inConfs = inDataConfig;
-        layerConfig.outConfs = outDataConfig;
-
-        conf.push_back(layerConfig);
-        return InferenceEngine::StatusCode::OK;
-    }
-
-    InferenceEngine::StatusCode init(InferenceEngine::LayerConfig& config,
-                                     InferenceEngine::ResponseDesc *resp) noexcept
-    {
-        return InferenceEngine::StatusCode::OK;
-    }
-
-private:
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_2)
-    std::shared_ptr<NgraphCustomOp> node;
-#else
-    InferenceEngine::CNNLayer cnnLayer;
-#endif
-    dnn::Layer* cvLayer;
-    std::vector<Mat> internals;
-};
-
-#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2020_2)
-class InfEngineNgraphCustomLayerFactory : public InferenceEngine::ILayerImplFactory {
-public:
-    explicit InfEngineNgraphCustomLayerFactory(const InferenceEngine::CNNLayer* layer) : cnnLayer(*layer)
-    {
-        // nothing
-    }
-
-    InferenceEngine::StatusCode
-    getImplementations(std::vector<InferenceEngine::ILayerImpl::Ptr>& impls,
-                       InferenceEngine::ResponseDesc* resp) noexcept override
-    {
-        impls.push_back(std::make_shared<InfEngineNgraphCustomLayer>(cnnLayer));
-        return InferenceEngine::StatusCode::OK;
-    }
-
-private:
-    InferenceEngine::CNNLayer cnnLayer;
-};
-#endif
-
-
-class InfEngineNgraphExtension : public InferenceEngine::IExtension
-{
-public:
-    void Unload() noexcept override {}
-    void Release() noexcept override { delete this; }
-    void GetVersion(const InferenceEngine::Version*&) const noexcept override {}
-
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_2)
-    std::vector<std::string> getImplTypes(const std::shared_ptr<ngraph::Node>& node) override {
-        return {"CPU"};
-    }
-
-    InferenceEngine::ILayerImpl::Ptr getImplementation(const std::shared_ptr<ngraph::Node>& node, const std::string& implType) override {
-        if (std::dynamic_pointer_cast<NgraphCustomOp>(node) && implType == "CPU") {
-            return std::make_shared<InfEngineNgraphCustomLayer>(node);
-        }
-        return nullptr;
-    }
-#else
-    virtual void SetLogCallback(InferenceEngine::IErrorListener&) noexcept {}
-
-    virtual InferenceEngine::StatusCode getPrimitiveTypes(char**&, unsigned int&,
-                                                          InferenceEngine::ResponseDesc*) noexcept
-    {
-        return InferenceEngine::StatusCode::OK;
-    }
-
-    InferenceEngine::StatusCode getFactoryFor(InferenceEngine::ILayerImplFactory*& factory,
-                                              const InferenceEngine::CNNLayer* cnnLayer,
-                                              InferenceEngine::ResponseDesc* resp) noexcept
-    {
-        if (cnnLayer->type != kOpenCVLayersType)
-            return InferenceEngine::StatusCode::NOT_IMPLEMENTED;
-        factory = new InfEngineNgraphCustomLayerFactory(cnnLayer);
-        return InferenceEngine::StatusCode::OK;
-    }
-#endif
-};
-
-#endif // OpenVINO >= 2022.1
-
-InfEngineNgraphNode::InfEngineNgraphNode(ngraph::Output<ngraph::Node>&& _node)
+InfEngineNgraphNode::InfEngineNgraphNode(ov::Output<ov::Node>&& _node)
     : BackendNode(DNN_BACKEND_INFERENCE_ENGINE_NGRAPH), node(std::move(_node)) {
     CV_Assert(node.get_node());
     CV_Assert(node.get_node_shared_ptr());
 }
 
-InfEngineNgraphNode::InfEngineNgraphNode(const ngraph::Output<ngraph::Node>& _node)
+InfEngineNgraphNode::InfEngineNgraphNode(const ov::Output<ov::Node>& _node)
     : BackendNode(DNN_BACKEND_INFERENCE_ENGINE_NGRAPH), node(_node) {
     CV_Assert(node.get_node());
     CV_Assert(node.get_node_shared_ptr());
@@ -400,27 +116,11 @@ InfEngineNgraphNode::InfEngineNgraphNode(const std::vector<Ptr<BackendNode> >& n
                                          std::vector<Mat>& outputs, std::vector<Mat>& internals)
     : BackendNode(DNN_BACKEND_INFERENCE_ENGINE_NGRAPH), cvLayer(cvLayer_)
 {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_3)
-    ngraph::OutputVector inp_nodes;
-#else
-    ngraph::NodeVector inp_nodes;
-#endif
+    ov::OutputVector inp_nodes;
     for (const auto& node : nodes)
         inp_nodes.emplace_back(node.dynamicCast<InfEngineNgraphNode>()->node);
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
     node = std::make_shared<NgraphCustomOp>(inp_nodes, cvLayer, outputs, internals);
-#else
-    std::ostringstream oss;
-    oss << (size_t)cvLayer.get();
-    std::map<std::string, InferenceEngine::Parameter> params = {
-        {"impl", oss.str()},
-        {"outputs", shapesToStr(outputs)},
-        {"internals", shapesToStr(internals)}
-    };
-    node = std::make_shared<NgraphCustomOp>(inp_nodes, params);
-#endif
-
     CV_Assert(!cvLayer->name.empty());
     setName(cvLayer->name);
 }
@@ -436,7 +136,7 @@ InfEngineNgraphNet::InfEngineNgraphNet(detail::NetImplBase& netImpl)
     device_name = "CPU";
 }
 
-InfEngineNgraphNet::InfEngineNgraphNet(detail::NetImplBase& netImpl, InferenceEngine::CNNNetwork& net)
+InfEngineNgraphNet::InfEngineNgraphNet(detail::NetImplBase& netImpl, std::shared_ptr<ov::Model>& net)
     : netImpl_(netImpl)
     , cnn(net)
 {
@@ -455,64 +155,31 @@ void InfEngineNgraphNet::createNet(Target targetId) {
     if (!hasNetOwner)
     {
         CV_Assert(!requestedOutputs.empty());
-        ngraph::ResultVector outs;
+        ov::ResultVector outs;
 
         for (auto output_node_it = requestedOutputs.begin(); output_node_it != requestedOutputs.end(); ++output_node_it)
         {
             CV_LOG_DEBUG(NULL, "DNN/NGRAPH: Add 'Result' output: " << output_node_it->first);
             CV_Assert(output_node_it->second);
-            auto out = std::make_shared<ngraph::op::Result>(output_node_it->second->node);
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
+            auto out = std::make_shared<ov::op::v0::Result>(output_node_it->second->node);
             out->set_friendly_name(output_node_it->first + (output_node_it->second->node.get_node()->get_output_size() == 1 ? "" : ".0"));
-#endif
             outs.push_back(out);
         }
         CV_Assert_N(!inputs_vec.empty(), !outs.empty());
-        ngraph_function = std::make_shared<ngraph::Function>(outs, inputs_vec);
+        ngraph_function = std::make_shared<ov::Model>(outs, inputs_vec);
         init(targetId);
     }
 }
 
-#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2022_1)
-static inline
-InferenceEngine::Layout estimateLayout(size_t dims);
-#endif
-
 void InfEngineNgraphNet::init(Target targetId)
 {
     if (!hasNetOwner)
     {
         if (targetId == DNN_TARGET_OPENCL_FP16)
         {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
             ov::pass::ConvertFP32ToFP16().run_on_model(ngraph_function);
-#else
-            auto nodes = ngraph_function->get_ordered_ops();
-            for (auto& node : nodes)
-            {
-                auto parameter = std::dynamic_pointer_cast<ngraph::op::Parameter>(node);
-                if (parameter && parameter->get_element_type() == ngraph::element::f32)
-                {
-                    parameter->set_element_type(ngraph::element::f16);
-                }
-                auto constant = std::dynamic_pointer_cast<ngraph::op::Constant>(node);
-                if (constant && constant->get_element_type() == ngraph::element::f32)
-                {
-                    const float* floatsData = constant->get_data_ptr<float>();
-                    size_t total = ngraph::shape_size(constant->get_shape());
-                    Mat floats(1, total, CV_32F, (void*)floatsData);
-                    Mat halfs;
-                    cv::convertFp16(floats, halfs);
-
-                    auto new_const = std::make_shared<ngraph::op::Constant>(ngraph::element::f16, constant->get_shape(), halfs.data);
-                    new_const->set_friendly_name(constant->get_friendly_name());
-                    ngraph::replace_node(constant, new_const);
-                }
-            }
-            ngraph_function->validate_nodes_and_infer_types();
-#endif  // OpenVINO >= 2022.1
         }
-        cnn = InferenceEngine::CNNNetwork(ngraph_function);
+        cnn = ngraph_function;
 
         if (DNN_IE_SERIALIZE)
         {
@@ -520,7 +187,7 @@ void InfEngineNgraphNet::init(Target targetId)
             std::string dumpFileNameBase = netImpl_.getDumpFileNameBase();
             try
             {
-                cnn.serialize(dumpFileNameBase + "_ngraph.xml", dumpFileNameBase + "_ngraph.bin");
+                ov::pass::Serialize(dumpFileNameBase + "_ngraph.xml", dumpFileNameBase + "_ngraph.bin").run_on_model(cnn);
             }
             catch (const std::exception& e)
             {
@@ -558,11 +225,9 @@ void InfEngineNgraphNet::init(Target targetId)
             CV_Error(Error::StsNotImplemented, "Unknown target");
     };
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
-    auto model = cnn.getFunction();
-    ov::preprocess::PrePostProcessor ppp(model);
+    ov::preprocess::PrePostProcessor ppp(cnn);
     int i = 0;
-    for (const auto& inp : model->inputs()) {  // TODO: not sure why but ngraph_function->inputs() here causes segfault.
+    for (const auto& inp : cnn->inputs()) {  // TODO: not sure why but ngraph_function->inputs() here causes segfault.
         const std::string& name = inp.get_node()->get_friendly_name();
         auto blobIt = allBlobs.find(name);
         CV_Assert(blobIt != allBlobs.end());
@@ -574,7 +239,7 @@ void InfEngineNgraphNet::init(Target targetId)
     }
 
     i = 0;
-    for (const auto& it : model->outputs())
+    for (const auto& it : cnn->outputs())
     {
         const std::string& name = it.get_node()->get_friendly_name();
         auto blobIt = allBlobs.find(name);
@@ -595,47 +260,21 @@ void InfEngineNgraphNet::init(Target targetId)
 
     ppp.build();
 
-#else
-
-    for (const auto& it : cnn.getInputsInfo())
-    {
-        const std::string& name = it.first;
-        auto blobIt = allBlobs.find(name);
-        CV_Assert(blobIt != allBlobs.end());
-        it.second->setPrecision(blobIt->second->getTensorDesc().getPrecision());
-    }
-
-    for (const auto& it : cnn.getOutputsInfo())
-    {
-        const std::string& name = it.first;
-        auto blobIt = allBlobs.find(name);
-        CV_Assert(blobIt != allBlobs.end());
-        InferenceEngine::TensorDesc& desc = blobIt->second->getTensorDesc();
-
-        auto outShape = it.second->getDims();
-        if (outShape != desc.getDims()) {
-            desc.reshape(outShape, estimateLayout(outShape.size()));
-        }
-
-        it.second->setPrecision(blobIt->second->getTensorDesc().getPrecision());  // Should be always FP32
-    }
-#endif // OpenVINO >= 2022.1
-
     initPlugin(cnn);
 }
 
-ngraph::ParameterVector InfEngineNgraphNet::setInputs(const std::vector<cv::Mat>& inputs,
+ov::ParameterVector InfEngineNgraphNet::setInputs(const std::vector<cv::Mat>& inputs,
                                    const std::vector<std::string>& names) {
     CV_Assert_N(inputs.size() == names.size());
-    ngraph::ParameterVector current_inp;
+    ov::ParameterVector current_inp;
     for (size_t i = 0; i < inputs.size(); i++)
     {
         std::vector<size_t> shape = getShape<size_t>(inputs[i]);
-        auto inp = std::make_shared<ngraph::op::Parameter>(ngraph::element::f32, ngraph::Shape(shape));
+        auto inp = std::make_shared<ov::op::v0::Parameter>(ov::element::f32, ov::Shape(shape));
         inp->set_friendly_name(names[i]);
 
         auto it = std::find_if(inputs_vec.begin(), inputs_vec.end(),
-                                [&inp](const std::shared_ptr<ngraph::op::Parameter>& a) {
+                                [&inp](const std::shared_ptr<ov::op::v0::Parameter>& a) {
                                 return a->get_friendly_name() == inp->get_friendly_name();
                   });
         if (it == inputs_vec.end()) {
@@ -649,14 +288,14 @@ ngraph::ParameterVector InfEngineNgraphNet::setInputs(const std::vector<cv::Mat>
 }
 
 
-void InfEngineNgraphNet::initPlugin(InferenceEngine::CNNNetwork& net)
+void InfEngineNgraphNet::initPlugin(std::shared_ptr<ov::Model>& net)
 {
     CV_Assert(!isInitialized());
 
     try
     {
         AutoLock lock(getInitializationMutex());
-        InferenceEngine::Core& ie = getCore(device_name);
+        ov::Core& ie = getCore(device_name);
         {
             isInit = true;
             std::vector<std::string> candidates;
@@ -671,18 +310,7 @@ void InfEngineNgraphNet::initPlugin(InferenceEngine::CNNNetwork& net)
                 const std::string& libName = candidates[i];
                 try
                 {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
                     ie.add_extension(libName);
-#else
-                    InferenceEngine::IExtensionPtr extension =
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2021_4)
-                        std::make_shared<InferenceEngine::Extension>(libName);
-#else
-                        InferenceEngine::make_so_pointer<InferenceEngine::IExtension>(libName);
-#endif
-
-                    ie.AddExtension(extension, "CPU");
-#endif
                     CV_LOG_INFO(NULL, "DNN-IE: Loaded extension plugin: " << libName);
                     found = true;
                     break;
@@ -693,30 +321,11 @@ void InfEngineNgraphNet::initPlugin(InferenceEngine::CNNNetwork& net)
             {
                 CV_LOG_WARNING(NULL, "DNN-IE: Can't load extension plugin (extra layers for some networks). Specify path via OPENCV_DNN_IE_EXTRA_PLUGIN_PATH parameter");
             }
-#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2022_1)
-            // Some of networks can work without a library of extra layers.
-            // OpenCV fallbacks as extensions.
-            try
-            {
-                ie.AddExtension(std::make_shared<InfEngineNgraphExtension>(), "CPU");
-            }
-            catch(const std::exception& e)
-            {
-                CV_LOG_INFO(NULL, "DNN-IE: Can't register OpenCV custom layers nGraph extension: " << e.what());
-            }
-#endif // OpenVINO < 2022.1
 #ifndef _WIN32
             // Limit the number of CPU threads.
             if (device_name == "CPU")
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
                 ie.set_property(device_name, ov::inference_num_threads(getNumThreads()));
-#else
-                ie.SetConfig({{
-                    InferenceEngine::PluginConfigParams::KEY_CPU_THREADS_NUM, format("%d", getNumThreads()),
-                }}, device_name);
-#endif // OpenVINO >= 2022.1
 #endif
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2021_2)
             if (device_name.find("GPU") == 0)
             {
 #if OPENCV_HAVE_FILESYSTEM_SUPPORT
@@ -727,24 +336,13 @@ void InfEngineNgraphNet::initPlugin(InferenceEngine::CNNNetwork& net)
                 if (!cache_path.empty() && cache_path != "disabled")
                 {
                     CV_LOG_INFO(NULL, "OpenCV/nGraph: using GPU kernels cache: " << cache_path);
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
                     ie.set_property(device_name, ov::cache_dir(cache_path));
-#else
-                    ie.SetConfig({{
-                        InferenceEngine::PluginConfigParams::KEY_CACHE_DIR, cache_path,
-                    }}, device_name);
-#endif // OpenVINO >= 2022.1
                 }
             }
-#endif
         }
-        std::map<std::string, std::string> config;
+        ov::AnyMap config;
         if (device_name == "MYRIAD" || device_name == "HDDL") {
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
             config.emplace("MYRIAD_DETECT_NETWORK_BATCH", "NO");
-#else
-            config.emplace("VPU_DETECT_NETWORK_BATCH", "NO");
-#endif
         }
 
         bool isHetero = device_name == "FPGA";
@@ -752,7 +350,7 @@ void InfEngineNgraphNet::initPlugin(InferenceEngine::CNNNetwork& net)
         // We do not check IR models because they can be with version less than IRv10
         if (!isHetero && device_name != "CPU" && !hasNetOwner)
         {
-            for (auto& node : net.getFunction()->get_ops())
+            for (auto& node : net->get_ops())
             {
                 if (node->description() == kOpenCVLayersType)
                 {
@@ -764,7 +362,7 @@ void InfEngineNgraphNet::initPlugin(InferenceEngine::CNNNetwork& net)
 
         std::string ieDevice = isHetero ? ("HETERO:" + device_name + ",CPU") : device_name;
         CV_LOG_INFO(NULL, "DNN/IE: Calling LoadNetwork(device=" << ieDevice << ")...");
-        netExec = ie.LoadNetwork(net, ieDevice, config);
+        netExec = ie.compile_model(net, ieDevice, config);
     }
     catch (const std::exception& ex)
     {
@@ -782,14 +380,13 @@ bool NgraphBackendLayer::getMemoryShapes(const std::vector<MatShape> &inputs,
                                             std::vector<MatShape> &outputs,
                                             std::vector<MatShape> &internals) const
 {
-    auto ngraphFunction = t_net.getFunction();
     bool equal_flag = true;
-    std::map<std::string, std::vector<size_t> > inShapes;
+    std::map<std::string, ov::PartialShape> inShapes;
     int i = 0;
-    for (const auto& inp : ngraphFunction->get_parameters())
+    for (const auto& inp : t_net->get_parameters())
     {
-        std::vector<size_t> oldShape = inp->get_shape();
-        std::vector<size_t> newShape(inputs[i].begin(), inputs[i].end());
+        ov::Shape oldShape = inp->get_shape();
+        ov::Shape newShape(inputs[i].begin(), inputs[i].end());
         inShapes.insert({inp->get_friendly_name(), newShape});
         if (oldShape != newShape)
         {
@@ -800,21 +397,17 @@ bool NgraphBackendLayer::getMemoryShapes(const std::vector<MatShape> &inputs,
 
     if (!equal_flag)
     {
-        InferenceEngine::CNNNetwork curr_t_net(t_net);
-        curr_t_net.reshape(inShapes);
+        std::shared_ptr<ov::Model> curr_t_net(t_net);
+        curr_t_net->reshape(inShapes);
     }
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
     std::vector<size_t> dims;
-    for (const auto& it : ngraphFunction->outputs()) {
+    for (const auto& it : t_net->outputs()) {
         if (it.get_node()->get_friendly_name() == name) {
             dims = it.get_partial_shape().get_max_shape();
         }
     }
     if (dims.empty())
         CV_Error(Error::StsError, format("Unable find result with name %s", name.c_str()));
-#else
-    std::vector<size_t> dims = t_net.getOutputsInfo()[name]->getDims();
-#endif
     outputs.push_back(MatShape(dims.begin(), dims.end()));
     return false;
 }
@@ -832,8 +425,6 @@ void NgraphBackendLayer::forward(InputArrayOfArrays inputs, OutputArrayOfArrays
     CV_Error(Error::StsInternal, "Choose Inference Engine as a preferable backend.");
 }
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
-
 ov::Tensor wrapToNgraphBlob(const Mat& m) {
     std::vector<size_t> shape = getShape<size_t>(m);
     if (m.type() == CV_32F)
@@ -848,60 +439,6 @@ ov::Tensor wrapToNgraphBlob(const Mat& m) {
         CV_Error(Error::StsNotImplemented, format("Unsupported data type %s", typeToString(m.type()).c_str()));
 }
 
-#else
-
-static InferenceEngine::Layout estimateLayout(int dims)
-{
-    if (dims == 4)
-        return InferenceEngine::Layout::NCHW;
-    else if (dims == 3)
-        return InferenceEngine::Layout::CHW;
-    else if (dims == 2)
-        return InferenceEngine::Layout::NC;
-    else if (dims == 1)
-        return InferenceEngine::Layout::C;
-    else if (dims == 5)
-        return InferenceEngine::Layout::NCDHW;
-    else
-        return InferenceEngine::Layout::ANY;
-}
-static inline
-InferenceEngine::Layout estimateLayout(size_t dims)
-{
-    return estimateLayout((int)dims);
-}
-
-static inline
-InferenceEngine::Layout estimateLayout(const Mat& m)
-{
-    return estimateLayout(m.dims);
-}
-
-InferenceEngine::Blob::Ptr wrapToNgraphBlob(const Mat& m, const std::vector<size_t>& shape,
-                                               InferenceEngine::Layout layout)
-{
-    if (m.type() == CV_32F)
-        return InferenceEngine::make_shared_blob<float>(
-               {InferenceEngine::Precision::FP32, shape, layout}, (float*)m.data);
-    else if (m.type() == CV_8U)
-        return InferenceEngine::make_shared_blob<uint8_t>(
-               {InferenceEngine::Precision::U8, shape, layout}, (uint8_t*)m.data);
-    else if (m.type() == CV_32SC1)
-        return InferenceEngine::make_shared_blob<int32_t>(
-               {InferenceEngine::Precision::I32, shape, layout}, (int32_t*)m.data);
-    else
-        CV_Error(Error::StsNotImplemented, format("Unsupported data type %s", typeToString(m.type()).c_str()));
-}
-
-InferenceEngine::Blob::Ptr wrapToNgraphBlob(const Mat& m, InferenceEngine::Layout layout)
-{
-    std::vector<size_t> shape = getShape<size_t>(m);
-    return wrapToNgraphBlob(m, shape, layout);
-}
-
-InferenceEngine::Blob::Ptr wrapToNgraphBlob(const Mat& m) { return wrapToNgraphBlob(m, estimateLayout(m)); }
-
-#endif // OpenVINO >= 2022.1
 
 NgraphBackendWrapper::NgraphBackendWrapper(int targetId, const cv::Mat& m)
     : BackendWrapper(DNN_BACKEND_INFERENCE_ENGINE_NGRAPH, targetId)
@@ -941,36 +478,10 @@ void NgraphBackendWrapper::setHostDirty()
     //CV_Error(Error::StsNotImplemented, "");
 }
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
 ov::Tensor copyBlob(const ov::Tensor& blob)
 {
     return ov::Tensor(blob.get_element_type(), blob.get_shape());
 }
-#else
-InferenceEngine::Blob::Ptr copyBlob(const InferenceEngine::Blob::Ptr& blob)
-{
-    InferenceEngine::Blob::Ptr copy;
-    auto description = blob->getTensorDesc();
-    InferenceEngine::Precision precision = description.getPrecision();
-    if (precision == InferenceEngine::Precision::FP32)
-    {
-        copy = InferenceEngine::make_shared_blob<float>(description);
-    }
-    else if (precision == InferenceEngine::Precision::U8)
-    {
-        copy = InferenceEngine::make_shared_blob<uint8_t>(description);
-    }
-    else
-    {
-        std::ostringstream msg;
-        msg << precision;
-        CV_Error_(Error::StsNotImplemented, ("Unsupported blob precision: %s", msg.str().c_str()));
-    }
-    copy->allocate();
-    return copy;
-}
-
-#endif // OpenVINO < 2022.1
 
 void InfEngineNgraphNet::reset()
 {
@@ -1022,7 +533,7 @@ void InfEngineNgraphNet::forward(const std::vector<Ptr<BackendWrapper> >& outBlo
         reqWrapper = Ptr<NgraphReqWrapper>(new NgraphReqWrapper());
         try
         {
-            reqWrapper->req = netExec.CreateInferRequest();
+            reqWrapper->req = netExec.create_infer_request();
         }
         catch (const std::exception& ex)
         {
@@ -1030,7 +541,6 @@ void InfEngineNgraphNet::forward(const std::vector<Ptr<BackendWrapper> >& outBlo
         }
         infRequests.push_back(reqWrapper);
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
         int i = 0;
         for (const auto& it : netExec.inputs())
         {
@@ -1048,27 +558,7 @@ void InfEngineNgraphNet::forward(const std::vector<Ptr<BackendWrapper> >& outBlo
             CV_Assert(blobIt != allBlobs.end());
             reqWrapper->req.set_output_tensor(i++, isAsync ? copyBlob(blobIt->second) : blobIt->second);
         }
-#else
-        InferenceEngine::BlobMap inpBlobs, outBlobs;
-        for (const auto& it : cnn.getInputsInfo())
-        {
-            const std::string& name = it.first;
-            auto blobIt = allBlobs.find(name);
-            CV_Assert(blobIt != allBlobs.end());
-            inpBlobs[name] = isAsync ? copyBlob(blobIt->second) : blobIt->second;
-        }
-        for (const auto& it : cnn.getOutputsInfo())
-        {
-            const std::string& name = it.first;
-            auto blobIt = allBlobs.find(name);
-            CV_Assert(blobIt != allBlobs.end());
-            outBlobs[name] = isAsync ? copyBlob(blobIt->second) : blobIt->second;
-        }
-        reqWrapper->req.SetInput(inpBlobs);
-        reqWrapper->req.SetOutput(outBlobs);
-#endif
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
     if (isAsync) {
         bool* isReady = &reqWrapper->isReady;
         auto* promises = &reqWrapper->outProms;
@@ -1112,86 +602,13 @@ void InfEngineNgraphNet::forward(const std::vector<Ptr<BackendWrapper> >& outBlo
             *isReady = true;
         });
     }
-#else // OpenVINO >= 2022.1
-
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2021_4)
-        InferenceEngine::InferRequest infRequest = reqWrapper->req;
-        NgraphReqWrapper* wrapperPtr = reqWrapper.get();
-        CV_Assert(wrapperPtr && "Internal error");
-#else
-        InferenceEngine::IInferRequest::Ptr infRequestPtr = reqWrapper->req;
-        CV_Assert(infRequestPtr);
-        InferenceEngine::IInferRequest& infRequest = *infRequestPtr.get();
-        infRequest.SetUserData(reqWrapper.get(), 0);
-#endif
-
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2021_4)
-        // do NOT capture 'reqWrapper' (smart ptr) in the lambda callback
-        infRequest.SetCompletionCallback<std::function<void(InferenceEngine::InferRequest, InferenceEngine::StatusCode)>>(
-            [wrapperPtr](InferenceEngine::InferRequest /*request*/, InferenceEngine::StatusCode status)
-#else
-        infRequest.SetCompletionCallback(
-            [](InferenceEngine::IInferRequest::Ptr requestPtr, InferenceEngine::StatusCode status)
-#endif
-            {
-                CV_LOG_DEBUG(NULL, "DNN(nGraph): completionCallback(" << (int)status << ")");
-#if !INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2021_4)
-                CV_Assert(requestPtr);
-                InferenceEngine::IInferRequest& request = *requestPtr.get();
-
-                NgraphReqWrapper* wrapperPtr;
-                request.GetUserData((void**)&wrapperPtr, 0);
-                CV_Assert(wrapperPtr && "Internal error");
-#endif
-                NgraphReqWrapper& wrapper = *wrapperPtr;
-
-                size_t processedOutputs = 0;
-                try
-                {
-                    for (; processedOutputs < wrapper.outProms.size(); ++processedOutputs)
-                    {
-                        const std::string& name = wrapper.outsNames[processedOutputs];
-                        Mat m = infEngineBlobToMat(wrapper.req.GetBlob(name));
-
-                        try
-                        {
-                            CV_Assert(status == InferenceEngine::StatusCode::OK);
-                            wrapper.outProms[processedOutputs].setValue(m.clone());
-                        }
-                        catch (...)
-                        {
-                            try {
-                                wrapper.outProms[processedOutputs].setException(std::current_exception());
-                            } catch(...) {
-                                CV_LOG_ERROR(NULL, "DNN: Exception occurred during async inference exception propagation");
-                            }
-                        }
-                    }
-                }
-                catch (...)
-                {
-                    std::exception_ptr e = std::current_exception();
-                    for (; processedOutputs < wrapper.outProms.size(); ++processedOutputs)
-                    {
-                        try {
-                            wrapper.outProms[processedOutputs].setException(e);
-                        } catch(...) {
-                            CV_LOG_ERROR(NULL, "DNN: Exception occurred during async inference exception propagation");
-                        }
-                    }
-                }
-                wrapper.isReady = true;
-            }
-        );
-#endif // OpenVINO >= 2022.1
     }
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
     if (isAsync)
     {
         // Copy actual data to infer request's input blobs.
         int i = 0;
-        for (const auto& it : cnn.getFunction()->get_parameters())
+        for (const auto& it : cnn->get_parameters())
         {
             const std::string& name = it->get_friendly_name();
             auto blobIt = allBlobs.find(name);
@@ -1210,54 +627,30 @@ void InfEngineNgraphNet::forward(const std::vector<Ptr<BackendWrapper> >& outBlo
     {
         reqWrapper->req.infer();
     }
-#else
-    if (isAsync)
-    {
-        // Copy actual data to infer request's input blobs.
-        for (const auto& it : cnn.getInputsInfo())
-        {
-            const std::string& name = it.first;
-            auto blobIt = allBlobs.find(name);
-            Mat srcMat = infEngineBlobToMat(blobIt->second);
-            Mat dstMat = infEngineBlobToMat(reqWrapper->req.GetBlob(name));
-            srcMat.copyTo(dstMat);
-        }
-
-        // Set promises to output blobs wrappers.
-        reqWrapper->makePromises(outBlobsWrappers);
-
-        reqWrapper->isReady = false;
-        reqWrapper->req.StartAsync();
-    }
-    else
-    {
-        reqWrapper->req.Infer();
-    }
-#endif // OpenVINO >= 2022.1
 }
 
-ngraph::Output<ngraph::Node> ngraphQuantize(ngraph::Output<ngraph::Node> input, float output_sc, float output_zp) {
+ov::Output<ov::Node> ngraphQuantize(ov::Output<ov::Node> input, float output_sc, float output_zp) {
     float outLow = -128, outHigh = 127;
     float inpLow = output_sc * (outLow - output_zp);
     float inpHigh = output_sc * (outHigh - output_zp);
-    return std::make_shared<ngraph::op::FakeQuantize>(input,
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &inpLow),
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &inpHigh),
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &outLow),
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &outHigh),
+    return std::make_shared<ov::op::v0::FakeQuantize>(input,
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &inpLow),
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &inpHigh),
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &outLow),
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &outHigh),
         256 // levels
     );
 }
 
-ngraph::Output<ngraph::Node> ngraphDequantize(ngraph::Output<ngraph::Node> input, float input_sc, float input_zp) {
+ov::Output<ov::Node> ngraphDequantize(ov::Output<ov::Node> input, float input_sc, float input_zp) {
     float inpLow = -128, inpHigh = 127;
     float outLow = input_sc * (inpLow - input_zp);
     float outHigh = input_sc * (inpHigh - input_zp);
-    return std::make_shared<ngraph::op::FakeQuantize>(input,
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &inpLow),
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &inpHigh),
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &outLow),
-        std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &outHigh),
+    return std::make_shared<ov::op::v0::FakeQuantize>(input,
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &inpLow),
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &inpHigh),
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &outLow),
+        std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &outHigh),
         256 // levels
     );
 }
diff --git a/modules/dnn/src/ie_ngraph.hpp b/modules/dnn/src/ie_ngraph.hpp
index 8672f1a3c2..19e07d62ac 100644
--- a/modules/dnn/src/ie_ngraph.hpp
+++ b/modules/dnn/src/ie_ngraph.hpp
@@ -17,7 +17,8 @@
 #pragma warning(disable : 4245)
 #pragma warning(disable : 4268)
 #endif
-#include <ngraph/ngraph.hpp>
+#include <openvino/openvino.hpp>
+#include <openvino/op/ops.hpp>
 #ifdef _MSC_VER
 #pragma warning(pop)
 #endif
@@ -30,12 +31,11 @@ namespace cv { namespace dnn {
 
 class InfEngineNgraphNode;
 
-
 class InfEngineNgraphNet
 {
 public:
     InfEngineNgraphNet(detail::NetImplBase& netImpl);
-    InfEngineNgraphNet(detail::NetImplBase& netImpl, InferenceEngine::CNNNetwork& net);
+    InfEngineNgraphNet(detail::NetImplBase& netImpl, std::shared_ptr<ov::Model>& net);
 
     void addOutput(const Ptr<InfEngineNgraphNode>& node);
 
@@ -44,8 +44,8 @@ public:
 
     void forward(const std::vector<Ptr<BackendWrapper> >& outBlobsWrappers, bool isAsync);
 
-    void initPlugin(InferenceEngine::CNNNetwork& net);
-    ngraph::ParameterVector setInputs(const std::vector<cv::Mat>& inputs, const std::vector<std::string>& names);
+    void initPlugin(std::shared_ptr<ov::Model>& net);
+    ov::ParameterVector setInputs(const std::vector<cv::Mat>& inputs, const std::vector<std::string>& names);
 
     void addBlobs(const std::vector<cv::Ptr<BackendWrapper> >& ptrs);
 
@@ -56,15 +56,11 @@ public:
 //private:
     detail::NetImplBase& netImpl_;
 
-    ngraph::ParameterVector inputs_vec;
-    std::shared_ptr<ngraph::Function> ngraph_function;
+    ov::ParameterVector inputs_vec;
+    std::shared_ptr<ov::Model> ngraph_function;
 
-    InferenceEngine::ExecutableNetwork netExec;
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
+    ov::CompiledModel netExec;
     std::map<std::string, ov::Tensor> allBlobs;
-#else
-    InferenceEngine::BlobMap allBlobs;
-#endif
     std::string device_name;
     bool isInit = false;
 
@@ -74,14 +70,14 @@ public:
 
         void makePromises(const std::vector<Ptr<BackendWrapper> >& outs);
 
-        InferenceEngine::InferRequest req;
+        ov::InferRequest req;
         std::vector<cv::AsyncPromise> outProms;
         std::vector<std::string> outsNames;
         bool isReady;
     };
     std::vector<Ptr<NgraphReqWrapper> > infRequests;
 
-    InferenceEngine::CNNNetwork cnn;
+    std::shared_ptr<ov::Model> cnn;
     bool hasNetOwner;
     std::unordered_map<std::string, InfEngineNgraphNode*> requestedOutputs;
 };
@@ -93,13 +89,13 @@ public:
                         std::vector<Mat*>& inputs, std::vector<Mat>& outputs,
                         std::vector<Mat>& internals);
 
-    InfEngineNgraphNode(ngraph::Output<ngraph::Node>&& _node);
-    InfEngineNgraphNode(const ngraph::Output<ngraph::Node>& _node);
+    InfEngineNgraphNode(ov::Output<ov::Node>&& _node);
+    InfEngineNgraphNode(const ov::Output<ov::Node>& _node);
 
     void setName(const std::string& name);
 
     // Inference Engine network object that allows to obtain the outputs of this layer.
-    ngraph::Output<ngraph::Node> node;
+    ov::Output<ov::Node> node;
     Ptr<InfEngineNgraphNet> net;
     Ptr<dnn::Layer> cvLayer;
 };
@@ -118,11 +114,7 @@ public:
 
     Mat* host;
     std::string name;
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
     ov::Tensor blob;
-#else
-    InferenceEngine::Blob::Ptr blob;
-#endif
     AsyncArray futureMat;
 };
 
@@ -132,7 +124,7 @@ public:
 class NgraphBackendLayer : public Layer
 {
 public:
-    NgraphBackendLayer(const InferenceEngine::CNNNetwork &t_net_) : t_net(t_net_) {};
+    NgraphBackendLayer(const std::shared_ptr<ov::Model> &t_net_) : t_net(t_net_) {};
 
     virtual bool getMemoryShapes(const std::vector<MatShape> &inputs,
                                  const int requiredOutputs,
@@ -145,11 +137,11 @@ public:
     virtual bool supportBackend(int backendId) CV_OVERRIDE;
 
 private:
-    InferenceEngine::CNNNetwork t_net;
+    std::shared_ptr<ov::Model> t_net;
 };
 
-ngraph::Output<ngraph::Node> ngraphQuantize(ngraph::Output<ngraph::Node> input, float output_sc, float output_zp);
-ngraph::Output<ngraph::Node> ngraphDequantize(ngraph::Output<ngraph::Node> input, float input_sc, float input_zp);
+ov::Output<ov::Node> ngraphQuantize(ov::Output<ov::Node> input, float output_sc, float output_zp);
+ov::Output<ov::Node> ngraphDequantize(ov::Output<ov::Node> input, float input_sc, float input_zp);
 
 #endif  // HAVE_DNN_NGRAPH
 
diff --git a/modules/dnn/src/int8layers/batch_norm_layer.cpp b/modules/dnn/src/int8layers/batch_norm_layer.cpp
index 7ef169deea..3fbf8cd191 100644
--- a/modules/dnn/src/int8layers/batch_norm_layer.cpp
+++ b/modules/dnn/src/int8layers/batch_norm_layer.cpp
@@ -7,8 +7,6 @@
 #include "../op_timvx.hpp"
 #include "../ie_ngraph.hpp"
 
-#include <opencv2/dnn/shape_utils.hpp>
-
 namespace cv
 {
 namespace dnn
@@ -250,11 +248,11 @@ public:
         std::vector<size_t> shape(input.get_shape().size(), 1);
         shape[1] = origin_weights.total();
 
-        ngraph::Output<ngraph::Node> res;
-        auto ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, shape, origin_weights.data);
-        auto ieBias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, shape, origin_bias.data);
-        res = std::make_shared<ngraph::op::v1::Multiply>(input, ieWeights);
-        res = std::make_shared<ngraph::op::v1::Add>(res, ieBias);
+        ov::Output<ov::Node> res;
+        auto ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, shape, origin_weights.data);
+        auto ieBias = std::make_shared<ov::op::v0::Constant>(ov::element::f32, shape, origin_bias.data);
+        res = std::make_shared<ov::op::v1::Multiply>(input, ieWeights);
+        res = std::make_shared<ov::op::v1::Add>(res, ieBias);
 
         res = ngraphQuantize(res, output_sc, output_zp);
         return new InfEngineNgraphNode(res);
diff --git a/modules/dnn/src/int8layers/convolution_layer.cpp b/modules/dnn/src/int8layers/convolution_layer.cpp
index ba9b31fe35..2695cb6d56 100644
--- a/modules/dnn/src/int8layers/convolution_layer.cpp
+++ b/modules/dnn/src/int8layers/convolution_layer.cpp
@@ -573,8 +573,8 @@ public:
         auto ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         std::vector<size_t> dims = ieInpNode.get_shape();
         CV_Check(dims.size(), dims.size() >= 3 && dims.size() <= 5, "");
-        CV_Assert(ieInpNode.get_element_type() == ngraph::element::f32);
-        ngraph::Output<ngraph::Node> ieWeights;
+        CV_Assert(ieInpNode.get_element_type() == ov::element::f32);
+        ov::Output<ov::Node> ieWeights;
         if (nodes.size() > 1)
             ieWeights = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
         const int inpCn = dims[1];
@@ -592,18 +592,18 @@ public:
 
         if (nodes.size() == 1)
         {
-            ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::i8, kernel_shape, blobs[0].data);
+            ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::i8, kernel_shape, blobs[0].data);
         }
         else
         {
-            auto shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                             ngraph::Shape{kernel_shape.size()}, std::vector<int64_t>(kernel_shape.begin(), kernel_shape.end()));
-            ieWeights  = std::make_shared<ngraph::op::v1::Reshape>(ieWeights, shape, true);
+            auto shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                             ov::Shape{kernel_shape.size()}, std::vector<int64_t>(kernel_shape.begin(), kernel_shape.end()));
+            ieWeights  = std::make_shared<ov::op::v1::Reshape>(ieWeights, shape, true);
         }
 
-        ngraph::op::PadType pad_type = ngraph::op::PadType::EXPLICIT;
+        ov::op::PadType pad_type = ov::op::PadType::EXPLICIT;
         if (!padMode.empty())
-            pad_type = padMode == "VALID" ? ngraph::op::PadType::VALID : ngraph::op::PadType::SAME_UPPER;
+            pad_type = padMode == "VALID" ? ov::op::PadType::VALID : ov::op::PadType::SAME_UPPER;
 
         ieInpNode = ngraphDequantize(ieInpNode, input_sc, input_zp);
 
@@ -627,31 +627,31 @@ public:
             outLows[i] = low * outputMultiplier[i] * output_sc / input_sc;
             outHighs[i] = high * outputMultiplier[i] * output_sc / input_sc;
         }
-        ieWeights = std::make_shared<ngraph::op::Convert>(ieWeights, ngraph::element::f32);
-        ieWeights = std::make_shared<ngraph::op::FakeQuantize>(ieWeights,
-            std::make_shared<ngraph::op::Constant>(ngraph::element::f32, quantShape, inpLows.data()),
-            std::make_shared<ngraph::op::Constant>(ngraph::element::f32, quantShape, inpHighs.data()),
-            std::make_shared<ngraph::op::Constant>(ngraph::element::f32, quantShape, outLows.data()),
-            std::make_shared<ngraph::op::Constant>(ngraph::element::f32, quantShape, outHighs.data()),
+        ieWeights = std::make_shared<ov::op::v0::Convert>(ieWeights, ov::element::f32);
+        ieWeights = std::make_shared<ov::op::v0::FakeQuantize>(ieWeights,
+            std::make_shared<ov::op::v0::Constant>(ov::element::f32, quantShape, inpLows.data()),
+            std::make_shared<ov::op::v0::Constant>(ov::element::f32, quantShape, inpHighs.data()),
+            std::make_shared<ov::op::v0::Constant>(ov::element::f32, quantShape, outLows.data()),
+            std::make_shared<ov::op::v0::Constant>(ov::element::f32, quantShape, outHighs.data()),
             256 // levels
         );
 
-        ngraph::Output<ngraph::Node> conv_node;
+        ov::Output<ov::Node> conv_node;
         if (group != 1) {
-            conv_node = std::make_shared<ngraph::op::v1::GroupConvolution>(
+            conv_node = std::make_shared<ov::op::v1::GroupConvolution>(
                                 ieInpNode, ieWeights,
-                                ngraph::Strides(strides),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(),   pads_end.end())),
-                                ngraph::Strides(dilations),
+                                ov::Strides(strides),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(),   pads_end.end())),
+                                ov::Strides(dilations),
                                 pad_type);
         } else {
-            conv_node = std::make_shared<ngraph::op::v1::Convolution>(
+            conv_node = std::make_shared<ov::op::v1::Convolution>(
                                 ieInpNode, ieWeights,
-                                ngraph::Strides(strides),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(), pads_end.end())),
-                                ngraph::Strides(dilations),
+                                ov::Strides(strides),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(), pads_end.end())),
+                                ov::Strides(dilations),
                                 pad_type);
         }
 
@@ -659,12 +659,12 @@ public:
         shape[1] = conv_node.get_shape()[1];
         if (biasvec.size() || nodes.size() == 3)
         {
-            std::shared_ptr<ngraph::Node> bias;
+            std::shared_ptr<ov::Node> bias;
             if (nodes.size() == 3)
             {
-                auto bias_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                    ngraph::Shape{shape.size()}, std::vector<int64_t>(shape.begin(), shape.end()));
-                bias = std::make_shared<ngraph::op::v1::Reshape>(nodes[2].dynamicCast<InfEngineNgraphNode>()->node, bias_shape, true);
+                auto bias_shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                    ov::Shape{shape.size()}, std::vector<int64_t>(shape.begin(), shape.end()));
+                bias = std::make_shared<ov::op::v1::Reshape>(nodes[2].dynamicCast<InfEngineNgraphNode>()->node, bias_shape, true);
             }
             else
             {
@@ -672,9 +672,9 @@ public:
                 for (int i = 0; i < numOutput; ++i) {
                     ovBias[i] = (biasvec[i] + input_zp * cv::sum(blobs[0].row(i))[0]) * outputMultiplier[i] * output_sc;
                 }
-                bias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape(shape), ovBias.data());
+                bias = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape(shape), ovBias.data());
             }
-            conv_node = std::make_shared<ngraph::op::v1::Add>(conv_node, bias, ngraph::op::AutoBroadcastType::NUMPY);
+            conv_node = std::make_shared<ov::op::v1::Add>(conv_node, bias, ov::op::AutoBroadcastType::NUMPY);
         }
 
         conv_node = ngraphQuantize(conv_node, output_sc, output_zp);
diff --git a/modules/dnn/src/int8layers/elementwise_layers.cpp b/modules/dnn/src/int8layers/elementwise_layers.cpp
index 2f40a4039f..f522efa0c1 100644
--- a/modules/dnn/src/int8layers/elementwise_layers.cpp
+++ b/modules/dnn/src/int8layers/elementwise_layers.cpp
@@ -253,24 +253,26 @@ public:
 
         input = ngraphDequantize(input, input_sc, input_zp);
 
-        ngraph::Output<ngraph::Node> res;
+        ov::Output<ov::Node> res;
         if (type == "ReLU6Int8") {
-            res = std::make_shared<ngraph::op::Clamp>(input, 0.0f, 6.0f);
+            res = std::make_shared<ov::op::v0::Clamp>(input, 0.0f, 6.0f);
         } else if (type == "ReLUInt8") {
             if (slope) {
-                auto param = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &slope);
-                res = std::make_shared<ngraph::op::PRelu>(input, param);
+                auto param = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &slope);
+                res = std::make_shared<ov::op::v0::PRelu>(input, param);
             } else {
-                res = std::make_shared<ngraph::op::Relu>(input);
+                res = std::make_shared<ov::op::v0::Relu>(input);
             }
         } else if (type == "ELUInt8") {
-            res = std::make_shared<ngraph::op::Elu>(input, 1.0f);
+            res = std::make_shared<ov::op::v0::Elu>(input, 1.0f);
         } else if (type == "MishInt8") {
-            res = std::make_shared<ngraph::op::v4::Mish>(input);
+            res = std::make_shared<ov::op::v4::Mish>(input);
+        } else if (type == "HardSwishInt8") {
+            res = std::make_shared<ov::op::v4::HSwish>(input);
         } else if (type == "AbsValInt8") {
-            res = std::make_shared<ngraph::op::Abs>(input);
+            res = std::make_shared<ov::op::v0::Abs>(input);
         } else if (type == "SigmoidInt8") {
-            res = std::make_shared<ngraph::op::Sigmoid>(input);
+            res = std::make_shared<ov::op::v0::Sigmoid>(input);
         } else {
             CV_Error(Error::StsNotImplemented, type + " activation with OpenVINO");
         }
diff --git a/modules/dnn/src/int8layers/eltwise_layer.cpp b/modules/dnn/src/int8layers/eltwise_layer.cpp
index a3bb6ec2d6..214d11525a 100644
--- a/modules/dnn/src/int8layers/eltwise_layer.cpp
+++ b/modules/dnn/src/int8layers/eltwise_layer.cpp
@@ -375,7 +375,7 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         CV_Assert(nodes.size() >= 2);
-        std::vector<ngraph::Output<ngraph::Node>> ieInpNodes(nodes.size());
+        std::vector<ov::Output<ov::Node>> ieInpNodes(nodes.size());
         for (size_t i = 0; i < nodes.size(); i++)
         {
             ieInpNodes[i] = nodes[i].dynamicCast<InfEngineNgraphNode>()->node;
@@ -389,9 +389,9 @@ public:
         for (size_t i = 1; i < ieInpNodes.size(); i++)
         {
             switch (op) {
-                case SUM:  res = std::make_shared<ngraph::op::v1::Add>(res, ieInpNodes[i]); break;
-                case PROD: res = std::make_shared<ngraph::op::v1::Multiply>(res, ieInpNodes[i]); break;
-                case MAX:  res = std::make_shared<ngraph::op::v1::Maximum>(res, ieInpNodes[i]); break;
+                case SUM:  res = std::make_shared<ov::op::v1::Add>(res, ieInpNodes[i]); break;
+                case PROD: res = std::make_shared<ov::op::v1::Multiply>(res, ieInpNodes[i]); break;
+                case MAX:  res = std::make_shared<ov::op::v1::Maximum>(res, ieInpNodes[i]); break;
                 default: CV_Error(Error::StsNotImplemented, "Unsupported eltwise operation");
             }
         }
diff --git a/modules/dnn/src/int8layers/fully_connected_layer.cpp b/modules/dnn/src/int8layers/fully_connected_layer.cpp
index ba5b0d79c1..5d13ba8f29 100644
--- a/modules/dnn/src/int8layers/fully_connected_layer.cpp
+++ b/modules/dnn/src/int8layers/fully_connected_layer.cpp
@@ -405,8 +405,8 @@ public:
         CV_CheckTypeEQ(blobs[1].type(), CV_32S, "");  // bias
         CV_CheckTypeEQ(outputMultiplier.type(), CV_32F, "");
 
-        ngraph::Output<ngraph::Node> input = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        ngraph::Output<ngraph::Node> ieWeights, ieBias, matmul;
+        ov::Output<ov::Node> input = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
+        ov::Output<ov::Node> ieWeights, ieBias, matmul;
         bool transA = false, transB = true;
         size_t numOutput = blobs[0].size[0];
 
@@ -414,15 +414,15 @@ public:
         {
             CV_Error(Error::StsNotImplemented, "");
             // auto inp2 = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-            // matmul = std::make_shared<ngraph::op::MatMul>(ieInpNode, inp2, transA, transB);
+            // matmul = std::make_shared<ov::op::v0::MatMul>(ieInpNode, inp2, transA, transB);
         }
         else
         {
             std::vector<int> shape(1 + normalize_axis(axis, input.get_shape().size()), 0);
             shape[shape.size() - 1] = -1;
-            input = std::make_shared<ngraph::op::v1::Reshape>(
+            input = std::make_shared<ov::op::v1::Reshape>(
                 input,
-                std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{shape.size()}, shape.data()),
+                std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{shape.size()}, shape.data()),
                 true
             );
 
@@ -439,16 +439,16 @@ public:
             }
 
             std::vector<size_t> weight_shape{(size_t)blobs[0].size[0], (size_t)blobs[0].size[1]};
-            ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::i8, weight_shape, blobs[0].data);
-            ieWeights = std::make_shared<ngraph::op::Convert>(ieWeights, ngraph::element::f32);
-            ieWeights = std::make_shared<ngraph::op::FakeQuantize>(ieWeights,
-                std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{numOutput, 1}, inpLows.data()),
-                std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{numOutput, 1}, inpHighs.data()),
-                std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{numOutput, 1}, outLows.data()),
-                std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{numOutput, 1}, outHighs.data()),
+            ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::i8, weight_shape, blobs[0].data);
+            ieWeights = std::make_shared<ov::op::v0::Convert>(ieWeights, ov::element::f32);
+            ieWeights = std::make_shared<ov::op::v0::FakeQuantize>(ieWeights,
+                std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{numOutput, 1}, inpLows.data()),
+                std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{numOutput, 1}, inpHighs.data()),
+                std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{numOutput, 1}, outLows.data()),
+                std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{numOutput, 1}, outHighs.data()),
                 256 // levels
             );
-            matmul = std::make_shared<ngraph::op::MatMul>(input, ieWeights, transA, transB);
+            matmul = std::make_shared<ov::op::v0::MatMul>(input, ieWeights, transA, transB);
         }
 
         if (blobs.size() > 1) {
@@ -457,9 +457,9 @@ public:
             for (int i = 0; i < ovBias.size(); ++i) {
                 ovBias[i] = (bias[i] + input_zp * cv::sum(blobs[0].row(i))[0]) * outputMultiplier.ptr<float>()[i] * output_sc;
             }
-            auto bias_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                            ngraph::Shape{blobs[1].total()}, ovBias.data());
-            matmul = std::make_shared<ngraph::op::v1::Add>(matmul, bias_node);
+            auto bias_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                            ov::Shape{blobs[1].total()}, ovBias.data());
+            matmul = std::make_shared<ov::op::v1::Add>(matmul, bias_node);
         }
 
         matmul = ngraphQuantize(matmul, output_sc, output_zp);
diff --git a/modules/dnn/src/int8layers/pooling_layer.cpp b/modules/dnn/src/int8layers/pooling_layer.cpp
index b321d730f7..cfd04bd2f4 100644
--- a/modules/dnn/src/int8layers/pooling_layer.cpp
+++ b/modules/dnn/src/int8layers/pooling_layer.cpp
@@ -11,6 +11,7 @@
 #include <float.h>
 #include <algorithm>
 #include <numeric>
+
 using std::max;
 using std::min;
 
@@ -284,22 +285,22 @@ public:
 
         input = ngraphDequantize(input, input_sc, input_zp);
 
-        ngraph::op::PadType pad_type = ngraph::op::PadType::EXPLICIT;
+        ov::op::PadType pad_type = ov::op::PadType::EXPLICIT;
         if (!padMode.empty())
-            pad_type = padMode == "VALID" ? ngraph::op::PadType::VALID : ngraph::op::PadType::SAME_UPPER;
+            pad_type = padMode == "VALID" ? ov::op::PadType::VALID : ov::op::PadType::SAME_UPPER;
 
-        auto rounding_type = ceilMode ? ngraph::op::RoundingType::CEIL : ngraph::op::RoundingType::FLOOR;
-        ngraph::Output<ngraph::Node> pool;
+        auto rounding_type = ceilMode ? ov::op::RoundingType::CEIL : ov::op::RoundingType::FLOOR;
+        ov::Output<ov::Node> pool;
         if (type == MAX) {
-            pool = std::make_shared<ngraph::op::v1::MaxPool>(input, ngraph::Strides(strides),
-                        ngraph::Shape(pads_begin), ngraph::Shape(pads_end), ngraph::Shape(kernel_size),
+            pool = std::make_shared<ov::op::v1::MaxPool>(input, ov::Strides(strides),
+                        ov::Shape(pads_begin), ov::Shape(pads_end), ov::Shape(kernel_size),
                         rounding_type, pad_type);
         } else if (type == AVE) {
-            pool = std::make_shared<ngraph::op::v1::AvgPool>(input, ngraph::Strides(strides),
-                        ngraph::Shape(pads_begin), ngraph::Shape(pads_end), ngraph::Shape(kernel_size),
+            pool = std::make_shared<ov::op::v1::AvgPool>(input, ov::Strides(strides),
+                        ov::Shape(pads_begin), ov::Shape(pads_end), ov::Shape(kernel_size),
                         !avePoolPaddedArea, rounding_type, pad_type);
         } else if (type == SUM) {
-            ngraph::Shape inpShape = input.get_shape();
+            ov::Shape inpShape = input.get_shape();
             CV_Assert(inpShape.size() == 2 + kernel_size.size());
             std::vector<int64_t> axes;
             for (size_t i = 0; i < kernel_size.size(); i++)
@@ -307,8 +308,8 @@ public:
                 if (inpShape[2 + i] == kernel_size[i])
                     axes.push_back(2 + i);
             }
-            auto reduction_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes.size()}, axes);
-            pool = std::make_shared<ngraph::op::v1::ReduceSum>(input, reduction_axes, true);
+            auto reduction_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes.size()}, axes);
+            pool = std::make_shared<ov::op::v1::ReduceSum>(input, reduction_axes, true);
         } else {
             CV_Error(Error::StsNotImplemented, format("INT8 Pooling type: %d", type));
         }
diff --git a/modules/dnn/src/int8layers/scale_layer.cpp b/modules/dnn/src/int8layers/scale_layer.cpp
index e50c4cea0e..25d48e3d17 100644
--- a/modules/dnn/src/int8layers/scale_layer.cpp
+++ b/modules/dnn/src/int8layers/scale_layer.cpp
@@ -191,7 +191,7 @@ public:
 #ifdef HAVE_DNN_NGRAPH
     virtual Ptr<BackendNode> initNgraph(const std::vector<Ptr<BackendWrapper> >& inputs, const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
-        std::vector<ngraph::Output<ngraph::Node>> ieInpNodes(nodes.size());
+        std::vector<ov::Output<ov::Node>> ieInpNodes(nodes.size());
         for (int i = 0; i < nodes.size(); ++i) {
             ieInpNodes[i] = nodes[i].dynamicCast<InfEngineNgraphNode>()->node;
         }
@@ -200,7 +200,7 @@ public:
 
         CV_Assert(!blobs.empty() || ieInpNodes.size() == 1 + (int)hasWeights + (int)hasBias);
 
-        ngraph::Output<ngraph::Node> weights, bias;
+        ov::Output<ov::Node> weights, bias;
         if (blobs.empty()) {
             if (hasWeights)
                 weights = ieInpNodes[1];
@@ -222,17 +222,17 @@ public:
             }
 
             if (hasWeights)
-                weights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, shape, blobs[0].data);
+                weights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, shape, blobs[0].data);
             if (hasBias)
-                bias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, shape, blobs[(int)hasWeights].data);
+                bias = std::make_shared<ov::op::v0::Constant>(ov::element::f32, shape, blobs[(int)hasWeights].data);
         }
 
-        ngraph::Output<ngraph::Node> res = ieInpNodes[0];
+        ov::Output<ov::Node> res = ieInpNodes[0];
         if (hasWeights) {
-            res = std::make_shared<ngraph::op::v1::Multiply>(res, weights);
+            res = std::make_shared<ov::op::v1::Multiply>(res, weights);
         }
         if (hasBias) {
-            res = std::make_shared<ngraph::op::v1::Add>(res, bias);
+            res = std::make_shared<ov::op::v1::Add>(res, bias);
         }
 
         res = ngraphQuantize(res, output_sc, output_zp);
diff --git a/modules/dnn/src/int8layers/softmax_layer.cpp b/modules/dnn/src/int8layers/softmax_layer.cpp
index 28c6837cca..e81b82b99f 100644
--- a/modules/dnn/src/int8layers/softmax_layer.cpp
+++ b/modules/dnn/src/int8layers/softmax_layer.cpp
@@ -204,11 +204,11 @@ public:
 
         input = ngraphDequantize(input, input_sc, input_zp);
 
-        ngraph::Output<ngraph::Node> res;
+        ov::Output<ov::Node> res;
         if (logSoftMax) {
-            res = std::make_shared<ngraph::op::v5::LogSoftmax>(input, axis);
+            res = std::make_shared<ov::op::v5::LogSoftmax>(input, axis);
         } else {
-            res = std::make_shared<ngraph::op::v1::Softmax>(input, axis);
+            res = std::make_shared<ov::op::v1::Softmax>(input, axis);
         }
 
         res = ngraphQuantize(res, output_sc, output_zp);
diff --git a/modules/dnn/src/layers/batch_norm_layer.cpp b/modules/dnn/src/layers/batch_norm_layer.cpp
index 1d95096e60..ccf5608317 100644
--- a/modules/dnn/src/layers/batch_norm_layer.cpp
+++ b/modules/dnn/src/layers/batch_norm_layer.cpp
@@ -18,8 +18,6 @@ Implementation of Batch Normalization layer.
 #include "../op_webnn.hpp"
 #include "../op_cann.hpp"
 
-#include <opencv2/dnn/shape_utils.hpp>
-
 #ifdef HAVE_OPENCL
 #include "opencl_kernels_dnn.hpp"
 #endif
@@ -459,14 +457,10 @@ public:
         auto ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         std::vector<size_t> shape(ieInpNode.get_shape().size(), 1);
         shape[1] = weights_.total();
-        auto weight = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape(shape), weights_.data);
-        auto bias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape(shape), bias_.data);
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2021_2)
-        auto scale_node = std::make_shared<ngraph::op::v1::Multiply>(ieInpNode, weight, ngraph::op::AutoBroadcastType::NUMPY);
-#else
-        auto scale_node = std::make_shared<ngraph::op::v0::Multiply>(ieInpNode, weight, ngraph::op::AutoBroadcastType::NUMPY);
-#endif
-        auto scale_shift = std::make_shared<ngraph::op::v1::Add>(scale_node, bias, ngraph::op::AutoBroadcastType::NUMPY);
+        auto weight = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape(shape), weights_.data);
+        auto bias = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape(shape), bias_.data);
+        auto scale_node = std::make_shared<ov::op::v1::Multiply>(ieInpNode, weight, ov::op::AutoBroadcastType::NUMPY);
+        auto scale_shift = std::make_shared<ov::op::v1::Add>(scale_node, bias, ov::op::AutoBroadcastType::NUMPY);
         return Ptr<BackendNode>(new InfEngineNgraphNode(scale_shift));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/blank_layer.cpp b/modules/dnn/src/layers/blank_layer.cpp
index 16de23b15e..9723975723 100644
--- a/modules/dnn/src/layers/blank_layer.cpp
+++ b/modules/dnn/src/layers/blank_layer.cpp
@@ -149,8 +149,8 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         auto ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        ngraph::OutputVector inp{ieInpNode};
-        auto blank = std::make_shared<ngraph::op::Concat>(inp, 0);
+        ov::OutputVector inp{ieInpNode};
+        auto blank = std::make_shared<ov::op::v0::Concat>(inp, 0);
         return Ptr<BackendNode>(new InfEngineNgraphNode(blank));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/concat_layer.cpp b/modules/dnn/src/layers/concat_layer.cpp
index a5af16f32e..14834d2cd3 100644
--- a/modules/dnn/src/layers/concat_layer.cpp
+++ b/modules/dnn/src/layers/concat_layer.cpp
@@ -59,7 +59,6 @@
 #include "../cuda4dnn/primitives/concat.hpp"
 using namespace cv::dnn::cuda4dnn;
 #endif
-
 namespace cv
 {
 namespace dnn
@@ -397,7 +396,7 @@ public:
         std::vector<size_t> maxDims(numDims, 0);
 
         CV_Assert(inputs.size() == nodes.size());
-        ngraph::OutputVector inp_nodes;
+        ov::OutputVector inp_nodes;
         for (int i = 0; i < nodes.size(); ++i)
         {
             auto inp = nodes[i].dynamicCast<InfEngineNgraphNode>()->node;
@@ -423,14 +422,14 @@ public:
             }
             if (needPadding)
             {
-                inp_nodes[i] = std::make_shared<ngraph::op::v1::Pad>(
+                inp_nodes[i] = std::make_shared<ov::op::v1::Pad>(
                     inp_nodes[i],
-                    std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{begins.size()}, begins.data()),
-                    std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{ends.size()}, ends.data()),
-                    ngraph::op::PadMode::CONSTANT);
+                    std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{begins.size()}, begins.data()),
+                    std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{ends.size()}, ends.data()),
+                    ov::op::PadMode::CONSTANT);
             }
         }
-        auto concat = std::make_shared<ngraph::op::Concat>(inp_nodes, cAxis);
+        auto concat = std::make_shared<ov::op::v0::Concat>(inp_nodes, cAxis);
         return Ptr<BackendNode>(new InfEngineNgraphNode(concat));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/const_layer.cpp b/modules/dnn/src/layers/const_layer.cpp
index 2a1e27db56..0fe7c22782 100644
--- a/modules/dnn/src/layers/const_layer.cpp
+++ b/modules/dnn/src/layers/const_layer.cpp
@@ -128,22 +128,22 @@ public:
     virtual Ptr<BackendNode> initNgraph(const std::vector<Ptr<BackendWrapper> >& inputs,
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
-        ngraph::element::Type dType;
+        ov::element::Type dType;
         if (blobs[0].depth() == CV_32F) {
-            dType = ngraph::element::f32;
+            dType = ov::element::f32;
         } else if (blobs[0].depth() == CV_32S) {
-            dType = ngraph::element::i32;
+            dType = ov::element::i32;
         } else if (blobs[0].depth() == CV_8S) {
-            dType = ngraph::element::i8;
+            dType = ov::element::i8;
         } else {
             CV_Error(Error::StsNotImplemented, format("Unexpected Const data depth: %d", blobs[0].depth()));
         }
-        std::shared_ptr<ngraph::Node> node =
-                    std::make_shared<ngraph::op::Constant>(dType,
+        std::shared_ptr<ov::Node> node =
+                    std::make_shared<ov::op::v0::Constant>(dType,
                                                            getShape<size_t>(blobs[0]),
                                                            blobs[0].data);
-        if (node->get_element_type() != ngraph::element::f32) {
-            node = std::make_shared<ngraph::op::Convert>(node, ngraph::element::f32);
+        if (node->get_element_type() != ov::element::f32) {
+            node = std::make_shared<ov::op::v0::Convert>(node, ov::element::f32);
         }
         return Ptr<BackendNode>(new InfEngineNgraphNode(node));
     }
diff --git a/modules/dnn/src/layers/convolution_layer.cpp b/modules/dnn/src/layers/convolution_layer.cpp
index dd6fa7bc1d..85c9f1d8d6 100644
--- a/modules/dnn/src/layers/convolution_layer.cpp
+++ b/modules/dnn/src/layers/convolution_layer.cpp
@@ -822,7 +822,7 @@ public:
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         std::vector<size_t> dims = ieInpNode.get_shape();
         CV_Check(dims.size(), dims.size() >= 3 && dims.size() <= 5, "");
-        ngraph::Output<ngraph::Node> ieWeights;
+        ov::Output<ov::Node> ieWeights;
         if (nodes.size() > 1)
             ieWeights = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
         const int inpCn = dims[1];
@@ -840,49 +840,49 @@ public:
 
         if (nodes.size() == 1)
         {
-            ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, kernel_shape, blobs[0].data);
+            ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, kernel_shape, blobs[0].data);
             if (fusedWeights)
             {
                 if (weightsMat.isContinuous())
                 {
-                    ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, kernel_shape, weightsMat.data);
+                    ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, kernel_shape, weightsMat.data);
                 }
                 else
                 {
                     Mat newWeights;
                     Mat cvWeights = weightsMat.colRange(0, blobs[0].total() / numOutput);
                     cvWeights.copyTo(newWeights);
-                    ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, kernel_shape, newWeights.data);
+                    ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, kernel_shape, newWeights.data);
                 }
             }
         }
         else
         {
-            auto shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                             ngraph::Shape{kernel_shape.size()}, std::vector<int64_t>(kernel_shape.begin(), kernel_shape.end()));
-            ieWeights  = std::make_shared<ngraph::op::v1::Reshape>(ieWeights, shape, true);
+            auto shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                             ov::Shape{kernel_shape.size()}, std::vector<int64_t>(kernel_shape.begin(), kernel_shape.end()));
+            ieWeights  = std::make_shared<ov::op::v1::Reshape>(ieWeights, shape, true);
         }
 
-        ngraph::op::PadType pad_type = ngraph::op::PadType::EXPLICIT;
+        ov::op::PadType pad_type = ov::op::PadType::EXPLICIT;
         if (!padMode.empty())
-            pad_type = padMode == "VALID" ? ngraph::op::PadType::VALID : ngraph::op::PadType::SAME_UPPER;
+            pad_type = padMode == "VALID" ? ov::op::PadType::VALID : ov::op::PadType::SAME_UPPER;
 
-        std::shared_ptr<ngraph::Node> conv_node;
+        std::shared_ptr<ov::Node> conv_node;
         if (group != 1) {
-            conv_node = std::make_shared<ngraph::op::v1::GroupConvolution>(
+            conv_node = std::make_shared<ov::op::v1::GroupConvolution>(
                                 ieInpNode, ieWeights,
-                                ngraph::Strides(strides),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(),   pads_end.end())),
-                                ngraph::Strides(dilations),
+                                ov::Strides(strides),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(),   pads_end.end())),
+                                ov::Strides(dilations),
                                 pad_type);
         } else {
-            conv_node = std::make_shared<ngraph::op::v1::Convolution>(
+            conv_node = std::make_shared<ov::op::v1::Convolution>(
                                 ieInpNode, ieWeights,
-                                ngraph::Strides(strides),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
-                                ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(), pads_end.end())),
-                                ngraph::Strides(dilations),
+                                ov::Strides(strides),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
+                                ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_end.begin(), pads_end.end())),
+                                ov::Strides(dilations),
                                 pad_type);
         }
 
@@ -890,18 +890,18 @@ public:
         {
             std::vector<size_t> shape(conv_node->get_shape().size(), 1);
             shape[1] = conv_node->get_shape()[1];
-            std::shared_ptr<ngraph::Node> bias;
+            std::shared_ptr<ov::Node> bias;
             if (nodes.size() == 3)
             {
-                auto bias_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                    ngraph::Shape{shape.size()}, std::vector<int64_t>(shape.begin(), shape.end()));
-                bias = std::make_shared<ngraph::op::v1::Reshape>(nodes[2].dynamicCast<InfEngineNgraphNode>()->node, bias_shape, true);
+                auto bias_shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                    ov::Shape{shape.size()}, std::vector<int64_t>(shape.begin(), shape.end()));
+                bias = std::make_shared<ov::op::v1::Reshape>(nodes[2].dynamicCast<InfEngineNgraphNode>()->node, bias_shape, true);
             }
             else
             {
-                bias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape(shape), biasvec.data());
+                bias = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape(shape), biasvec.data());
             }
-            auto conv_bias = std::make_shared<ngraph::op::v1::Add>(conv_node, bias, ngraph::op::AutoBroadcastType::NUMPY);
+            auto conv_bias = std::make_shared<ov::op::v1::Add>(conv_node, bias, ov::op::AutoBroadcastType::NUMPY);
             return Ptr<BackendNode>(new InfEngineNgraphNode(conv_bias));
         }
         return Ptr<BackendNode>(new InfEngineNgraphNode(conv_node));
@@ -2251,13 +2251,13 @@ public:
 
        auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
        std::vector<size_t> kernel_shape = getShape<size_t>(blobs[0]);
-       auto ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, kernel_shape, blobs[0].data);
+       auto ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, kernel_shape, blobs[0].data);
 
         if (fusedWeights)
         {
             Mat newWeights;
             transpose(weightsMat, newWeights);
-            ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, kernel_shape, newWeights.data);
+            ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, kernel_shape, newWeights.data);
         }
         std::vector<size_t> paddings_end;
         if (padMode == "SAME")
@@ -2269,24 +2269,24 @@ public:
         } else {
             paddings_end = pads_end;
         }
-        ngraph::op::PadType pad_type = padMode == "VALID" ? ngraph::op::PadType::VALID : ngraph::op::PadType::EXPLICIT;
+        ov::op::PadType pad_type = padMode == "VALID" ? ov::op::PadType::VALID : ov::op::PadType::EXPLICIT;
 
-        auto deconv = std::make_shared<ngraph::op::v1::ConvolutionBackpropData>(
+        auto deconv = std::make_shared<ov::op::v1::ConvolutionBackpropData>(
                           ieInpNode,
                           ieWeights,
-                          ngraph::Strides(strides),
-                          ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
-                          ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(paddings_end.begin(), paddings_end.end())),
-                          ngraph::Strides(dilations),
+                          ov::Strides(strides),
+                          ov::CoordinateDiff(std::vector<std::ptrdiff_t>(pads_begin.begin(), pads_begin.end())),
+                          ov::CoordinateDiff(std::vector<std::ptrdiff_t>(paddings_end.begin(), paddings_end.end())),
+                          ov::Strides(dilations),
                           pad_type,
-                          ngraph::CoordinateDiff(std::vector<std::ptrdiff_t>(adjust_pads.begin(), adjust_pads.end())));
+                          ov::CoordinateDiff(std::vector<std::ptrdiff_t>(adjust_pads.begin(), adjust_pads.end())));
 
         if (hasBias() || fusedBias)
         {
             std::vector<size_t> shape(deconv->get_shape().size(), 1);
             shape[1] = numOutput;
-            auto bias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape(shape), blobs[1].data);
-            auto deconv_bias = std::make_shared<ngraph::op::v1::Add>(deconv, bias, ngraph::op::AutoBroadcastType::NUMPY);
+            auto bias = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape(shape), blobs[1].data);
+            auto deconv_bias = std::make_shared<ov::op::v1::Add>(deconv, bias, ov::op::AutoBroadcastType::NUMPY);
             return Ptr<BackendNode>(new InfEngineNgraphNode(deconv_bias));
         }
 
diff --git a/modules/dnn/src/layers/crop_and_resize_layer.cpp b/modules/dnn/src/layers/crop_and_resize_layer.cpp
index a6f58f8983..e735242929 100644
--- a/modules/dnn/src/layers/crop_and_resize_layer.cpp
+++ b/modules/dnn/src/layers/crop_and_resize_layer.cpp
@@ -138,23 +138,23 @@ public:
         offsets[3] = 2;
         dims[3] = 7;
 
-        auto lower_bounds = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                             ngraph::Shape{offsets.size()}, offsets.data());
-        auto upper_bounds = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                             ngraph::Shape{dims.size()}, dims.data());
-        auto strides = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                        ngraph::Shape{dims.size()}, std::vector<int64_t>((int64_t)dims.size(), 1));
-        auto slice = std::make_shared<ngraph::op::v1::StridedSlice>(rois,
+        auto lower_bounds = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                             ov::Shape{offsets.size()}, offsets.data());
+        auto upper_bounds = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                             ov::Shape{dims.size()}, dims.data());
+        auto strides = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                        ov::Shape{dims.size()}, std::vector<int64_t>((int64_t)dims.size(), 1));
+        auto slice = std::make_shared<ov::op::v1::StridedSlice>(rois,
                                       lower_bounds, upper_bounds, strides, std::vector<int64_t>{}, std::vector<int64_t>{});
 
         // Reshape rois from 4D to 2D
         std::vector<int64_t> shapeData = {dims[2], 5};
-        auto shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, shapeData.data());
-        auto reshape = std::make_shared<ngraph::op::v1::Reshape>(slice, shape, true);
+        auto shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2}, shapeData.data());
+        auto reshape = std::make_shared<ov::op::v1::Reshape>(slice, shape, true);
 
         auto roiPooling =
-            std::make_shared<ngraph::op::v0::ROIPooling>(input, reshape,
-                                                         ngraph::Shape{(size_t)outHeight, (size_t)outWidth},
+            std::make_shared<ov::op::v0::ROIPooling>(input, reshape,
+                                                         ov::Shape{(size_t)outHeight, (size_t)outWidth},
                                                          1.0f, "bilinear");
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(roiPooling));
diff --git a/modules/dnn/src/layers/detection_output_layer.cpp b/modules/dnn/src/layers/detection_output_layer.cpp
index 26f483a770..07f96e2a08 100644
--- a/modules/dnn/src/layers/detection_output_layer.cpp
+++ b/modules/dnn/src/layers/detection_output_layer.cpp
@@ -55,11 +55,7 @@
 
 #ifdef HAVE_DNN_NGRAPH
 #include "../ie_ngraph.hpp"
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-#include <ngraph/op/detection_output.hpp>
-#else
-#include <ngraph/op/experimental/layers/detection_output.hpp>
-#endif
+#include <openvino/op/detection_output.hpp>
 #endif
 
 #ifdef HAVE_CUDA
@@ -1012,26 +1008,26 @@ public:
 
         if (_locPredTransposed) {
             // Convert box predictions from yxYX to xyXY
-            box_logits = std::make_shared<ngraph::op::v1::Reshape>(box_logits,
-                std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{3}, std::vector<int32_t>{0, -1, 2}),
+            box_logits = std::make_shared<ov::op::v1::Reshape>(box_logits,
+                std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{3}, std::vector<int32_t>{0, -1, 2}),
                 true
             );
             int axis = 2;
-            box_logits = std::make_shared<ngraph::op::v1::Reverse>(box_logits,
-                std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{1}, &axis),
-                ngraph::op::v1::Reverse::Mode::INDEX
+            box_logits = std::make_shared<ov::op::v1::Reverse>(box_logits,
+                std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{1}, &axis),
+                ov::op::v1::Reverse::Mode::INDEX
             );
         }
 
-        auto shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{2}, std::vector<int32_t>{0, -1});
-        box_logits = std::make_shared<ngraph::op::v1::Reshape>(box_logits, shape, true);
-        class_preds = std::make_shared<ngraph::op::v1::Reshape>(class_preds, shape, true);
-        proposals = std::make_shared<ngraph::op::v1::Reshape>(proposals,
-            std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{3}, std::vector<int32_t>{0, _varianceEncodedInTarget ? 1 : 2, -1}),
+        auto shape = std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{2}, std::vector<int32_t>{0, -1});
+        box_logits = std::make_shared<ov::op::v1::Reshape>(box_logits, shape, true);
+        class_preds = std::make_shared<ov::op::v1::Reshape>(class_preds, shape, true);
+        proposals = std::make_shared<ov::op::v1::Reshape>(proposals,
+            std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{3}, std::vector<int32_t>{0, _varianceEncodedInTarget ? 1 : 2, -1}),
             true
         );
 
-        ngraph::op::DetectionOutputAttrs attrs;
+        ov::op::v0::DetectionOutput::Attributes attrs;
         attrs.num_classes                = _numClasses;
         attrs.background_label_id        = _backgroundLabelId;
         attrs.top_k                      = _topK > 0 ? _topK : _keepTopK;
@@ -1044,7 +1040,7 @@ public:
         attrs.code_type                  = std::string{"caffe.PriorBoxParameter." + _codeType};
         attrs.normalized                 = true;
 
-        auto det_out = std::make_shared<ngraph::op::DetectionOutput>(box_logits, class_preds,
+        auto det_out = std::make_shared<ov::op::v0::DetectionOutput>(box_logits, class_preds,
                        proposals, attrs);
         return Ptr<BackendNode>(new InfEngineNgraphNode(det_out));
     }
diff --git a/modules/dnn/src/layers/elementwise_layers.cpp b/modules/dnn/src/layers/elementwise_layers.cpp
index 746db69603..f96a7f2f08 100644
--- a/modules/dnn/src/layers/elementwise_layers.cpp
+++ b/modules/dnn/src/layers/elementwise_layers.cpp
@@ -490,13 +490,13 @@ struct ReLUFunctor : public BaseFunctor
 #endif
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
         if (slope) {
-            auto param = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &slope);
-            return std::make_shared<ngraph::op::PRelu>(node, param);
+            auto param = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &slope);
+            return std::make_shared<ov::op::v0::PRelu>(node, param);
         }
-        return std::make_shared<ngraph::op::Relu>(node);
+        return std::make_shared<ov::op::v0::Relu>(node);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -674,9 +674,9 @@ struct ReLU6Functor : public BaseFunctor
 
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::Clamp>(node, minValue, maxValue);
+        return std::make_shared<ov::op::v0::Clamp>(node, minValue, maxValue);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -796,7 +796,7 @@ struct BaseDefaultFunctor : public BaseFunctor
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
         CV_Error(Error::StsNotImplemented, "");
     }
@@ -929,9 +929,9 @@ struct TanHFunctor : public BaseDefaultFunctor<TanHFunctor>
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::Tanh>(node);
+        return std::make_shared<ov::op::v0::Tanh>(node);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -998,10 +998,10 @@ struct SwishFunctor : public BaseDefaultFunctor<SwishFunctor>
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        auto sigmoid = std::make_shared<ngraph::op::Sigmoid>(node);
-        return std::make_shared<ngraph::op::v1::Multiply>(node, sigmoid);
+        auto sigmoid = std::make_shared<ov::op::v0::Sigmoid>(node);
+        return std::make_shared<ov::op::v1::Multiply>(node, sigmoid);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -1074,9 +1074,9 @@ struct MishFunctor : public BaseDefaultFunctor<MishFunctor>
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::v4::Mish>(node);
+        return std::make_shared<ov::op::v4::Mish>(node);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -1151,9 +1151,9 @@ struct SigmoidFunctor : public BaseDefaultFunctor<SigmoidFunctor>
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::Sigmoid>(node);
+        return std::make_shared<ov::op::v0::Sigmoid>(node);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -1231,9 +1231,9 @@ struct ELUFunctor : public BaseDefaultFunctor<ELUFunctor>
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::Elu>(node, alpha);
+        return std::make_shared<ov::op::v0::Elu>(node, alpha);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -1301,9 +1301,9 @@ struct AbsValFunctor : public BaseDefaultFunctor<AbsValFunctor>
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::Abs>(node);
+        return std::make_shared<ov::op::v0::Abs>(node);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -1594,9 +1594,9 @@ struct SqrtFunctor : public BaseDefaultFunctor<SqrtFunctor>
 #endif  // HAVE_HALIDE
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        return std::make_shared<ngraph::op::v0::Sqrt>(node);
+        return std::make_shared<ov::op::v0::Sqrt>(node);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -2343,22 +2343,22 @@ struct PowerFunctor : public BaseFunctor
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        auto scale_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                                 ngraph::Shape{1}, &scale);
-        auto shift_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                                 ngraph::Shape{1}, &shift);
+        auto scale_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                                 ov::Shape{1}, &scale);
+        auto shift_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                                 ov::Shape{1}, &shift);
 
-        auto mul = std::make_shared<ngraph::op::v1::Multiply>(scale_node, node, ngraph::op::AutoBroadcastType::NUMPY);
-        auto scale_shift = std::make_shared<ngraph::op::v1::Add>(mul, shift_node, ngraph::op::AutoBroadcastType::NUMPY);
+        auto mul = std::make_shared<ov::op::v1::Multiply>(scale_node, node, ov::op::AutoBroadcastType::NUMPY);
+        auto scale_shift = std::make_shared<ov::op::v1::Add>(mul, shift_node, ov::op::AutoBroadcastType::NUMPY);
 
         if (power == 1)
             return scale_shift;
 
-        auto power_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                                 ngraph::Shape{1}, &power);
-        return std::make_shared<ngraph::op::v1::Power>(scale_shift, power_node, ngraph::op::AutoBroadcastType::NUMPY);
+        auto power_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                                 ov::Shape{1}, &power);
+        return std::make_shared<ov::op::v1::Power>(scale_shift, power_node, ov::op::AutoBroadcastType::NUMPY);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -2453,15 +2453,15 @@ struct ExpFunctor : public BaseDefaultFunctor<ExpFunctor>
 #endif  // HAVE_HALIDE
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
-        auto scale_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                                 ngraph::Shape{1}, &normScale);
-        auto shift_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                                 ngraph::Shape{1}, &normShift);
-        auto mul = std::make_shared<ngraph::op::v1::Multiply>(scale_node, node, ngraph::op::AutoBroadcastType::NUMPY);
-        auto scale_shift = std::make_shared<ngraph::op::v1::Add>(mul, shift_node, ngraph::op::AutoBroadcastType::NUMPY);
-        return std::make_shared<ngraph::op::v0::Exp>(scale_shift);
+        auto scale_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                                 ov::Shape{1}, &normScale);
+        auto shift_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                                 ov::Shape{1}, &normShift);
+        auto mul = std::make_shared<ov::op::v1::Multiply>(scale_node, node, ov::op::AutoBroadcastType::NUMPY);
+        auto scale_shift = std::make_shared<ov::op::v1::Add>(mul, shift_node, ov::op::AutoBroadcastType::NUMPY);
+        return std::make_shared<ov::op::v0::Exp>(scale_shift);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -2612,11 +2612,11 @@ struct ChannelsPReLUFunctor : public BaseFunctor
 #endif // HAVE_CANN
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
         const size_t numChannels = scale.total();
-        auto slope = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{numChannels}, scale.data);
-        return std::make_shared<ngraph::op::PRelu>(node, slope);
+        auto slope = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{numChannels}, scale.data);
+        return std::make_shared<ov::op::v0::PRelu>(node, slope);
     }
 #endif  // HAVE_DNN_NGRAPH
 
@@ -2692,11 +2692,11 @@ struct PReLUFunctor : public ChannelsPReLUFunctor
     }
 
 #ifdef HAVE_DNN_NGRAPH
-    std::shared_ptr<ngraph::Node> initNgraphAPI(const ngraph::Output<ngraph::Node>& node)
+    std::shared_ptr<ov::Node> initNgraphAPI(const ov::Output<ov::Node>& node)
     {
         auto shape = getShape<size_t>(scale);
-        auto slope = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, shape, scale.ptr<float>());
-        return std::make_shared<ngraph::op::PRelu>(node, slope);
+        auto slope = std::make_shared<ov::op::v0::Constant>(ov::element::f32, shape, scale.ptr<float>());
+        return std::make_shared<ov::op::v0::PRelu>(node, slope);
     }
 #endif  // HAVE_DNN_NGRAPH
 };
diff --git a/modules/dnn/src/layers/eltwise_layer.cpp b/modules/dnn/src/layers/eltwise_layer.cpp
index 49b3c02de3..37b8b1ba66 100644
--- a/modules/dnn/src/layers/eltwise_layer.cpp
+++ b/modules/dnn/src/layers/eltwise_layer.cpp
@@ -899,24 +899,24 @@ public:
         CV_Assert(nodes.size() >= 2);
         auto curr_node = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         if (!coeffs.empty()) {
-            auto coeff = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &coeffs[0]);
-            curr_node = std::make_shared<ngraph::op::v1::Multiply>(curr_node, coeff, ngraph::op::AutoBroadcastType::NUMPY);
+            auto coeff = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &coeffs[0]);
+            curr_node = std::make_shared<ov::op::v1::Multiply>(curr_node, coeff, ov::op::AutoBroadcastType::NUMPY);
         }
 
-        std::shared_ptr<ngraph::Node> res;
+        std::shared_ptr<ov::Node> res;
         for (size_t i = 1; i < nodes.size(); i++)
         {
             auto next_node = nodes[i].dynamicCast<InfEngineNgraphNode>()->node;
             if (!coeffs.empty()) {
-                auto coeff = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &coeffs[i]);
-                next_node = std::make_shared<ngraph::op::v1::Multiply>(next_node, coeff, ngraph::op::AutoBroadcastType::NUMPY);
+                auto coeff = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &coeffs[i]);
+                next_node = std::make_shared<ov::op::v1::Multiply>(next_node, coeff, ov::op::AutoBroadcastType::NUMPY);
             }
             switch (op) {
-                case SUM:  res = std::make_shared<ngraph::op::v1::Add>(curr_node, next_node); break;
-                case PROD: res = std::make_shared<ngraph::op::v1::Multiply>(curr_node, next_node); break;
-                case DIV:  res = std::make_shared<ngraph::op::v1::Divide>(curr_node, next_node); break;
-                case MAX:  res = std::make_shared<ngraph::op::v1::Maximum>(curr_node, next_node); break;
-                case MIN:  res = std::make_shared<ngraph::op::v1::Minimum>(curr_node, next_node); break;
+                case SUM:  res = std::make_shared<ov::op::v1::Add>(curr_node, next_node); break;
+                case PROD: res = std::make_shared<ov::op::v1::Multiply>(curr_node, next_node); break;
+                case DIV:  res = std::make_shared<ov::op::v1::Divide>(curr_node, next_node); break;
+                case MAX:  res = std::make_shared<ov::op::v1::Maximum>(curr_node, next_node); break;
+                case MIN:  res = std::make_shared<ov::op::v1::Minimum>(curr_node, next_node); break;
                 default: CV_Error(Error::StsNotImplemented, "Unsupported eltwise operation");
             }
             curr_node = res;
diff --git a/modules/dnn/src/layers/flatten_layer.cpp b/modules/dnn/src/layers/flatten_layer.cpp
index 9ff3bec38b..48950601f2 100644
--- a/modules/dnn/src/layers/flatten_layer.cpp
+++ b/modules/dnn/src/layers/flatten_layer.cpp
@@ -56,6 +56,7 @@
 using namespace cv::dnn::cuda4dnn;
 #endif
 
+
 namespace cv
 {
 namespace dnn
@@ -224,9 +225,9 @@ public:
         outputShapeVec.push_back(flattenedDimensionSize);
         outputShapeVec.insert(outputShapeVec.end(), dims.begin() + endAxis + 1, dims.end());
 
-        auto shape   = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                       ngraph::Shape({outputShapeVec.size()}), outputShapeVec.data());
-        auto reshape = std::make_shared<ngraph::op::v1::Reshape>(ieInpNode, shape, true);
+        auto shape   = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                       ov::Shape({outputShapeVec.size()}), outputShapeVec.data());
+        auto reshape = std::make_shared<ov::op::v1::Reshape>(ieInpNode, shape, true);
         return Ptr<BackendNode>(new InfEngineNgraphNode(reshape));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/fully_connected_layer.cpp b/modules/dnn/src/layers/fully_connected_layer.cpp
index 1c27043f1a..0694c4bc69 100644
--- a/modules/dnn/src/layers/fully_connected_layer.cpp
+++ b/modules/dnn/src/layers/fully_connected_layer.cpp
@@ -794,20 +794,20 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        std::shared_ptr<ngraph::Node> matmul;
+        std::shared_ptr<ov::Node> matmul;
 
         if (nodes.size() == 2)
         {
             auto& inp2 = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-            matmul = std::make_shared<ngraph::op::MatMul>(ieInpNode, inp2, transA, transB);
+            matmul = std::make_shared<ov::op::v0::MatMul>(ieInpNode, inp2, transA, transB);
         }
         else
         {
             std::vector<int> shape(1 + normalize_axis(axis, ieInpNode.get_shape().size()), 0);
             shape[shape.size() - 1] = -1;
-            auto inp = std::make_shared<ngraph::op::v1::Reshape>(
+            auto inp = std::make_shared<ov::op::v1::Reshape>(
                 ieInpNode,
-                std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{shape.size()}, shape.data()),
+                std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{shape.size()}, shape.data()),
                 true
             );
 
@@ -817,14 +817,14 @@ public:
             } else {
                 weight_shape = {(size_t)blobs[0].size[0], (size_t)blobs[0].size[1]};
             }
-            auto ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, weight_shape, blobs[0].data);
-            matmul = std::make_shared<ngraph::op::MatMul>(inp, ieWeights, transA, transB);
+            auto ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, weight_shape, blobs[0].data);
+            matmul = std::make_shared<ov::op::v0::MatMul>(inp, ieWeights, transA, transB);
         }
 
         if (bias) {
-            auto bias_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                              ngraph::Shape{(size_t)blobs[1].size[1]}, blobs[1].data);
-            matmul = std::make_shared<ngraph::op::v1::Add>(matmul, bias_node, ngraph::op::AutoBroadcastType::NUMPY);
+            auto bias_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                              ov::Shape{(size_t)blobs[1].size[1]}, blobs[1].data);
+            matmul = std::make_shared<ov::op::v1::Add>(matmul, bias_node, ov::op::AutoBroadcastType::NUMPY);
         }
         return Ptr<BackendNode>(new InfEngineNgraphNode(matmul));
     }
diff --git a/modules/dnn/src/layers/gemm_layer.cpp b/modules/dnn/src/layers/gemm_layer.cpp
index 821700c83e..8c7000a8e6 100644
--- a/modules/dnn/src/layers/gemm_layer.cpp
+++ b/modules/dnn/src/layers/gemm_layer.cpp
@@ -285,45 +285,45 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         auto ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        std::shared_ptr<ngraph::Node> matmul;
+        std::shared_ptr<ov::Node> matmul;
 
         if (nodes.size() == 2)
         {
             auto& inp2 = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-            matmul = std::make_shared<ngraph::op::MatMul>(ieInpNode, inp2, trans_a, trans_b);
+            matmul = std::make_shared<ov::op::v0::MatMul>(ieInpNode, inp2, trans_a, trans_b);
         }
         else
         {
-            std::shared_ptr<ngraph::Node> ieWeights = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, getShape(blobs[0]), blobs[0].data);
+            std::shared_ptr<ov::Node> ieWeights = std::make_shared<ov::op::v0::Constant>(ov::element::f32, getShape(blobs[0]), blobs[0].data);
 
             int flatten_axis = ieInpNode.get_shape().size() - ieWeights->get_shape().size();
             if (flatten_axis > 0) {
                 std::vector<int> shape(1 + flatten_axis, 0);
                 shape[shape.size() - 1] = -1;
-                ieInpNode = std::make_shared<ngraph::op::v1::Reshape>(
+                ieInpNode = std::make_shared<ov::op::v1::Reshape>(
                     ieInpNode,
-                    std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{shape.size()}, shape.data()),
+                    std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{shape.size()}, shape.data()),
                     true
                 );
             }
-            matmul = std::make_shared<ngraph::op::MatMul>(ieInpNode, ieWeights, trans_a, trans_b);
+            matmul = std::make_shared<ov::op::v0::MatMul>(ieInpNode, ieWeights, trans_a, trans_b);
         }
         if (alpha != 1.0f) {
-            matmul = std::make_shared<ngraph::op::v1::Multiply>(matmul,
-                std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &alpha)
+            matmul = std::make_shared<ov::op::v1::Multiply>(matmul,
+                std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &alpha)
             );
         }
 
         if (have_bias && const_C) {
             Mat bias = blobs.back();
-            auto shape = bias.total() == bias.size[0] ? ngraph::Shape{bias.total()} : getShape(bias);
-            std::shared_ptr<ngraph::Node> bias_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, shape, bias.data);
+            auto shape = bias.total() == bias.size[0] ? ov::Shape{bias.total()} : getShape(bias);
+            std::shared_ptr<ov::Node> bias_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, shape, bias.data);
             if (beta != 1.0f) {
-                bias_node = std::make_shared<ngraph::op::v1::Multiply>(bias_node,
-                    std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &beta)
+                bias_node = std::make_shared<ov::op::v1::Multiply>(bias_node,
+                    std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &beta)
                 );
             }
-            matmul = std::make_shared<ngraph::op::v1::Add>(matmul, bias_node, ngraph::op::AutoBroadcastType::NUMPY);
+            matmul = std::make_shared<ov::op::v1::Add>(matmul, bias_node, ov::op::AutoBroadcastType::NUMPY);
         }
         return Ptr<BackendNode>(new InfEngineNgraphNode(matmul));
     }
diff --git a/modules/dnn/src/layers/instance_norm_layer.cpp b/modules/dnn/src/layers/instance_norm_layer.cpp
index b43e9bbb7a..29f3732dc2 100644
--- a/modules/dnn/src/layers/instance_norm_layer.cpp
+++ b/modules/dnn/src/layers/instance_norm_layer.cpp
@@ -225,33 +225,26 @@ public:
 
         auto ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         const auto &input_shape = ieInpNode.get_shape();
-        std::shared_ptr<ngraph::Node> mvn, result;
+        std::shared_ptr<ov::Node> mvn, result;
 
         // mvn
-#if INF_ENGINE_VER_MAJOR_LE(INF_ENGINE_RELEASE_2021_2)
-        // https://docs.openvino.ai/2021.4/api/ngraph_python_api/_autosummary/ngraph.opset3.mvn.html?highlight=mvn#ngraph.opset3.mvn
-        bool across_channels = false;
-        bool normalize_variance = true;
-        mvn = std::make_shared<ngraph::op::MVN>(ieInpNode, across_channels, normalize_variance, epsilon);
-#else
         // https://docs.openvino.ai/2023.1/openvino_docs_ops_normalization_MVN_6.html
         std::vector<int64_t> axes_v(input_shape.size() - 2);
         std::iota(axes_v.begin(), axes_v.end(), 2); // {2, 3, ...} for nd input tensor, n>=3
-        auto axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes_v.size()}, axes_v.data());
+        auto axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes_v.size()}, axes_v.data());
         bool normalize_variance = true;
-        mvn = std::make_shared<ngraph::op::v6::MVN>(ieInpNode, axes, normalize_variance, epsilon, ngraph::op::MVNEpsMode::INSIDE_SQRT);
-#endif
+        mvn = std::make_shared<ov::op::v6::MVN>(ieInpNode, axes, normalize_variance, epsilon, ov::op::MVNEpsMode::INSIDE_SQRT);
 
         // instance norm = scale * mvn + bias
         auto scale = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
         std::vector<int64_t> shared_shape_v(input_shape.size(), 1);
         shared_shape_v[1] = -1;
-        auto shared_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{shared_shape_v.size()}, shared_shape_v.data());
-        scale  = std::make_shared<ngraph::op::v1::Reshape>(scale, shared_shape, true);
-        result = std::make_shared<ngraph::op::v1::Multiply>(mvn, scale);
+        auto shared_shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{shared_shape_v.size()}, shared_shape_v.data());
+        scale  = std::make_shared<ov::op::v1::Reshape>(scale, shared_shape, true);
+        result = std::make_shared<ov::op::v1::Multiply>(mvn, scale);
         auto bias = nodes[2].dynamicCast<InfEngineNgraphNode>()->node;
-        bias  = std::make_shared<ngraph::op::v1::Reshape>(bias, shared_shape, true);
-        result = std::make_shared<ngraph::op::v1::Add>(result, bias);
+        bias  = std::make_shared<ov::op::v1::Reshape>(bias, shared_shape, true);
+        result = std::make_shared<ov::op::v1::Add>(result, bias);
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(result));
     }
diff --git a/modules/dnn/src/layers/layer_norm.cpp b/modules/dnn/src/layers/layer_norm.cpp
index f3d2667a0a..aae5ff9f41 100644
--- a/modules/dnn/src/layers/layer_norm.cpp
+++ b/modules/dnn/src/layers/layer_norm.cpp
@@ -263,42 +263,35 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE {
         auto ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         const auto &input_shape = ieInpNode.get_shape();
-        std::shared_ptr<ngraph::Node> mvn, result;
+        std::shared_ptr<ov::Node> mvn, result;
 
         // mvn
-#if INF_ENGINE_VER_MAJOR_LE(INF_ENGINE_RELEASE_2021_2)
-        // https://docs.openvino.ai/2021.4/api/ngraph_python_api/_autosummary/ngraph.opset3.mvn.html?highlight=mvn#ngraph.opset3.mvn
-        bool across_channels = false;
-        bool normalize_variance = true;
-        mvn = std::make_shared<ngraph::op::MVN>(ieInpNode, across_channels, normalize_variance, epsilon);
-#else
         // https://docs.openvino.ai/2023.1/openvino_docs_ops_normalization_MVN_6.html
         std::vector<int64_t> axes_v(input_shape.size() - axis);
         std::iota(axes_v.begin(), axes_v.end(), axis);
-        auto axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes_v.size()}, axes_v.data());
+        auto axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes_v.size()}, axes_v.data());
         bool normalize_variance = true;
-        mvn = std::make_shared<ngraph::op::v6::MVN>(ieInpNode, axes, normalize_variance, epsilon, ngraph::op::MVNEpsMode::INSIDE_SQRT);
-#endif
+        mvn = std::make_shared<ov::op::v6::MVN>(ieInpNode, axes, normalize_variance, epsilon, ov::op::MVNEpsMode::INSIDE_SQRT);
 
         // layer norm = scale * mvn + bias
         auto scale = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-        ngraph::Output<ngraph::Node> bias;
+        ov::Output<ov::Node> bias;
         if (nodes.size() == 3) {
             bias = nodes[2].dynamicCast<InfEngineNgraphNode>()->node;
         }
         if (axis == -1 || axis == input_shape.size() - 1) { // special case for 1D tensor (2D mat)
             std::vector<int64_t> shared_shape_v(input_shape.size(), 1);
             shared_shape_v.back() = -1;
-            auto shared_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{shared_shape_v.size()}, shared_shape_v.data());
-            scale  = std::make_shared<ngraph::op::v1::Reshape>(scale, shared_shape, true);
+            auto shared_shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{shared_shape_v.size()}, shared_shape_v.data());
+            scale  = std::make_shared<ov::op::v1::Reshape>(scale, shared_shape, true);
             if (nodes.size() == 3) {
-                bias  = std::make_shared<ngraph::op::v1::Reshape>(bias, shared_shape, true);
+                bias  = std::make_shared<ov::op::v1::Reshape>(bias, shared_shape, true);
             }
         }
 
-        result = std::make_shared<ngraph::op::v1::Multiply>(mvn, scale);
+        result = std::make_shared<ov::op::v1::Multiply>(mvn, scale);
         if (nodes.size() == 3) {
-            result = std::make_shared<ngraph::op::v1::Add>(result, bias);
+            result = std::make_shared<ov::op::v1::Add>(result, bias);
         }
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(result));
diff --git a/modules/dnn/src/layers/lrn_layer.cpp b/modules/dnn/src/layers/lrn_layer.cpp
index f8de64cb32..a86004ac18 100644
--- a/modules/dnn/src/layers/lrn_layer.cpp
+++ b/modules/dnn/src/layers/lrn_layer.cpp
@@ -483,8 +483,8 @@ public:
             axes.resize(ieInpNode.get_shape().size() - 2);
             std::iota(axes.begin(), axes.end(), 2);
         }
-        auto ngraph_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes.size()}, axes.data());
-        auto lrn = std::make_shared<ngraph::op::LRN>(ieInpNode, ngraph_axes, alphaSize, beta, bias, size);
+        auto ngraph_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes.size()}, axes.data());
+        auto lrn = std::make_shared<ov::op::v0::LRN>(ieInpNode, ngraph_axes, alphaSize, beta, bias, size);
         return Ptr<BackendNode>(new InfEngineNgraphNode(lrn));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/matmul_layer.cpp b/modules/dnn/src/layers/matmul_layer.cpp
index c6cea65d87..2fccbde4ac 100644
--- a/modules/dnn/src/layers/matmul_layer.cpp
+++ b/modules/dnn/src/layers/matmul_layer.cpp
@@ -216,15 +216,15 @@ class MatMulLayerImpl CV_FINAL : public MatMulLayer {
     virtual Ptr<BackendNode> initNgraph(const std::vector<Ptr<BackendWrapper> >& inputs,
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE {
         auto& input_A_node = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        std::shared_ptr<ngraph::Node> matmul;
+        std::shared_ptr<ov::Node> matmul;
 
         if (nodes.size() == 2) {
             auto &input_B_node = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-            matmul = std::make_shared<ngraph::op::MatMul>(input_A_node, input_B_node, trans_a, trans_b);
+            matmul = std::make_shared<ov::op::v0::MatMul>(input_A_node, input_B_node, trans_a, trans_b);
         } else {
             auto input_B_shape = getShape<size_t>(blobs[0]);
-            auto input_B_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, input_B_shape, blobs[0].data);
-            matmul = std::make_shared<ngraph::op::MatMul>(input_A_node, input_B_node, trans_a, trans_b);
+            auto input_B_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, input_B_shape, blobs[0].data);
+            matmul = std::make_shared<ov::op::v0::MatMul>(input_A_node, input_B_node, trans_a, trans_b);
         }
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(matmul));
diff --git a/modules/dnn/src/layers/max_unpooling_layer.cpp b/modules/dnn/src/layers/max_unpooling_layer.cpp
index 7ed6c64ae8..d81d585d4e 100644
--- a/modules/dnn/src/layers/max_unpooling_layer.cpp
+++ b/modules/dnn/src/layers/max_unpooling_layer.cpp
@@ -200,29 +200,29 @@ public:
         getMemoryShapes(inpShapes, 1, outShapes, internals);
 
         Mat zeros = Mat::zeros(1, total(outShapes[0]), CV_32F);
-        auto zeroInp = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{zeros.total()}, zeros.data);
+        auto zeroInp = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{zeros.total()}, zeros.data);
 
         int newShape = -1;
-        features = std::make_shared<ngraph::op::v1::Reshape>(
+        features = std::make_shared<ov::op::v1::Reshape>(
             features,
-            std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{1}, &newShape),
+            std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{1}, &newShape),
             true
         );
-        indices = std::make_shared<ngraph::op::v1::Reshape>(
+        indices = std::make_shared<ov::op::v1::Reshape>(
             indices,
-            std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{1}, &newShape),
+            std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{1}, &newShape),
             true
         );
-        if (indices.get_element_type() != ngraph::element::i32 && indices.get_element_type() != ngraph::element::i64) {
-            indices = std::make_shared<ngraph::op::Convert>(indices, ngraph::element::i64);
+        if (indices.get_element_type() != ov::element::i32 && indices.get_element_type() != ov::element::i64) {
+            indices = std::make_shared<ov::op::v0::Convert>(indices, ov::element::i64);
         }
 
         int axis = 0;
-        std::shared_ptr<ngraph::Node> unpool = std::make_shared<ngraph::op::ScatterElementsUpdate>(zeroInp, indices, features,
-            std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{1}, &axis));
+        std::shared_ptr<ov::Node> unpool = std::make_shared<ov::op::v3::ScatterElementsUpdate>(zeroInp, indices, features,
+            std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{1}, &axis));
 
-        auto shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i32, ngraph::Shape{outShapes[0].size()}, outShapes[0].data());
-        unpool = std::make_shared<ngraph::op::v1::Reshape>(unpool, shape, true);
+        auto shape = std::make_shared<ov::op::v0::Constant>(ov::element::i32, ov::Shape{outShapes[0].size()}, outShapes[0].data());
+        unpool = std::make_shared<ov::op::v1::Reshape>(unpool, shape, true);
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(unpool));
     }
diff --git a/modules/dnn/src/layers/mvn_layer.cpp b/modules/dnn/src/layers/mvn_layer.cpp
index 43c33d82af..69a5bc8b1a 100644
--- a/modules/dnn/src/layers/mvn_layer.cpp
+++ b/modules/dnn/src/layers/mvn_layer.cpp
@@ -336,15 +336,11 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-#if INF_ENGINE_VER_MAJOR_LE(INF_ENGINE_RELEASE_2021_2)
-        auto mvn = std::make_shared<ngraph::op::MVN>(ieInpNode, acrossChannels, normVariance, eps);
-#else
         int64_t start_axis = acrossChannels ? 1 : 2;
         std::vector<int64_t> axes_v(ieInpNode.get_shape().size() - start_axis);
         std::iota(axes_v.begin(), axes_v.end(), start_axis);
-        auto axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes_v.size()}, axes_v.data());
-        auto mvn = std::make_shared<ngraph::op::v6::MVN>(ieInpNode, axes, normVariance, eps, ngraph::op::MVNEpsMode::INSIDE_SQRT);
-#endif
+        auto axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes_v.size()}, axes_v.data());
+        auto mvn = std::make_shared<ov::op::v6::MVN>(ieInpNode, axes, normVariance, eps, ov::op::MVNEpsMode::INSIDE_SQRT);
         return Ptr<BackendNode>(new InfEngineNgraphNode(mvn));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/nary_eltwise_layers.cpp b/modules/dnn/src/layers/nary_eltwise_layers.cpp
index c988ec69f2..b20148ac31 100644
--- a/modules/dnn/src/layers/nary_eltwise_layers.cpp
+++ b/modules/dnn/src/layers/nary_eltwise_layers.cpp
@@ -897,7 +897,6 @@ public:
         CV_Assert(inputs.size());
         return inputs.size() * total(outputs[0]);
     }
-
 #ifdef HAVE_DNN_NGRAPH
     virtual Ptr<BackendNode> initNgraph(const std::vector<Ptr<BackendWrapper> >& inputs, const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
@@ -907,22 +906,30 @@ public:
 
         if (inp0.get_element_type() != inp1.get_element_type()) {
             auto dtype = preferableTarget == DNN_TARGET_OPENCL_FP16 || preferableTarget == DNN_TARGET_MYRIAD ?
-                        ngraph::element::f16 : ngraph::element::f32;
+                        ov::element::f16 : ov::element::f32;
             if (inp0.get_element_type() != dtype)
-                inp0 = std::make_shared<ngraph::op::v0::Convert>(inp0, dtype);
+                inp0 = std::make_shared<ov::op::v0::Convert>(inp0, dtype);
             if (inp1.get_element_type() != dtype)
-                inp1 = std::make_shared<ngraph::op::v0::Convert>(inp1, dtype);
+                inp1 = std::make_shared<ov::op::v0::Convert>(inp1, dtype);
         }
 
-        std::shared_ptr<ngraph::Node> node;
+        std::shared_ptr<ov::Node> node;
         if (op == OPERATION::ADD)
-            node = std::make_shared<ngraph::op::v1::Add>(inp0, inp1);
+            node = std::make_shared<ov::op::v1::Add>(inp0, inp1);
         else if (op == OPERATION::PROD)
-            node = std::make_shared<ngraph::op::v1::Multiply>(inp0, inp1);
+            node = std::make_shared<ov::op::v1::Multiply>(inp0, inp1);
         else if (op == OPERATION::GREATER_EQUAL)
-            node = std::make_shared<ngraph::op::v1::GreaterEqual>(inp0, inp1);
+            node = std::make_shared<ov::op::v1::GreaterEqual>(inp0, inp1);
         else if (op == OPERATION::LESS_EQUAL)
-            node = std::make_shared<ngraph::op::v1::LessEqual>(inp0, inp1);
+            node = std::make_shared<ov::op::v1::LessEqual>(inp0, inp1);
+        // Ideally we should do this but int32 internal blobs are converted to float32 data type in inference.
+        // TODO: Remove data type convertion when we have type inference.
+        else if (op == OPERATION::MOD) {
+            auto inp0_i64 = std::make_shared<ov::op::v0::Convert>(inp0, ov::element::i64);
+            auto inp1_i64 = std::make_shared<ov::op::v0::Convert>(inp1, ov::element::i64);
+            auto mod = std::make_shared<ov::op::v1::FloorMod>(inp0_i64, inp1_i64);
+            node = std::make_shared<ov::op::v0::Convert>(mod, ov::element::f32);
+        }
         else
             CV_Error(Error::StsNotImplemented, "Operation is not implemented for nGraph backend");
         return Ptr<BackendNode>(new InfEngineNgraphNode(node));
diff --git a/modules/dnn/src/layers/normalize_bbox_layer.cpp b/modules/dnn/src/layers/normalize_bbox_layer.cpp
index 431eeab82d..5dfb5d128b 100644
--- a/modules/dnn/src/layers/normalize_bbox_layer.cpp
+++ b/modules/dnn/src/layers/normalize_bbox_layer.cpp
@@ -283,8 +283,8 @@ public:
             axes_data.resize(ieInpNode.get_shape().size() - 1);
             std::iota(axes_data.begin(), axes_data.end(), 1);
         }
-        auto axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes_data.size()}, axes_data);
-        auto norm = std::make_shared<ngraph::op::v0::NormalizeL2>(ieInpNode, axes, epsilon, ngraph::op::EpsMode::ADD);
+        auto axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes_data.size()}, axes_data);
+        auto norm = std::make_shared<ov::op::v0::NormalizeL2>(ieInpNode, axes, epsilon, ov::op::EpsMode::ADD);
 
         CV_Assert(blobs.empty() || numChannels == blobs[0].total());
         std::vector<size_t> shape(ieInpNode.get_shape().size(), 1);
@@ -292,13 +292,9 @@ public:
         shape[1] = numChannels;
         if (!blobs.empty())
         {
-            auto weight = std::make_shared<ngraph::op::Constant>(
-                                      ngraph::element::f32, ngraph::Shape(shape), blobs[0].data);
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2021_2)
-            auto mul = std::make_shared<ngraph::op::v1::Multiply>(norm, weight, ngraph::op::AutoBroadcastType::NUMPY);
-#else
-            auto mul = std::make_shared<ngraph::op::v0::Multiply>(norm, weight, ngraph::op::AutoBroadcastType::NUMPY);
-#endif
+            auto weight = std::make_shared<ov::op::v0::Constant>(
+                                      ov::element::f32, ov::Shape(shape), blobs[0].data);
+            auto mul = std::make_shared<ov::op::v1::Multiply>(norm, weight, ov::op::AutoBroadcastType::NUMPY);
             return Ptr<BackendNode>(new InfEngineNgraphNode(mul));
         }
         return Ptr<BackendNode>(new InfEngineNgraphNode(norm));
diff --git a/modules/dnn/src/layers/padding_layer.cpp b/modules/dnn/src/layers/padding_layer.cpp
index f66d44b222..8bb8ab663d 100644
--- a/modules/dnn/src/layers/padding_layer.cpp
+++ b/modules/dnn/src/layers/padding_layer.cpp
@@ -278,14 +278,14 @@ public:
             begins[i] = static_cast<int64_t>(paddings[i].first);
             ends[i]   = static_cast<int64_t>(paddings[i].second);
         }
-        auto padding_below = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{begins.size()}, begins.data());
-        auto padding_above = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{ends.size()}, ends.data());
-        auto pad_mode = paddingType == "constant" ? ngraph::op::PadMode::CONSTANT : ngraph::op::PadMode::REFLECT; // SYMMETRIC
-        auto arg_pad_value = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{}, &paddingValue);;
+        auto padding_below = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{begins.size()}, begins.data());
+        auto padding_above = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{ends.size()}, ends.data());
+        auto pad_mode = paddingType == "constant" ? ov::op::PadMode::CONSTANT : ov::op::PadMode::REFLECT; // SYMMETRIC
+        auto arg_pad_value = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{}, &paddingValue);;
 
         auto pad = paddingType == "constant" ?
-             std::make_shared<ngraph::op::v1::Pad>(ieInpNode, padding_below, padding_above, arg_pad_value, pad_mode) :
-             std::make_shared<ngraph::op::v1::Pad>(ieInpNode, padding_below, padding_above, pad_mode);
+             std::make_shared<ov::op::v1::Pad>(ieInpNode, padding_below, padding_above, arg_pad_value, pad_mode) :
+             std::make_shared<ov::op::v1::Pad>(ieInpNode, padding_below, padding_above, pad_mode);
         return Ptr<BackendNode>(new InfEngineNgraphNode(pad));
     }
 #endif
diff --git a/modules/dnn/src/layers/permute_layer.cpp b/modules/dnn/src/layers/permute_layer.cpp
index 4e6ca2543d..a4f9181d45 100644
--- a/modules/dnn/src/layers/permute_layer.cpp
+++ b/modules/dnn/src/layers/permute_layer.cpp
@@ -475,9 +475,9 @@ public:
     {
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         std::vector<int64_t> order(_order.begin(), _order.end());
-        auto tr_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                       ngraph::Shape({order.size()}), order.data());
-        auto transpose = std::make_shared<ngraph::op::Transpose>(ieInpNode, tr_axes);
+        auto tr_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                       ov::Shape({order.size()}), order.data());
+        auto transpose = std::make_shared<ov::op::v1::Transpose>(ieInpNode, tr_axes);
         return Ptr<BackendNode>(new InfEngineNgraphNode(transpose));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/pooling_layer.cpp b/modules/dnn/src/layers/pooling_layer.cpp
index fb980c4152..92c7a7f72c 100644
--- a/modules/dnn/src/layers/pooling_layer.cpp
+++ b/modules/dnn/src/layers/pooling_layer.cpp
@@ -51,13 +51,8 @@
 
 #ifdef HAVE_DNN_NGRAPH
 #include "../ie_ngraph.hpp"
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-#include <ngraph/op/roi_pooling.hpp>
-#include <ngraph/op/psroi_pooling.hpp>
-#else
-#include <ngraph/op/experimental/layers/roi_pooling.hpp>
-#include <ngraph/op/experimental/layers/psroi_pooling.hpp>
-#endif
+#include <openvino/op/roi_pooling.hpp>
+#include <openvino/op/psroi_pooling.hpp>
 #endif
 
 #include "../op_vkcom.hpp"
@@ -588,20 +583,20 @@ public:
         CV_Assert_N((inputs.size() == 1 && (type == MAX || type == AVE || type == SUM)) || inputs.size() == 2, nodes.size() == inputs.size());
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
 
-        ngraph::op::PadType pad_type = ngraph::op::PadType::EXPLICIT;
+        ov::op::PadType pad_type = ov::op::PadType::EXPLICIT;
         if (!padMode.empty())
-            pad_type = padMode == "VALID" ? ngraph::op::PadType::VALID : ngraph::op::PadType::SAME_UPPER;
+            pad_type = padMode == "VALID" ? ov::op::PadType::VALID : ov::op::PadType::SAME_UPPER;
 
-        auto rounding_type = ceilMode ? ngraph::op::RoundingType::CEIL : ngraph::op::RoundingType::FLOOR;
+        auto rounding_type = ceilMode ? ov::op::RoundingType::CEIL : ov::op::RoundingType::FLOOR;
         if (type == AVE) {
             auto exclude_pad = !avePoolPaddedArea;
-            auto ave_pool = std::make_shared<ngraph::op::v1::AvgPool>(ieInpNode, ngraph::Strides(strides),
-                            ngraph::Shape(pads_begin), ngraph::Shape(pads_end), ngraph::Shape(kernel_size),
+            auto ave_pool = std::make_shared<ov::op::v1::AvgPool>(ieInpNode, ov::Strides(strides),
+                            ov::Shape(pads_begin), ov::Shape(pads_end), ov::Shape(kernel_size),
                             exclude_pad, rounding_type, pad_type);
             return Ptr<BackendNode>(new InfEngineNgraphNode(ave_pool));
         }
         else if (type == SUM) {
-            ngraph::Shape inpShape = ieInpNode.get_shape();
+            ov::Shape inpShape = ieInpNode.get_shape();
             CV_Assert(inpShape.size() == 2 + kernel_size.size());
             std::vector<int64_t> axes;
             for (size_t i = 0; i < kernel_size.size(); i++)
@@ -609,37 +604,33 @@ public:
                 if (inpShape[2 + i] == kernel_size[i])
                     axes.push_back(2 + i);
             }
-            auto reduction_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{axes.size()}, axes);
-            auto reduce_sum = std::make_shared<ngraph::op::v1::ReduceSum>(ieInpNode, reduction_axes, true);
+            auto reduction_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{axes.size()}, axes);
+            auto reduce_sum = std::make_shared<ov::op::v1::ReduceSum>(ieInpNode, reduction_axes, true);
             return Ptr<BackendNode>(new InfEngineNgraphNode(reduce_sum));
         }
         else if (type == MAX) {
-            std::shared_ptr<ngraph::Node> max_pool;
+            std::shared_ptr<ov::Node> max_pool;
             if (computeMaxIdx) {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
                 std::vector<size_t> dilations(kernel_size.size(), 1);
-                max_pool = std::make_shared<ngraph::op::v8::MaxPool>(ieInpNode, ngraph::Strides(strides), ngraph::Strides(dilations),
-                                ngraph::Shape(pads_begin), ngraph::Shape(pads_end), ngraph::Shape(kernel_size),
+                max_pool = std::make_shared<ov::op::v8::MaxPool>(ieInpNode, ov::Strides(strides), ov::Strides(dilations),
+                                ov::Shape(pads_begin), ov::Shape(pads_end), ov::Shape(kernel_size),
                                 rounding_type, pad_type);
-#else
-                CV_Error(Error::StsNotImplemented, "OpenVINO MaxPool with indices");
-#endif
             } else {
-                max_pool = std::make_shared<ngraph::op::v1::MaxPool>(ieInpNode, ngraph::Strides(strides),
-                                ngraph::Shape(pads_begin), ngraph::Shape(pads_end), ngraph::Shape(kernel_size),
+                max_pool = std::make_shared<ov::op::v1::MaxPool>(ieInpNode, ov::Strides(strides),
+                                ov::Shape(pads_begin), ov::Shape(pads_end), ov::Shape(kernel_size),
                                 rounding_type, pad_type);
             }
             return Ptr<BackendNode>(new InfEngineNgraphNode(max_pool));
         }
         else if (type == ROI) {
             auto& coords = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-            auto roi = std::make_shared<ngraph::op::ROIPooling>(ieInpNode, coords,
-                       ngraph::Shape{(size_t)pooledSize.height, (size_t)pooledSize.width}, spatialScale, "max");
+            auto roi = std::make_shared<ov::op::v0::ROIPooling>(ieInpNode, coords,
+                       ov::Shape{(size_t)pooledSize.height, (size_t)pooledSize.width}, spatialScale, "max");
             return Ptr<BackendNode>(new InfEngineNgraphNode(roi));
         }
         else if (type == PSROI) {
             auto& coords = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-            auto psroi = std::make_shared<ngraph::op::PSROIPooling>(ieInpNode, coords,
+            auto psroi = std::make_shared<ov::op::v0::PSROIPooling>(ieInpNode, coords,
                          (size_t)psRoiOutChannels, (size_t)pooledSize.width, spatialScale, 1, 1, "average");
             return Ptr<BackendNode>(new InfEngineNgraphNode(psroi));
         }
diff --git a/modules/dnn/src/layers/prior_box_layer.cpp b/modules/dnn/src/layers/prior_box_layer.cpp
index bf25927480..4cfffb6095 100644
--- a/modules/dnn/src/layers/prior_box_layer.cpp
+++ b/modules/dnn/src/layers/prior_box_layer.cpp
@@ -47,13 +47,8 @@
 
 #ifdef HAVE_DNN_NGRAPH
 #include "../ie_ngraph.hpp"
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-#include <ngraph/op/prior_box.hpp>
-#include <ngraph/op/prior_box_clustered.hpp>
-#else
-#include <ngraph/op/experimental/layers/prior_box.hpp>
-#include <ngraph/op/experimental/layers/prior_box_clustered.hpp>
-#endif
+#include <openvino/op/prior_box.hpp>
+#include <openvino/op/prior_box_clustered.hpp>
 #endif
 
 #include "../op_vkcom.hpp"
@@ -513,23 +508,23 @@ public:
         CV_Assert(nodes.size() == 2);
         auto layer = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         auto image = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
-        auto layer_shape = std::make_shared<ngraph::op::ShapeOf>(layer);
-        auto image_shape = std::make_shared<ngraph::op::ShapeOf>(image);
+        auto layer_shape = std::make_shared<ov::op::v3::ShapeOf>(layer);
+        auto image_shape = std::make_shared<ov::op::v3::ShapeOf>(image);
 
-        auto lower_bounds = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{1}, std::vector<int64_t>{2});
-        auto upper_bounds = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{1}, std::vector<int64_t>{4});
-        auto strides      = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{1}, std::vector<int64_t>{1});
+        auto lower_bounds = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{2});
+        auto upper_bounds = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{4});
+        auto strides      = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{1});
 
-        auto slice_layer = std::make_shared<ngraph::op::v1::StridedSlice>(layer_shape,
+        auto slice_layer = std::make_shared<ov::op::v1::StridedSlice>(layer_shape,
                                             lower_bounds, upper_bounds, strides, std::vector<int64_t>{}, std::vector<int64_t>{});
-        auto slice_image = std::make_shared<ngraph::op::v1::StridedSlice>(image_shape,
+        auto slice_image = std::make_shared<ov::op::v1::StridedSlice>(image_shape,
                                             lower_bounds, upper_bounds, strides, std::vector<int64_t>{}, std::vector<int64_t>{});
 
         if (_explicitSizes)
         {
             CV_Assert_N(!_boxWidths.empty(), !_boxHeights.empty(), !_variance.empty());
             CV_Assert(_boxWidths.size() == _boxHeights.size());
-            ngraph::op::PriorBoxClusteredAttrs attrs;
+            ov::op::v0::PriorBoxClustered::Attributes attrs;
             attrs.widths = _boxWidths;
             attrs.heights = _boxHeights;
             attrs.clip = _clip;
@@ -539,14 +534,14 @@ public:
             attrs.step_widths = _stepX;
             attrs.variances = _variance;
 
-            auto priorBox = std::make_shared<ngraph::op::PriorBoxClustered>(slice_layer, slice_image, attrs);
-            auto axis = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{1}, std::vector<int64_t>{0});
-            auto unsqueeze = std::make_shared<ngraph::op::v0::Unsqueeze>(priorBox, axis);
+            auto priorBox = std::make_shared<ov::op::v0::PriorBoxClustered>(slice_layer, slice_image, attrs);
+            auto axis = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{0});
+            auto unsqueeze = std::make_shared<ov::op::v0::Unsqueeze>(priorBox, axis);
             return Ptr<BackendNode>(new InfEngineNgraphNode(unsqueeze));
         }
         else
         {
-            ngraph::op::PriorBoxAttrs attrs;
+            ov::op::v0::PriorBox::Attributes attrs;
             attrs.min_size = _minSize;
             attrs.max_size = _maxSize;
             // doesn't work with empty aspectRatio
@@ -560,9 +555,9 @@ public:
             attrs.step = _stepX;
             attrs.scale_all_sizes = !_aspectRatios.empty();
 
-            auto priorBox = std::make_shared<ngraph::op::PriorBox>(slice_layer, slice_image, attrs);
-            auto axis = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{1}, std::vector<int64_t>{0});
-            auto unsqueeze = std::make_shared<ngraph::op::v0::Unsqueeze>(priorBox, axis);
+            auto priorBox = std::make_shared<ov::op::v0::PriorBox>(slice_layer, slice_image, attrs);
+            auto axis = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{1}, std::vector<int64_t>{0});
+            auto unsqueeze = std::make_shared<ov::op::v0::Unsqueeze>(priorBox, axis);
             return Ptr<BackendNode>(new InfEngineNgraphNode(unsqueeze));
         }
     }
diff --git a/modules/dnn/src/layers/proposal_layer.cpp b/modules/dnn/src/layers/proposal_layer.cpp
index 2f2a33cc6f..c7baf7870c 100644
--- a/modules/dnn/src/layers/proposal_layer.cpp
+++ b/modules/dnn/src/layers/proposal_layer.cpp
@@ -10,11 +10,7 @@
 
 #ifdef HAVE_DNN_NGRAPH
 #include "../ie_ngraph.hpp"
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-#include <ngraph/op/proposal.hpp>
-#else
-#include <ngraph/op/experimental/layers/proposal.hpp>
-#endif
+#include <openvino/op/proposal.hpp>
 #endif
 
 namespace cv { namespace dnn {
@@ -344,7 +340,7 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         CV_Assert(nodes.size() == 3);
-        ngraph::op::ProposalAttrs attr;
+        ov::op::v0::Proposal::Attributes attr;
         attr.base_size     = baseSize;
         attr.nms_thresh    = nmsThreshold;
         attr.feat_stride   = featStride;
@@ -367,12 +363,12 @@ public:
         auto& image_shape  = nodes[2].dynamicCast<InfEngineNgraphNode>()->node;
 
         CV_Assert_N(image_shape.get_shape().size() == 2, image_shape.get_shape().front() == 1);
-        auto shape   = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                       ngraph::Shape{1},
+        auto shape   = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                       ov::Shape{1},
                        std::vector<int64_t>{(int64_t)image_shape.get_shape().back()});
-        auto reshape = std::make_shared<ngraph::op::v1::Reshape>(image_shape, shape, true);
+        auto reshape = std::make_shared<ov::op::v1::Reshape>(image_shape, shape, true);
 
-        auto proposal = std::make_shared<ngraph::op::Proposal>(class_probs, class_logits, reshape, attr);
+        auto proposal = std::make_shared<ov::op::v0::Proposal>(class_probs, class_logits, reshape, attr);
         return Ptr<BackendNode>(new InfEngineNgraphNode(proposal));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/region_layer.cpp b/modules/dnn/src/layers/region_layer.cpp
index 38b809e0f9..5eecc9139d 100644
--- a/modules/dnn/src/layers/region_layer.cpp
+++ b/modules/dnn/src/layers/region_layer.cpp
@@ -60,7 +60,6 @@
 using namespace cv::dnn::cuda4dnn;
 #endif
 
-
 namespace cv
 {
 namespace dnn
@@ -122,7 +121,7 @@ public:
     {
 #ifdef HAVE_DNN_NGRAPH
     if (backendId == DNN_BACKEND_INFERENCE_ENGINE_NGRAPH)
-        return INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2020_2) && preferableTarget != DNN_TARGET_MYRIAD && new_coords == 0;
+        return preferableTarget != DNN_TARGET_MYRIAD && new_coords == 0;
 #endif
 #ifdef HAVE_CUDA
         if (backendId == DNN_BACKEND_CUDA)
@@ -473,55 +472,55 @@ public:
 
         int64_t cols = b * h * w * anchors;
         int64_t rows = c / anchors;
-        auto shape_node = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2},  std::vector<int64_t>{cols, rows});
-        auto tr_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, std::vector<int64_t>{1, 0});
+        auto shape_node = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2},  std::vector<int64_t>{cols, rows});
+        auto tr_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2}, std::vector<int64_t>{1, 0});
 
-        std::shared_ptr<ngraph::Node> input2d;
+        std::shared_ptr<ov::Node> input2d;
         {
-            input2d = std::make_shared<ngraph::op::v1::Reshape>(input, shape_node, true);
-            input2d = std::make_shared<ngraph::op::Transpose>(input2d, tr_axes);
+            input2d = std::make_shared<ov::op::v1::Reshape>(input, shape_node, true);
+            input2d = std::make_shared<ov::op::v1::Transpose>(input2d, tr_axes);
         }
 
-        std::shared_ptr<ngraph::Node> region;
+        std::shared_ptr<ov::Node> region;
         {
-            auto new_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{4}, std::vector<int64_t>{0, 3, 1, 2});
-            auto tr_input = std::make_shared<ngraph::op::Transpose>(input, new_axes);
+            auto new_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{4}, std::vector<int64_t>{0, 3, 1, 2});
+            auto tr_input = std::make_shared<ov::op::v1::Transpose>(input, new_axes);
 
             std::vector<float> anchors_vec(blobs[0].ptr<float>(), blobs[0].ptr<float>() + blobs[0].total());
             std::vector<int64_t> mask(anchors, 1);
-            region = std::make_shared<ngraph::op::RegionYolo>(tr_input, coords, classes, anchors, useSoftmax, mask, 1, 3, anchors_vec);
+            region = std::make_shared<ov::op::v0::RegionYolo>(tr_input, coords, classes, anchors, useSoftmax, mask, 1, 3, anchors_vec);
 
             auto tr_shape = tr_input->get_shape();
-            auto shape_as_inp = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                                                       ngraph::Shape{tr_shape.size()},
+            auto shape_as_inp = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                                                       ov::Shape{tr_shape.size()},
                                                                        std::vector<int64_t>(tr_shape.begin(), tr_shape.end()));
 
-            region = std::make_shared<ngraph::op::v1::Reshape>(region, shape_as_inp, true);
-            new_axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{4}, std::vector<int64_t>{0, 2, 3, 1});
-            region = std::make_shared<ngraph::op::Transpose>(region, new_axes);
+            region = std::make_shared<ov::op::v1::Reshape>(region, shape_as_inp, true);
+            new_axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{4}, std::vector<int64_t>{0, 2, 3, 1});
+            region = std::make_shared<ov::op::v1::Transpose>(region, new_axes);
 
-            region = std::make_shared<ngraph::op::v1::Reshape>(region, shape_node, true);
-            region = std::make_shared<ngraph::op::Transpose>(region, tr_axes);
+            region = std::make_shared<ov::op::v1::Reshape>(region, shape_node, true);
+            region = std::make_shared<ov::op::v1::Transpose>(region, tr_axes);
         }
 
-        auto strides = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, std::vector<int64_t>{1, 1});
+        auto strides = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2}, std::vector<int64_t>{1, 1});
         std::vector<int64_t> boxes_shape{b, anchors, h, w};
-        auto shape_3d = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{boxes_shape.size()}, boxes_shape.data());
+        auto shape_3d = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{boxes_shape.size()}, boxes_shape.data());
 
-        ngraph::Shape box_broad_shape{1, (size_t)anchors, (size_t)h, (size_t)w};
-        auto scale_x_y_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &scale_x_y);
-        auto shift_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, std::vector<float>{0.5});
+        ov::Shape box_broad_shape{1, (size_t)anchors, (size_t)h, (size_t)w};
+        auto scale_x_y_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &scale_x_y);
+        auto shift_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, std::vector<float>{0.5});
 
-        auto axis = ngraph::op::Constant::create<int64_t>(ngraph::element::i64, ngraph::Shape{}, {0});
-        auto splits = ngraph::op::Constant::create<int64_t>(ngraph::element::i64, ngraph::Shape{5}, {1, 1, 1, 1, rows - 4});
-        auto split = std::make_shared<ngraph::op::v1::VariadicSplit>(input2d, axis, splits);
-        std::shared_ptr<ngraph::Node> box_x;
+        auto axis = ov::op::v0::Constant::create<int64_t>(ov::element::i64, ov::Shape{}, {0});
+        auto splits = ov::op::v0::Constant::create<int64_t>(ov::element::i64, ov::Shape{5}, {1, 1, 1, 1, rows - 4});
+        auto split = std::make_shared<ov::op::v1::VariadicSplit>(input2d, axis, splits);
+        std::shared_ptr<ov::Node> box_x;
         {
-            box_x = std::make_shared<ngraph::op::Sigmoid>(split->output(0));
-            box_x = std::make_shared<ngraph::op::v1::Subtract>(box_x, shift_node, ngraph::op::AutoBroadcastType::NUMPY);
-            box_x = std::make_shared<ngraph::op::v1::Multiply>(box_x, scale_x_y_node, ngraph::op::AutoBroadcastType::NUMPY);
-            box_x = std::make_shared<ngraph::op::v1::Add>(box_x, shift_node, ngraph::op::AutoBroadcastType::NUMPY);
-            box_x = std::make_shared<ngraph::op::v1::Reshape>(box_x, shape_3d, true);
+            box_x = std::make_shared<ov::op::v0::Sigmoid>(split->output(0));
+            box_x = std::make_shared<ov::op::v1::Subtract>(box_x, shift_node, ov::op::AutoBroadcastType::NUMPY);
+            box_x = std::make_shared<ov::op::v1::Multiply>(box_x, scale_x_y_node, ov::op::AutoBroadcastType::NUMPY);
+            box_x = std::make_shared<ov::op::v1::Add>(box_x, shift_node, ov::op::AutoBroadcastType::NUMPY);
+            box_x = std::make_shared<ov::op::v1::Reshape>(box_x, shape_3d, true);
 
             std::vector<float> x_indices(w * h * anchors);
             auto begin = x_indices.begin();
@@ -534,20 +533,20 @@ public:
             {
                 std::copy(begin, begin + w * anchors, begin + j * w * anchors);
             }
-            auto horiz = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, box_broad_shape, x_indices.data());
-            box_x = std::make_shared<ngraph::op::v1::Add>(box_x, horiz, ngraph::op::AutoBroadcastType::NUMPY);
+            auto horiz = std::make_shared<ov::op::v0::Constant>(ov::element::f32, box_broad_shape, x_indices.data());
+            box_x = std::make_shared<ov::op::v1::Add>(box_x, horiz, ov::op::AutoBroadcastType::NUMPY);
 
-            auto cols_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, std::vector<float>{float(w)});
-            box_x = std::make_shared<ngraph::op::v1::Divide>(box_x, cols_node, ngraph::op::AutoBroadcastType::NUMPY);
+            auto cols_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, std::vector<float>{float(w)});
+            box_x = std::make_shared<ov::op::v1::Divide>(box_x, cols_node, ov::op::AutoBroadcastType::NUMPY);
         }
 
-        std::shared_ptr<ngraph::Node> box_y;
+        std::shared_ptr<ov::Node> box_y;
         {
-            box_y = std::make_shared<ngraph::op::Sigmoid>(split->output(1));
-            box_y = std::make_shared<ngraph::op::v1::Subtract>(box_y, shift_node, ngraph::op::AutoBroadcastType::NUMPY);
-            box_y = std::make_shared<ngraph::op::v1::Multiply>(box_y, scale_x_y_node, ngraph::op::AutoBroadcastType::NUMPY);
-            box_y = std::make_shared<ngraph::op::v1::Add>(box_y, shift_node, ngraph::op::AutoBroadcastType::NUMPY);
-            box_y = std::make_shared<ngraph::op::v1::Reshape>(box_y, shape_3d, true);
+            box_y = std::make_shared<ov::op::v0::Sigmoid>(split->output(1));
+            box_y = std::make_shared<ov::op::v1::Subtract>(box_y, shift_node, ov::op::AutoBroadcastType::NUMPY);
+            box_y = std::make_shared<ov::op::v1::Multiply>(box_y, scale_x_y_node, ov::op::AutoBroadcastType::NUMPY);
+            box_y = std::make_shared<ov::op::v1::Add>(box_y, shift_node, ov::op::AutoBroadcastType::NUMPY);
+            box_y = std::make_shared<ov::op::v1::Reshape>(box_y, shape_3d, true);
 
             std::vector<float> y_indices(h * anchors);
             for (int i = 0; i < h; i++)
@@ -555,13 +554,13 @@ public:
                 std::fill(y_indices.begin() + i * anchors, y_indices.begin() + (i + 1) * anchors, i);
             }
 
-            auto vert = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1, (size_t)anchors, (size_t)h, 1}, y_indices.data());
-            box_y = std::make_shared<ngraph::op::v1::Add>(box_y, vert, ngraph::op::AutoBroadcastType::NUMPY);
-            auto rows_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, std::vector<float>{float(h)});
-            box_y = std::make_shared<ngraph::op::v1::Divide>(box_y, rows_node, ngraph::op::AutoBroadcastType::NUMPY);
+            auto vert = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1, (size_t)anchors, (size_t)h, 1}, y_indices.data());
+            box_y = std::make_shared<ov::op::v1::Add>(box_y, vert, ov::op::AutoBroadcastType::NUMPY);
+            auto rows_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, std::vector<float>{float(h)});
+            box_y = std::make_shared<ov::op::v1::Divide>(box_y, rows_node, ov::op::AutoBroadcastType::NUMPY);
         }
 
-        std::shared_ptr<ngraph::Node> box_w, box_h;
+        std::shared_ptr<ov::Node> box_w, box_h;
         {
             int hNorm, wNorm;
             if (nodes.size() > 1)
@@ -596,53 +595,53 @@ public:
                 std::copy(bias_h.begin(), bias_h.begin() + h * anchors, bias_h.begin() + i * h * anchors);
             }
 
-            box_w = std::make_shared<ngraph::op::v0::Exp>(split->output(2));
-            box_w = std::make_shared<ngraph::op::v1::Reshape>(box_w, shape_3d, true);
-            auto anchor_w_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, box_broad_shape, bias_w.data());
-            box_w = std::make_shared<ngraph::op::v1::Multiply>(box_w, anchor_w_node, ngraph::op::AutoBroadcastType::NUMPY);
+            box_w = std::make_shared<ov::op::v0::Exp>(split->output(2));
+            box_w = std::make_shared<ov::op::v1::Reshape>(box_w, shape_3d, true);
+            auto anchor_w_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, box_broad_shape, bias_w.data());
+            box_w = std::make_shared<ov::op::v1::Multiply>(box_w, anchor_w_node, ov::op::AutoBroadcastType::NUMPY);
 
-            box_h = std::make_shared<ngraph::op::v0::Exp>(split->output(3));
-            box_h = std::make_shared<ngraph::op::v1::Reshape>(box_h, shape_3d, true);
-            auto anchor_h_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, box_broad_shape, bias_h.data());
-            box_h = std::make_shared<ngraph::op::v1::Multiply>(box_h, anchor_h_node, ngraph::op::AutoBroadcastType::NUMPY);
+            box_h = std::make_shared<ov::op::v0::Exp>(split->output(3));
+            box_h = std::make_shared<ov::op::v1::Reshape>(box_h, shape_3d, true);
+            auto anchor_h_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, box_broad_shape, bias_h.data());
+            box_h = std::make_shared<ov::op::v1::Multiply>(box_h, anchor_h_node, ov::op::AutoBroadcastType::NUMPY);
         }
 
-        auto region_splits = ngraph::op::Constant::create<int64_t>(ngraph::element::i64, ngraph::Shape{3}, {4, 1, rows - 5});
-        auto region_split = std::make_shared<ngraph::op::v1::VariadicSplit>(region, axis, region_splits);
+        auto region_splits = ov::op::v0::Constant::create<int64_t>(ov::element::i64, ov::Shape{3}, {4, 1, rows - 5});
+        auto region_split = std::make_shared<ov::op::v1::VariadicSplit>(region, axis, region_splits);
 
-        std::shared_ptr<ngraph::Node> scale;
+        std::shared_ptr<ov::Node> scale;
         {
             float thr = classfix == -1 ? 0.5 : 0;
-            auto thresh_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, std::vector<float>{thr});
-            auto mask = std::make_shared<ngraph::op::v1::Less>(region_split->output(1), thresh_node);
-            auto zero_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, mask->get_shape(), std::vector<float>(cols, 0));
-            scale = std::make_shared<ngraph::op::v1::Select>(mask, zero_node, region_split->output(1));
+            auto thresh_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, std::vector<float>{thr});
+            auto mask = std::make_shared<ov::op::v1::Less>(region_split->output(1), thresh_node);
+            auto zero_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, mask->get_shape(), std::vector<float>(cols, 0));
+            scale = std::make_shared<ov::op::v1::Select>(mask, zero_node, region_split->output(1));
         }
 
-        std::shared_ptr<ngraph::Node> probs;
+        std::shared_ptr<ov::Node> probs;
         {
-            probs = std::make_shared<ngraph::op::v1::Multiply>(region_split->output(2), scale, ngraph::op::AutoBroadcastType::NUMPY);
-            auto thresh_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{1}, &thresh);
-            auto mask = std::make_shared<ngraph::op::v1::Greater>(probs, thresh_node);
-            auto zero_node = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, mask->get_shape(), std::vector<float>((rows - 5) * cols, 0));
-            probs = std::make_shared<ngraph::op::v1::Select>(mask, probs, zero_node);
+            probs = std::make_shared<ov::op::v1::Multiply>(region_split->output(2), scale, ov::op::AutoBroadcastType::NUMPY);
+            auto thresh_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{1}, &thresh);
+            auto mask = std::make_shared<ov::op::v1::Greater>(probs, thresh_node);
+            auto zero_node = std::make_shared<ov::op::v0::Constant>(ov::element::f32, mask->get_shape(), std::vector<float>((rows - 5) * cols, 0));
+            probs = std::make_shared<ov::op::v1::Select>(mask, probs, zero_node);
         }
 
 
-        auto concat_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, std::vector<int64_t>{1, cols});
-        box_x = std::make_shared<ngraph::op::v1::Reshape>(box_x, concat_shape, true);
-        box_y = std::make_shared<ngraph::op::v1::Reshape>(box_y, concat_shape, true);
-        box_w = std::make_shared<ngraph::op::v1::Reshape>(box_w, concat_shape, true);
-        box_h = std::make_shared<ngraph::op::v1::Reshape>(box_h, concat_shape, true);
+        auto concat_shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2}, std::vector<int64_t>{1, cols});
+        box_x = std::make_shared<ov::op::v1::Reshape>(box_x, concat_shape, true);
+        box_y = std::make_shared<ov::op::v1::Reshape>(box_y, concat_shape, true);
+        box_w = std::make_shared<ov::op::v1::Reshape>(box_w, concat_shape, true);
+        box_h = std::make_shared<ov::op::v1::Reshape>(box_h, concat_shape, true);
 
-        ngraph::NodeVector inp_nodes{box_x, box_y, box_w, box_h, scale, probs};
-        std::shared_ptr<ngraph::Node> result = std::make_shared<ngraph::op::Concat>(inp_nodes, 0);
-        result = std::make_shared<ngraph::op::Transpose>(result, tr_axes);
+        ov::NodeVector inp_nodes{box_x, box_y, box_w, box_h, scale, probs};
+        std::shared_ptr<ov::Node> result = std::make_shared<ov::op::v0::Concat>(inp_nodes, 0);
+        result = std::make_shared<ov::op::v1::Transpose>(result, tr_axes);
         if (b > 1)
         {
             std::vector<int64_t> sizes{b, static_cast<int64_t>(result->get_shape()[0]) / b, static_cast<int64_t>(result->get_shape()[1])};
-            auto shape_node = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{sizes.size()}, sizes.data());
-            result = std::make_shared<ngraph::op::v1::Reshape>(result, shape_node, true);
+            auto shape_node = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{sizes.size()}, sizes.data());
+            result = std::make_shared<ov::op::v1::Reshape>(result, shape_node, true);
         }
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(result));
diff --git a/modules/dnn/src/layers/reorg_layer.cpp b/modules/dnn/src/layers/reorg_layer.cpp
index ac7d1abfb1..4a8686bae1 100644
--- a/modules/dnn/src/layers/reorg_layer.cpp
+++ b/modules/dnn/src/layers/reorg_layer.cpp
@@ -52,11 +52,7 @@
 #include "../op_inf_engine.hpp"
 #ifdef HAVE_DNN_NGRAPH
 #include "../ie_ngraph.hpp"
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-#include <ngraph/op/reorg_yolo.hpp>
-#else
-#include <ngraph/op/experimental/layers/reorg_yolo.hpp>
-#endif
+#include <openvino/op/reorg_yolo.hpp>
 #endif
 
 #include "../op_cuda.hpp"
@@ -205,7 +201,7 @@ public:
                                         const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        auto reorg = std::make_shared<ngraph::op::ReorgYolo>(ieInpNode, ngraph::Strides{(size_t)reorgStride});
+        auto reorg = std::make_shared<ov::op::v0::ReorgYolo>(ieInpNode, ov::Strides{(size_t)reorgStride});
         return Ptr<BackendNode>(new InfEngineNgraphNode(reorg));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/reshape_layer.cpp b/modules/dnn/src/layers/reshape_layer.cpp
index a72236c472..f259629e96 100644
--- a/modules/dnn/src/layers/reshape_layer.cpp
+++ b/modules/dnn/src/layers/reshape_layer.cpp
@@ -369,9 +369,9 @@ public:
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
 
         std::vector<int64_t> out(outShapes[0].begin(), outShapes[0].end());
-        auto shape   = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                       ngraph::Shape{out.size()}, out.data());
-        auto reshape = std::make_shared<ngraph::op::v1::Reshape>(ieInpNode, shape, true);
+        auto shape   = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                       ov::Shape{out.size()}, out.data());
+        auto reshape = std::make_shared<ov::op::v1::Reshape>(ieInpNode, shape, true);
         return Ptr<BackendNode>(new InfEngineNgraphNode(reshape));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/resize_layer.cpp b/modules/dnn/src/layers/resize_layer.cpp
index fe27748319..12a3b613e1 100644
--- a/modules/dnn/src/layers/resize_layer.cpp
+++ b/modules/dnn/src/layers/resize_layer.cpp
@@ -13,11 +13,7 @@
 
 #ifdef HAVE_DNN_NGRAPH
 #include "../ie_ngraph.hpp"
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-#include <ngraph/op/interpolate.hpp>
-#else
-#include <ngraph/op/experimental/layers/interpolate.hpp>
-#endif
+#include <openvino/op/interpolate.hpp>
 #endif
 
 #ifdef HAVE_CUDA
@@ -376,81 +372,39 @@ public:
     {
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
 
-#if INF_ENGINE_VER_MAJOR_LE(INF_ENGINE_RELEASE_2021_2)
-        ngraph::op::InterpolateAttrs attrs;
-        attrs.pads_begin.push_back(0);
-        attrs.pads_end.push_back(0);
-        attrs.axes = ngraph::AxisSet{2, 3};
-        attrs.align_corners = alignCorners;
-
-        if (interpolation == "nearest") {
-            attrs.mode = "nearest";
-            attrs.antialias = false;
-        } else if (interpolation == "bilinear") {
-            attrs.mode = "linear";
-        } else {
-            CV_Error(Error::StsNotImplemented, "Unsupported interpolation: " + interpolation);
-        }
-
-        std::vector<int64_t> shape = {outHeight, outWidth};
-        auto out_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, shape.data());
-        auto interp = std::make_shared<ngraph::op::Interpolate>(ieInpNode, out_shape, attrs);
-#else
-        ngraph::op::v4::Interpolate::InterpolateAttrs attrs;
+        ov::op::v4::Interpolate::InterpolateAttrs attrs;
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
         if (interpolation == "nearest") {
-            attrs.mode = ngraph::op::v4::Interpolate::InterpolateMode::NEAREST;
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::HALF_PIXEL;
+            attrs.mode = ov::op::v4::Interpolate::InterpolateMode::NEAREST;
+            attrs.coordinate_transformation_mode = ov::op::v4::Interpolate::CoordinateTransformMode::HALF_PIXEL;
         } else if (interpolation == "bilinear") {
-            attrs.mode = ngraph::op::v4::Interpolate::InterpolateMode::LINEAR_ONNX;
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::ASYMMETRIC;
+            attrs.mode = ov::op::v4::Interpolate::InterpolateMode::LINEAR_ONNX;
+            attrs.coordinate_transformation_mode = ov::op::v4::Interpolate::CoordinateTransformMode::ASYMMETRIC;
         } else {
             CV_Error(Error::StsNotImplemented, format("Unsupported interpolation: %s", interpolation.c_str()));
         }
-        attrs.shape_calculation_mode = ngraph::op::v4::Interpolate::ShapeCalcMode::SIZES;
+        attrs.shape_calculation_mode = ov::op::v4::Interpolate::ShapeCalcMode::SIZES;
 
         CV_Assert(!halfPixelCenters || !alignCorners);
         if (halfPixelCenters) {
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::HALF_PIXEL;
+            attrs.coordinate_transformation_mode = ov::op::v4::Interpolate::CoordinateTransformMode::HALF_PIXEL;
         } else if (alignCorners) {
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::ALIGN_CORNERS;
+            attrs.coordinate_transformation_mode = ov::op::v4::Interpolate::CoordinateTransformMode::ALIGN_CORNERS;
         }
 
-        attrs.nearest_mode = ngraph::op::v4::Interpolate::NearestMode::ROUND_PREFER_FLOOR;
-#else
-        if (interpolation == "nearest") {
-            attrs.mode = ngraph::op::v4::Interpolate::InterpolateMode::nearest;
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::half_pixel;
-        } else if (interpolation == "bilinear") {
-            attrs.mode = ngraph::op::v4::Interpolate::InterpolateMode::linear_onnx;
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::asymmetric;
-        } else {
-            CV_Error(Error::StsNotImplemented, format("Unsupported interpolation: %s", interpolation.c_str()));
-        }
-        attrs.shape_calculation_mode = ngraph::op::v4::Interpolate::ShapeCalcMode::sizes;
+        attrs.nearest_mode = ov::op::v4::Interpolate::NearestMode::ROUND_PREFER_FLOOR;
 
-        CV_Assert(!halfPixelCenters || !alignCorners);
-        if (halfPixelCenters) {
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::half_pixel;
-        } else if (alignCorners) {
-            attrs.coordinate_transformation_mode = ngraph::op::v4::Interpolate::CoordinateTransformMode::align_corners;
-        }
-
-        attrs.nearest_mode = ngraph::op::v4::Interpolate::NearestMode::round_prefer_floor;
-#endif // OpenVINO >= 2022.1
 
         std::vector<int64_t> shape = {outHeight, outWidth};
-        auto out_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, shape.data());
+        auto out_shape = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2}, shape.data());
 
         auto& input_shape = ieInpNode.get_shape();
         CV_Assert_N(input_shape[2] != 0, input_shape[3] != 0);
         std::vector<float> scales = {static_cast<float>(outHeight) / input_shape[2], static_cast<float>(outWidth) / input_shape[3]};
-        auto scales_shape = std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape{2}, scales.data());
+        auto scales_shape = std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape{2}, scales.data());
 
-        auto axes = std::make_shared<ngraph::op::Constant>(ngraph::element::i64, ngraph::Shape{2}, std::vector<int64_t>{2, 3});
-        auto interp = std::make_shared<ngraph::op::v4::Interpolate>(ieInpNode, out_shape, scales_shape, axes, attrs);
-#endif
+        auto axes = std::make_shared<ov::op::v0::Constant>(ov::element::i64, ov::Shape{2}, std::vector<int64_t>{2, 3});
+        auto interp = std::make_shared<ov::op::v4::Interpolate>(ieInpNode, out_shape, scales_shape, axes, attrs);
         return Ptr<BackendNode>(new InfEngineNgraphNode(interp));
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/layers/scale_layer.cpp b/modules/dnn/src/layers/scale_layer.cpp
index 2a4e1a05d5..00ca4322f8 100644
--- a/modules/dnn/src/layers/scale_layer.cpp
+++ b/modules/dnn/src/layers/scale_layer.cpp
@@ -331,7 +331,7 @@ public:
     virtual Ptr<BackendNode> initNgraph(const std::vector<Ptr<BackendWrapper> >& inputs, const std::vector<Ptr<BackendNode> >& nodes) CV_OVERRIDE
     {
         auto ieInpNode0 = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
-        ngraph::Output<ngraph::Node> ieInpNode1;
+        ov::Output<ov::Node> ieInpNode1;
         if (nodes.size() > 1)
             ieInpNode1 = nodes[1].dynamicCast<InfEngineNgraphNode>()->node;
 
@@ -346,31 +346,26 @@ public:
         int cAxis = normalize_axis(axis, shape.size());
         shape[cAxis] = numChannels;
 
-        std::shared_ptr<ngraph::Node> node;
+        std::shared_ptr<ov::Node> node;
         if (hasWeights)
         {
-            ngraph::Output<ngraph::Node> weight = blobs.empty() ? ieInpNode1 :
-                          std::make_shared<ngraph::op::Constant>(ngraph::element::f32, ngraph::Shape(shape), blobs[0].data);
-
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2021_2)
-            node = std::make_shared<ngraph::op::v1::Multiply>(ieInpNode0, weight, ngraph::op::AutoBroadcastType::NUMPY);
-#else
-            node = std::make_shared<ngraph::op::v0::Multiply>(ieInpNode0, weight, ngraph::op::AutoBroadcastType::NUMPY);
-#endif
+            ov::Output<ov::Node> weight = blobs.empty() ? ieInpNode1 :
+                          std::make_shared<ov::op::v0::Constant>(ov::element::f32, ov::Shape(shape), blobs[0].data);
+            node = std::make_shared<ov::op::v1::Multiply>(ieInpNode0, weight, ov::op::AutoBroadcastType::NUMPY);
         }
         if (hasBias || !hasWeights)
         {
-            ngraph::Output<ngraph::Node> bias;
+            ov::Output<ov::Node> bias;
             if (hasBias)
             {
                 bias = blobs.empty() ? ieInpNode1 :
-                       std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                              ngraph::Shape(shape), blobs.back().data);
+                       std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                              ov::Shape(shape), blobs.back().data);
             }
             else
-                bias = std::make_shared<ngraph::op::Constant>(ngraph::element::f32,
-                                                              ngraph::Shape(shape), std::vector<float>(numChannels, 0).data());
-            node = std::make_shared<ngraph::op::v1::Add>(node, bias, ngraph::op::AutoBroadcastType::NUMPY);
+                bias = std::make_shared<ov::op::v0::Constant>(ov::element::f32,
+                                                              ov::Shape(shape), std::vector<float>(numChannels, 0).data());
+            node = std::make_shared<ov::op::v1::Add>(node, bias, ov::op::AutoBroadcastType::NUMPY);
         }
         return Ptr<BackendNode>(new InfEngineNgraphNode(node));
     }
diff --git a/modules/dnn/src/layers/slice_layer.cpp b/modules/dnn/src/layers/slice_layer.cpp
index c44d18182e..1947635214 100644
--- a/modules/dnn/src/layers/slice_layer.cpp
+++ b/modules/dnn/src/layers/slice_layer.cpp
@@ -768,14 +768,14 @@ public:
             dims.push_back(finalSliceRanges[0][i].end);
         }
 
-        auto lower_bounds = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                             ngraph::Shape{offsets.size()}, offsets.data());
-        auto upper_bounds = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                             ngraph::Shape{dims.size()}, dims.data());
-        auto strides = std::make_shared<ngraph::op::Constant>(ngraph::element::i64,
-                                        ngraph::Shape{dims.size()}, std::vector<int64_t>((int64_t)dims.size(), 1));
-
-        auto slice = std::make_shared<ngraph::op::v1::StridedSlice>(ieInpNode,
+        auto lower_bounds = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                             ov::Shape{offsets.size()}, offsets.data());
+        auto upper_bounds = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                             ov::Shape{dims.size()}, dims.data());
+        auto strides = std::make_shared<ov::op::v0::Constant>(ov::element::i64,
+                                        ov::Shape{dims.size()}, std::vector<int64_t>((int64_t)dims.size(), 1));
+
+        auto slice = std::make_shared<ov::op::v1::StridedSlice>(ieInpNode,
                                       lower_bounds, upper_bounds, strides, std::vector<int64_t>{}, std::vector<int64_t>{});
 
         return Ptr<BackendNode>(new InfEngineNgraphNode(slice));
diff --git a/modules/dnn/src/layers/softmax_layer.cpp b/modules/dnn/src/layers/softmax_layer.cpp
index ff559e980a..56abc888e3 100644
--- a/modules/dnn/src/layers/softmax_layer.cpp
+++ b/modules/dnn/src/layers/softmax_layer.cpp
@@ -314,9 +314,9 @@ public:
         auto& ieInpNode = nodes[0].dynamicCast<InfEngineNgraphNode>()->node;
         int axis = normalize_axis(axisRaw, ieInpNode.get_shape().size());
         if (logSoftMax) {
-            return new InfEngineNgraphNode(std::make_shared<ngraph::op::v5::LogSoftmax>(ieInpNode, axis));
+            return new InfEngineNgraphNode(std::make_shared<ov::op::v5::LogSoftmax>(ieInpNode, axis));
         } else {
-            return new InfEngineNgraphNode(std::make_shared<ngraph::op::v1::Softmax>(ieInpNode, axis));
+            return new InfEngineNgraphNode(std::make_shared<ov::op::v1::Softmax>(ieInpNode, axis));
         }
     }
 #endif  // HAVE_DNN_NGRAPH
diff --git a/modules/dnn/src/net_openvino.cpp b/modules/dnn/src/net_openvino.cpp
index adcfea60f0..501a596e5d 100644
--- a/modules/dnn/src/net_openvino.cpp
+++ b/modules/dnn/src/net_openvino.cpp
@@ -9,6 +9,12 @@
 #include <opencv2/core/utils/configuration.private.hpp>
 #include <opencv2/core/utils/logger.hpp>
 
+#include "op_inf_engine.hpp"
+
+#ifdef HAVE_INF_ENGINE
+#include <openvino/op/util/op_types.hpp>
+#endif
+
 #include "net_impl.hpp"
 
 #include "backend.hpp"
@@ -146,7 +152,7 @@ public:
     //string dump(bool forceAllocation = false) const override;
 
     static
-    Net createNetworkFromModelOptimizer(InferenceEngine::CNNNetwork& ieNet);
+    Net createNetworkFromModelOptimizer(std::shared_ptr<ov::Model>& ieNet);
 
 };  // NetImplOpenVINO
 
@@ -320,11 +326,6 @@ void NetImplOpenVINO::initBackend(const std::vector<LayerPin>& blobsToKeep_)
         return;
     }
 
-#if INF_ENGINE_VER_MAJOR_LT(INF_ENGINE_RELEASE_2022_1)
-    bool supportsCPUFallback = !isArmComputePlugin() && (preferableTarget == DNN_TARGET_CPU ||
-                               openvino::checkTarget(DNN_TARGET_CPU));
-#endif
-
     // Build Inference Engine networks from sets of layers that support this
     // backend. Split a whole model on several Inference Engine networks if
     // some of layers are not implemented.
@@ -342,49 +343,8 @@ void NetImplOpenVINO::initBackend(const std::vector<LayerPin>& blobsToKeep_)
 
         bool fused = ld.skip;
         Ptr<Layer> layer = ld.layerInstance;
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
         if (ld.id == 0)
             continue;
-#else
-        if (!fused && !layer->supportBackend(preferableBackend))
-        {
-            CV_LOG_DEBUG(NULL, "DNN/IE:    NOT supported!");
-            bool customizable = ld.id != 0 && supportsCPUFallback;
-
-            // TODO: there is a bug in Myriad plugin with custom layers shape infer.
-            if (preferableTarget == DNN_TARGET_MYRIAD || preferableTarget == DNN_TARGET_HDDL)
-            {
-                for (int i = 0; customizable && i < ld.inputBlobs.size(); ++i)
-                {
-                    customizable = ld.inputBlobs[i]->size[0] == 1;
-                }
-            }
-
-            if (preferableTarget == DNN_TARGET_OPENCL)
-                customizable &= ld.type != "Eltwise";
-
-            if (!customizable)
-            {
-                CV_LOG_DEBUG(NULL, "DNN/IE:    NOT customizable!");
-                addNgraphOutputs(ld);
-                net = Ptr<InfEngineNgraphNet>();
-                layer->preferableTarget = DNN_TARGET_CPU;
-
-                for (int i = 0; i < ld.inputBlobsId.size(); ++i)
-                {
-                    LayerData& inpLd = layers[ld.inputBlobsId[i].lid];
-                    Ptr<BackendNode> inpNode = inpLd.backendNodes[preferableBackend];
-                    if (!inpNode.empty())
-                    {
-                        Ptr<InfEngineNgraphNode> ieNode = inpNode.dynamicCast<InfEngineNgraphNode>();
-                        CV_Assert(!ieNode.empty());
-                        ieNode->net->addOutput(ieNode);
-                    }
-                }
-                continue;
-            }
-        }
-#endif
         ld.skip = true;  // Initially skip all Inference Engine supported layers.
 
         // Create a new network if one of inputs from different Inference Engine graph.
@@ -480,25 +440,14 @@ void NetImplOpenVINO::initBackend(const std::vector<LayerPin>& blobsToKeep_)
                     continue;
 
                 // Handle parameters from other subnets. Output port is not used in this case
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
-                if ((ngraph::op::is_parameter(ngraph_input_node) || ngraph::op::is_constant(ngraph_input_node)) &&
-#else
-                if ((ngraph_input_node->is_parameter() || ngraph_input_node->is_constant()) &&
-#endif
-
-                        ngraph_input_node->get_output_size() == 1)
+                if ((ov::op::util::is_parameter(ngraph_input_node) || ov::op::util::is_constant(ngraph_input_node)) &&
+                    ngraph_input_node->get_output_size() == 1)
                 {
                     inputNodes[i] = Ptr<BackendNode>(new InfEngineNgraphNode(ngraph_input_node));
                     continue;
                 }
                 CV_CheckLT((size_t)oid, ngraph_input_node->get_output_size(), "");
-#if INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_4)
                 inputNodes[i] = new InfEngineNgraphNode(ngraph_input_node->output(oid));
-#elif INF_ENGINE_VER_MAJOR_GT(INF_ENGINE_RELEASE_2020_3)
-                inputNodes[i] = Ptr<BackendNode>(new InfEngineNgraphNode(ieInpNode->node->get_output_as_single_output_node(oid)));
-#else
-                inputNodes[i] = Ptr<BackendNode>(new InfEngineNgraphNode(ieInpNode->node->get_output_as_single_output_node(oid, false)));
-#endif
             }
 
             if (layer->supportBackend(preferableBackend))
@@ -606,9 +555,7 @@ void NetImplOpenVINO::initBackend(const std::vector<LayerPin>& blobsToKeep_)
             }
         }
     }
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
     CV_Assert(uniqueNets.size() == 1);
-#endif
 }
 
 
@@ -708,18 +655,15 @@ void switchToOpenVINOBackend(Net& net)
 
 
 /*static*/
-Net NetImplOpenVINO::createNetworkFromModelOptimizer(InferenceEngine::CNNNetwork& ieNet)
+Net NetImplOpenVINO::createNetworkFromModelOptimizer(std::shared_ptr<ov::Model>& ieNet)
 {
     CV_TRACE_FUNCTION();
 
     CV_TRACE_REGION("register_inputs");
 
-    auto ngraphFunction = ieNet.getFunction();
-    CV_Assert(ngraphFunction);
-
     std::vector<String> inputsNames;
     std::vector<MatShape> inp_shapes;
-    for (auto& it : ngraphFunction->get_parameters())
+    for (auto& it : ieNet->get_parameters())
     {
         inputsNames.push_back(it->get_friendly_name());
         std::vector<size_t> dims = it->get_shape();
@@ -728,16 +672,9 @@ Net NetImplOpenVINO::createNetworkFromModelOptimizer(InferenceEngine::CNNNetwork
     // nGraph models produce output "Result" layers which have "/sink_port" suffix in their names.
     // Their inputs are actual model outputs and we change friendly name to it.
     // By this workaround, we produce similar outputs names comparing to ieNet.getOutputsInfo()
-    for (int i = 0; i < ngraphFunction->get_output_size(); ++i) {
-        auto res = ngraphFunction->output(i);
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
+    for (int i = 0; i < ieNet->get_output_size(); ++i) {
+        auto res = ieNet->output(i);
         const std::string& name = res.get_any_name();
-#else
-        auto out = res.get_node()->input(0).get_source_output();
-        std::string name = out.get_node()->get_friendly_name();
-        if (out.get_node()->get_output_size() > 1)
-            name += "." + std::to_string(out.get_index());
-#endif
         if (res.get_node()->get_friendly_name() != name)
             res.get_node()->set_friendly_name(name);
     }
@@ -759,7 +696,7 @@ Net NetImplOpenVINO::createNetworkFromModelOptimizer(InferenceEngine::CNNNetwork
 
     Ptr<BackendNode> backendNode;
     {
-        auto fake_node = std::make_shared<ngraph::op::Parameter>(ngraph::element::f32, ngraph::Shape {});
+        auto fake_node = std::make_shared<ov::op::v0::Parameter>(ov::element::f32, ov::Shape {});
         Ptr<InfEngineNgraphNode> backendNodeNGraph(new InfEngineNgraphNode(fake_node));
         backendNodeNGraph->net = Ptr<InfEngineNgraphNet>(new InfEngineNgraphNet(openvino_impl, ieNet));
         backendNode = backendNodeNGraph;
@@ -767,9 +704,9 @@ Net NetImplOpenVINO::createNetworkFromModelOptimizer(InferenceEngine::CNNNetwork
 
     CV_TRACE_REGION_NEXT("register_outputs");
 
-    std::vector<std::shared_ptr<ngraph::Node>> ngraphOperations = ngraphFunction->get_ops();
+    std::vector<std::shared_ptr<ov::Node>> ngraphOperations = ieNet->get_ops();
 
-    for (auto& it : ngraphFunction->get_results())
+    for (auto& it : ieNet->get_results())
     {
         CV_TRACE_REGION("output");
         const auto& outputName = it->get_friendly_name();
@@ -834,11 +771,11 @@ Net openvino_readNetwork(const String& modelPath, const String& binPath)
 {
     FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;
 
-    InferenceEngine::Core& ie = getCore("");
-    InferenceEngine::CNNNetwork ieNet;
+    ov::Core& ie = getCore("");
+    std::shared_ptr<ov::Model> ieNet;
     try
     {
-        ieNet = ie.ReadNetwork(modelPath, binPath);
+        ieNet = ie.read_model(modelPath, binPath);
     }
     catch (const std::exception& e)
     {
@@ -857,22 +794,15 @@ Net openvino_readNetwork(
 {
     FPDenormalsIgnoreHintScope fp_denormals_ignore_scope;
 
-    InferenceEngine::Core& ie = getCore("");
+    ov::Core& ie = getCore("");
 
     std::string model; model.assign((char*)bufferModelConfigPtr, bufferModelConfigSize);
 
-    InferenceEngine::CNNNetwork ieNet;
+    std::shared_ptr<ov::Model> ieNet;
     try
     {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
         ov::Tensor weights_blob(ov::element::u8, {bufferWeightsSize}, (void*)bufferWeightsPtr);
         ieNet = ie.read_model(model, weights_blob);
-#else
-        InferenceEngine::TensorDesc tensorDesc(InferenceEngine::Precision::U8, { bufferWeightsSize }, InferenceEngine::Layout::C);
-        InferenceEngine::Blob::CPtr weights_blob = InferenceEngine::make_shared_blob<uint8_t>(tensorDesc, (uint8_t*)bufferWeightsPtr, bufferWeightsSize);
-
-        ieNet = ie.ReadNetwork(model, weights_blob);
-#endif
     }
     catch (const std::exception& e)
     {
diff --git a/modules/dnn/src/op_inf_engine.cpp b/modules/dnn/src/op_inf_engine.cpp
index f9e3993d20..04f1da7c71 100644
--- a/modules/dnn/src/op_inf_engine.cpp
+++ b/modules/dnn/src/op_inf_engine.cpp
@@ -10,7 +10,7 @@
 #include <opencv2/dnn/shape_utils.hpp>
 
 #ifdef HAVE_INF_ENGINE
-#include <ie_extension.h>
+#include <openvino/core/extension.hpp>
 #elif defined(ENABLE_PLUGINS)
 // using plugin API
 #include "backend.hpp"
@@ -39,60 +39,6 @@ cv::String setInferenceEngineBackendType(const cv::String& newBackendType)
 
 CV__DNN_INLINE_NS_END
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
-namespace InferenceEngine {
-
-CNNNetwork::CNNNetwork() {}
-
-CNNNetwork::CNNNetwork(std::shared_ptr<ov::Model> model) : model(model) {}
-
-std::shared_ptr<ov::Model> CNNNetwork::getFunction() const {
-    return model;
-}
-
-void CNNNetwork::serialize(const std::string& xmlPath, const std::string& binPath) {
-    ov::pass::Serialize(xmlPath, binPath).run_on_model(model);
-}
-
-void CNNNetwork::reshape(const std::map<std::string, std::vector<size_t> >& shapes) {
-    std::map<std::string, ov::PartialShape> partialShapes;
-    for (const auto& it : shapes) {
-        ov::PartialShape shape;
-        shape.insert(shape.begin(), it.second.begin(), it.second.end());
-        partialShapes.insert({it.first, shape});
-    }
-    model->reshape(partialShapes);
-}
-
-std::vector<std::string> Core::GetAvailableDevices() {
-    return get_available_devices();
-}
-
-void Core::UnregisterPlugin(const std::string& id) {
-    unload_plugin(id);
-}
-
-CNNNetwork Core::ReadNetwork(const std::string& xmlPath, const std::string& binPath) {
-    return read_model(xmlPath, binPath);
-}
-
-ExecutableNetwork Core::LoadNetwork(CNNNetwork net, const std::string& device,
-                                    const std::map<std::string, std::string>& config) {
-    ov::AnyMap props;
-    for (const auto& it : config) {
-        props.insert(it);
-    }
-    return compile_model(net.getFunction(), device, props);
-}
-
-ExecutableNetwork::ExecutableNetwork() {}
-
-ExecutableNetwork::ExecutableNetwork(const ov::CompiledModel& copy) : CompiledModel(copy) {}
-
-ov::InferRequest ExecutableNetwork::CreateInferRequest() { return create_infer_request(); }
-
-}  // namespace InferenceEngine
-
 Mat infEngineBlobToMat(const ov::Tensor& blob)
 {
     std::vector<size_t> dims = blob.get_shape();
@@ -118,67 +64,40 @@ void infEngineBlobsToMats(const ov::TensorVector& blobs,
         mats[i] = infEngineBlobToMat(blobs[i]);
 }
 
-#else
-
-Mat infEngineBlobToMat(const InferenceEngine::Blob::Ptr& blob)
-{
-    // NOTE: Inference Engine sizes are reversed.
-    std::vector<size_t> dims = blob->getTensorDesc().getDims();
-    std::vector<int> size(dims.begin(), dims.end());
-    auto precision = blob->getTensorDesc().getPrecision();
-
-    int type = -1;
-    switch (precision)
-    {
-        case InferenceEngine::Precision::FP32: type = CV_32F; break;
-        case InferenceEngine::Precision::U8: type = CV_8U; break;
-        default:
-            CV_Error(Error::StsNotImplemented, "Unsupported blob precision");
-    }
-    return Mat(size, type, (void*)blob->buffer());
-}
-
-void infEngineBlobsToMats(const std::vector<InferenceEngine::Blob::Ptr>& blobs,
-                          std::vector<Mat>& mats)
-{
-    mats.resize(blobs.size());
-    for (int i = 0; i < blobs.size(); ++i)
-        mats[i] = infEngineBlobToMat(blobs[i]);
-}
-#endif // OpenVINO >= 2022.1
 
 static bool init_IE_plugins()
 {
     // load and hold IE plugins
-    static InferenceEngine::Core* init_core = new InferenceEngine::Core();  // 'delete' is never called
-    (void)init_core->GetAvailableDevices();
+    static ov::Core* init_core = new ov::Core();  // 'delete' is never called
+    (void)init_core->get_available_devices();
     return true;
 }
-static InferenceEngine::Core& retrieveIECore(const std::string& id, std::map<std::string, std::shared_ptr<InferenceEngine::Core> >& cores)
+static ov::Core& retrieveIECore(const std::string& id, std::map<std::string, std::shared_ptr<ov::Core> >& cores)
 {
     AutoLock lock(getInitializationMutex());
-    std::map<std::string, std::shared_ptr<InferenceEngine::Core> >::iterator i = cores.find(id);
+    std::map<std::string, std::shared_ptr<ov::Core> >::iterator i = cores.find(id);
     if (i == cores.end())
     {
-        std::shared_ptr<InferenceEngine::Core> core = std::make_shared<InferenceEngine::Core>();
+        std::shared_ptr<ov::Core> core = std::make_shared<ov::Core>();
         cores[id] = core;
         return *core.get();
     }
     return *(i->second).get();
 }
-static InferenceEngine::Core& create_IE_Core_instance(const std::string& id)
+static ov::Core& create_IE_Core_instance(const std::string& id)
 {
-    static std::map<std::string, std::shared_ptr<InferenceEngine::Core> > cores;
+    static std::map<std::string, std::shared_ptr<ov::Core> > cores;
     return retrieveIECore(id, cores);
 }
-static InferenceEngine::Core& create_IE_Core_pointer(const std::string& id)
+static ov::Core& create_IE_Core_pointer(const std::string& id)
 {
     // load and hold IE plugins
-    static std::map<std::string, std::shared_ptr<InferenceEngine::Core> >* cores =
-            new std::map<std::string, std::shared_ptr<InferenceEngine::Core> >();
+    static std::map<std::string, std::shared_ptr<ov::Core> >* cores =
+            new std::map<std::string, std::shared_ptr<ov::Core> >();
     return retrieveIECore(id, *cores);
 }
-InferenceEngine::Core& getCore(const std::string& id)
+
+ov::Core& getCore(const std::string& id)
 {
     // to make happy memory leak tools use:
     // - OPENCV_DNN_INFERENCE_ENGINE_HOLD_PLUGINS=0
@@ -195,7 +114,7 @@ InferenceEngine::Core& getCore(const std::string& id)
 #endif
             );
 
-    InferenceEngine::Core& core = param_DNN_INFERENCE_ENGINE_CORE_LIFETIME_WORKAROUND
+    ov::Core& core = param_DNN_INFERENCE_ENGINE_CORE_LIFETIME_WORKAROUND
             ? create_IE_Core_pointer(id)
             : create_IE_Core_instance(id);
     return core;
@@ -204,17 +123,13 @@ InferenceEngine::Core& getCore(const std::string& id)
 
 static bool detectArmPlugin_()
 {
-    InferenceEngine::Core& ie = getCore("CPU");
-    const std::vector<std::string> devices = ie.GetAvailableDevices();
+    ov::Core& ie = getCore("CPU");
+    const std::vector<std::string> devices = ie.get_available_devices();
     for (std::vector<std::string>::const_iterator i = devices.begin(); i != devices.end(); ++i)
     {
         if (i->find("CPU") != std::string::npos)
         {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
             const std::string name = ie.get_property(*i, ov::device::full_name);
-#else
-            const std::string name = ie.GetMetric(*i, METRIC_KEY(FULL_DEVICE_NAME)).as<std::string>();
-#endif
             CV_LOG_INFO(NULL, "CPU plugin: " << name);
             return name.find("arm_compute::NEON") != std::string::npos;
         }
@@ -228,17 +143,13 @@ static bool detectMyriadX_(const std::string& device)
     AutoLock lock(getInitializationMutex());
 
     // Lightweight detection
-    InferenceEngine::Core& ie = getCore(device);
-    const std::vector<std::string> devices = ie.GetAvailableDevices();
+    ov::Core& ie = getCore(device);
+    const std::vector<std::string> devices = ie.get_available_devices();
     for (std::vector<std::string>::const_iterator i = devices.begin(); i != devices.end(); ++i)
     {
         if (i->find(device) != std::string::npos)
         {
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
             const std::string name = ie.get_property(*i, ov::device::full_name);
-#else
-            const std::string name = ie.GetMetric(*i, METRIC_KEY(FULL_DEVICE_NAME)).as<std::string>();
-#endif
             CV_LOG_INFO(NULL, "Myriad device: " << name);
             return name.find("MyriadX") != std::string::npos || name.find("Myriad X") != std::string::npos || name.find("HDDL") != std::string::npos;
         }
@@ -259,11 +170,11 @@ void resetMyriadDevice()
 
     AutoLock lock(getInitializationMutex());
 
-    InferenceEngine::Core& ie = getCore("MYRIAD");
+    ov::Core& ie = getCore("MYRIAD");
     try
     {
-        ie.UnregisterPlugin("MYRIAD");
-        ie.UnregisterPlugin("HETERO");
+        ie.unload_plugin("MYRIAD");
+        ie.unload_plugin("HETERO");
     }
     catch (...) {}
 #endif  // HAVE_INF_ENGINE
@@ -276,11 +187,11 @@ void releaseHDDLPlugin()
 
     AutoLock lock(getInitializationMutex());
 
-    InferenceEngine::Core& ie = getCore("HDDL");
+    ov::Core& ie = getCore("HDDL");
     try
     {
-        ie.UnregisterPlugin("HDDL");
-        ie.UnregisterPlugin("HETERO");
+        ie.unload_plugin("HDDL");
+        ie.unload_plugin("HETERO");
     }
     catch (...) {}
 #endif  // HAVE_INF_ENGINE
@@ -351,7 +262,7 @@ namespace openvino {
 bool checkTarget(Target target)
 {
     // Lightweight detection
-    const std::vector<std::string> devices = getCore("").GetAvailableDevices();
+    const std::vector<std::string> devices = getCore("").get_available_devices();
     for (std::vector<std::string>::const_iterator i = devices.begin(); i != devices.end(); ++i)
     {
         if (std::string::npos != i->find("MYRIAD") && target == DNN_TARGET_MYRIAD)
diff --git a/modules/dnn/src/op_inf_engine.hpp b/modules/dnn/src/op_inf_engine.hpp
index c5a2e58683..236b21b1a3 100644
--- a/modules/dnn/src/op_inf_engine.hpp
+++ b/modules/dnn/src/op_inf_engine.hpp
@@ -19,19 +19,13 @@
 
 #ifdef HAVE_INF_ENGINE
 
-#define INF_ENGINE_RELEASE_2020_2 2020020000
-#define INF_ENGINE_RELEASE_2020_3 2020030000
-#define INF_ENGINE_RELEASE_2020_4 2020040000
-#define INF_ENGINE_RELEASE_2021_1 2021010000
-#define INF_ENGINE_RELEASE_2021_2 2021020000
-#define INF_ENGINE_RELEASE_2021_3 2021030000
-#define INF_ENGINE_RELEASE_2021_4 2021040000
 #define INF_ENGINE_RELEASE_2022_1 2022010000
 #define INF_ENGINE_RELEASE_2023_0 2023000000
+#define INF_ENGINE_RELEASE_2024_0 2024000000
 
 #ifndef INF_ENGINE_RELEASE
-#warning("IE version have not been provided via command-line. Using 2021.4 by default")
-#define INF_ENGINE_RELEASE INF_ENGINE_RELEASE_2021_4
+#warning("IE version have not been provided via command-line. Using 2022.1 by default")
+#define INF_ENGINE_RELEASE INF_ENGINE_RELEASE_2022_1
 #endif
 
 #define INF_ENGINE_VER_MAJOR_GT(ver) (((INF_ENGINE_RELEASE) / 10000) > ((ver) / 10000))
@@ -45,13 +39,9 @@
 #pragma GCC diagnostic ignored "-Wsuggest-override"
 #endif
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
 #include <openvino/openvino.hpp>
 #include <openvino/pass/serialize.hpp>
 #include <openvino/pass/convert_fp32_to_fp16.hpp>
-#else
-#include <inference_engine.hpp>
-#endif
 
 #if defined(__GNUC__) && __GNUC__ >= 5
 //#pragma GCC diagnostic pop
@@ -76,18 +66,10 @@ CV__DNN_INLINE_NS_END
 
 Backend& getInferenceEngineBackendTypeParam();
 
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
 Mat infEngineBlobToMat(const ov::Tensor& blob);
 
 void infEngineBlobsToMats(const ov::TensorVector& blobs,
                           std::vector<Mat>& mats);
-#else
-Mat infEngineBlobToMat(const InferenceEngine::Blob::Ptr& blob);
-
-void infEngineBlobsToMats(const std::vector<InferenceEngine::Blob::Ptr>& blobs,
-                          std::vector<Mat>& mats);
-#endif  // OpenVINO >= 2022.1
-
 
 CV__DNN_INLINE_NS_BEGIN
 
@@ -99,54 +81,7 @@ bool isArmComputePlugin();
 
 CV__DNN_INLINE_NS_END
 
-// A series of wrappers for classes from OpenVINO API 2.0.
-// Need just for less conditional compilation inserts.
-#if INF_ENGINE_VER_MAJOR_GE(INF_ENGINE_RELEASE_2022_1)
-namespace InferenceEngine {
-
-class CNNNetwork {
-public:
-    CNNNetwork();
-
-    CNNNetwork(std::shared_ptr<ov::Model> model);
-
-    std::shared_ptr<ov::Model> getFunction() const;
-
-    void serialize(const std::string& xmlPath, const std::string& binPath);
-
-    void reshape(const std::map<std::string, std::vector<size_t> >& shapes);
-
-private:
-    std::shared_ptr<ov::Model> model = nullptr;
-};
-
-typedef ov::InferRequest InferRequest;
-
-class ExecutableNetwork : public ov::CompiledModel {
-public:
-    ExecutableNetwork();
-
-    ExecutableNetwork(const ov::CompiledModel& copy);
-
-    ov::InferRequest CreateInferRequest();
-};
-
-class Core : public ov::Core {
-public:
-    std::vector<std::string> GetAvailableDevices();
-
-    void UnregisterPlugin(const std::string& id);
-
-    CNNNetwork ReadNetwork(const std::string& xmlPath, const std::string& binPath);
-
-    ExecutableNetwork LoadNetwork(CNNNetwork net, const std::string& device,
-                                  const std::map<std::string, std::string>& config);
-};
-
-}
-#endif // OpenVINO >= 2022.1
-
-InferenceEngine::Core& getCore(const std::string& id);
+ov::Core& getCore(const std::string& id);
 
 template<typename T = size_t>
 static inline std::vector<T> getShape(const Mat& mat)
diff --git a/modules/dnn/src/op_webnn.hpp b/modules/dnn/src/op_webnn.hpp
index 5b77b10827..6f96289b80 100644
--- a/modules/dnn/src/op_webnn.hpp
+++ b/modules/dnn/src/op_webnn.hpp
@@ -111,7 +111,7 @@ public:
     void addBlobs(const std::vector<cv::Ptr<BackendWrapper> >& ptrs);
 
     void createNet(Target targetId);
-    // void setNodePtr(std::shared_ptr<ngraph::Node>* ptr);
+    // void setNodePtr(std::shared_ptr<ov::Node>* ptr);
 
     void reset();
 
diff --git a/modules/dnn/test/test_ie_models.cpp b/modules/dnn/test/test_ie_models.cpp
index c6667c7ad2..eff389035d 100644
--- a/modules/dnn/test/test_ie_models.cpp
+++ b/modules/dnn/test/test_ie_models.cpp
@@ -9,7 +9,6 @@
 #ifdef HAVE_INF_ENGINE
 #include <opencv2/core/utils/filesystem.hpp>
 
-
 //
 // Synchronize headers include statements with src/op_inf_engine.hpp
 //
@@ -26,14 +25,11 @@
 #pragma GCC visibility push(default)
 #endif
 
-#include <inference_engine.hpp>
-#include <ie_icnn_network.hpp>
-#include <ie_extension.h>
-
 #if defined(__GNUC__)
 #pragma GCC visibility pop
 #endif
 
+#include <openvino/runtime/core.hpp>
 
 namespace opencv_test { namespace {
 
@@ -62,7 +58,6 @@ static void initDLDTDataPath()
 
 using namespace cv;
 using namespace cv::dnn;
-using namespace InferenceEngine;
 
 struct OpenVINOModelTestCaseInfo
 {
@@ -161,27 +156,6 @@ inline static std::string getOpenVINOModel(const std::string &modelName, bool is
     return std::string();
 }
 
-static inline void genData(const InferenceEngine::TensorDesc& desc, Mat& m, Blob::Ptr& dataPtr)
-{
-    const std::vector<size_t>& dims = desc.getDims();
-    if (desc.getPrecision() == InferenceEngine::Precision::FP32)
-    {
-        m.create(std::vector<int>(dims.begin(), dims.end()), CV_32F);
-        randu(m, -1, 1);
-        dataPtr = make_shared_blob<float>(desc, (float*)m.data);
-    }
-    else if (desc.getPrecision() == InferenceEngine::Precision::I32)
-    {
-        m.create(std::vector<int>(dims.begin(), dims.end()), CV_32S);
-        randu(m, -100, 100);
-        dataPtr = make_shared_blob<int>(desc, (int*)m.data);
-    }
-    else
-    {
-        FAIL() << "Unsupported precision: " << desc.getPrecision();
-    }
-}
-
 void runIE(Target target, const std::string& xmlPath, const std::string& binPath,
            std::map<std::string, cv::Mat>& inputsMap, std::map<std::string, cv::Mat>& outputsMap)
 {
@@ -189,25 +163,12 @@ void runIE(Target target, const std::string& xmlPath, const std::string& binPath
 
     std::string device_name;
 
-#if defined(INF_ENGINE_RELEASE) && INF_ENGINE_VER_MAJOR_GT(2019010000)
-    Core ie;
-#else
-    InferenceEnginePluginPtr enginePtr;
-    InferencePlugin plugin;
-#endif
-
-#if defined(INF_ENGINE_RELEASE) && INF_ENGINE_VER_MAJOR_GT(2019030000)
-    CNNNetwork net = ie.ReadNetwork(xmlPath, binPath);
-#else
-    CNNNetReader reader;
-    reader.ReadNetwork(xmlPath);
-    reader.ReadWeights(binPath);
+    ov::Core core;
 
-    CNNNetwork net = reader.getNetwork();
-#endif
+    auto model = core.read_model(xmlPath, binPath);
 
-    ExecutableNetwork netExec;
-    InferRequest infRequest;
+    ov::CompiledModel compiledModel;
+    ov::InferRequest infRequest;
 
     try
     {
@@ -230,10 +191,6 @@ void runIE(Target target, const std::string& xmlPath, const std::string& binPath
                 CV_Error(Error::StsNotImplemented, "Unknown target");
         };
 
-#if defined(INF_ENGINE_RELEASE) && INF_ENGINE_VER_MAJOR_LE(2019010000)
-        auto dispatcher = InferenceEngine::PluginDispatcher({""});
-        enginePtr = dispatcher.getPluginByDevice(device_name);
-#endif
         if (target == DNN_TARGET_CPU || target == DNN_TARGET_FPGA)
         {
             std::string suffixes[] = {"_avx2", "_sse4", ""};
@@ -255,68 +212,90 @@ void runIE(Target target, const std::string& xmlPath, const std::string& binPath
 #endif  // _WIN32
                 try
                 {
-                    IExtensionPtr extension = make_so_pointer<IExtension>(libName);
-#if defined(INF_ENGINE_RELEASE) && INF_ENGINE_VER_MAJOR_GT(2019010000)
-                    ie.AddExtension(extension, device_name);
-#else
-                    enginePtr->AddExtension(extension, 0);
-#endif
+                    core.add_extension(libName);
                     break;
                 }
                 catch(...) {}
             }
             // Some of networks can work without a library of extra layers.
         }
-#if defined(INF_ENGINE_RELEASE) && INF_ENGINE_VER_MAJOR_GT(2019010000)
-        netExec = ie.LoadNetwork(net, device_name);
-#else
-        plugin = InferencePlugin(enginePtr);
-        netExec = plugin.LoadNetwork(net, {});
-#endif
-        infRequest = netExec.CreateInferRequest();
+        compiledModel = core.compile_model(model, device_name);
+        infRequest = compiledModel.create_infer_request();
     }
     catch (const std::exception& ex)
     {
         CV_Error(Error::StsAssert, format("Failed to initialize Inference Engine backend: %s", ex.what()));
     }
 
-    // Fill input blobs.
+    // Fill input tensors.
     inputsMap.clear();
-    BlobMap inputBlobs;
-    for (auto& it : net.getInputsInfo())
+    for (auto&& it : model->inputs())
     {
-        const InferenceEngine::TensorDesc& desc = it.second->getTensorDesc();
-        genData(desc, inputsMap[it.first], inputBlobs[it.first]);
+        auto type = it.get_element_type();
+        auto shape = it.get_shape();
+        auto& m = inputsMap[it.get_any_name()];
+
+        auto tensor = ov::Tensor(type, shape);
+        if (type == ov::element::f32)
+        {
+            m.create(std::vector<int>(shape.begin(), shape.end()), CV_32F);
+            randu(m, -1, 1);
+        }
+        else if (type == ov::element::i32)
+        {
+            m.create(std::vector<int>(shape.begin(), shape.end()), CV_32S);
+            randu(m, -100, 100);
+        }
+        else
+        {
+            FAIL() << "Unsupported precision: " << type;
+        }
+        std::memcpy(tensor.data(), m.data, tensor.get_byte_size());
+
         if (cvtest::debugLevel > 0)
         {
-            const std::vector<size_t>& dims = desc.getDims();
-            std::cout << "Input: '" << it.first << "' precision=" << desc.getPrecision() << " dims=" << dims.size() << " [";
-            for (auto d : dims)
+            std::cout << "Input: '" << it.get_any_name() << "' precision=" << type << " dims=" << shape << " [";
+            for (auto d : shape)
                 std::cout << " " << d;
-            std::cout << "]  ocv_mat=" << inputsMap[it.first].size << " of " << typeToString(inputsMap[it.first].type()) << std::endl;
+            std::cout << "]  ocv_mat=" << inputsMap[it.get_any_name()].size << " of " << typeToString(inputsMap[it.get_any_name()].type()) << std::endl;
         }
+        infRequest.set_tensor(it, tensor);
     }
-    infRequest.SetInput(inputBlobs);
+    infRequest.infer();
+
 
-    // Fill output blobs.
+    // Fill output tensors.
     outputsMap.clear();
-    BlobMap outputBlobs;
-    for (auto& it : net.getOutputsInfo())
+    for (const auto& it : model->outputs())
     {
-        const InferenceEngine::TensorDesc& desc = it.second->getTensorDesc();
-        genData(desc, outputsMap[it.first], outputBlobs[it.first]);
+        auto type = it.get_element_type();
+        auto shape = it.get_shape();
+        auto& m = outputsMap[it.get_any_name()];
+
+        auto tensor = infRequest.get_tensor(it);
+        if (type == ov::element::f32)
+        {
+            m.create(std::vector<int>(shape.begin(), shape.end()), CV_32F);
+        }
+        else if (type == ov::element::i32)
+        {
+            m.create(std::vector<int>(shape.begin(), shape.end()), CV_32S);
+        }
+        else
+        {
+            FAIL() << "Unsupported precision: " << type;
+        }
+        std::memcpy(m.data, tensor.data(), tensor.get_byte_size());
+
         if (cvtest::debugLevel > 0)
         {
-            const std::vector<size_t>& dims = desc.getDims();
-            std::cout << "Output: '" << it.first << "' precision=" << desc.getPrecision() << " dims=" << dims.size() << " [";
-            for (auto d : dims)
+            std::cout << "Output: '" << it.get_any_name() << "' precision=" << type << " dims=" << shape << " [";
+            for (auto d : shape)
                 std::cout << " " << d;
-            std::cout << "]  ocv_mat=" << outputsMap[it.first].size << " of " << typeToString(outputsMap[it.first].type()) << std::endl;
+            std::cout << "]  ocv_mat=" << outputsMap[it.get_any_name()].size << " of " << typeToString(outputsMap[it.get_any_name()].type()) << std::endl;
         }
-    }
-    infRequest.SetOutput(outputBlobs);
 
-    infRequest.Infer();
+    }
 }
 
 void runCV(Backend backendId, Target targetId, const std::string& xmlPath, const std::string& binPath,